{"meta":{"title":"鬼头兵","subtitle":"人生苦短，几十岁了还不行乐？","description":"我们梦中见！","author":"Rui","url":"http://blog.guitoubing.top"},"pages":[{"title":"404 Not Found 该页无法显示","date":"2018-11-05T17:08:58.000Z","updated":"2019-12-13T06:08:19.960Z","comments":true,"path":"/404.html","permalink":"http://blog.guitoubing.top//404.html","excerpt":"","text":""},{"title":"关于我","date":"2017-10-20T13:04:41.000Z","updated":"2019-12-13T06:08:19.950Z","comments":true,"path":"about/index.html","permalink":"http://blog.guitoubing.top/about/index.html","excerpt":"","text":"用心写文，用脚上传每日一问 芳芳到底什么时候下班呢？"},{"title":"categories","date":"2019-03-11T04:41:37.000Z","updated":"2019-12-13T06:08:19.960Z","comments":true,"path":"categories/index.html","permalink":"http://blog.guitoubing.top/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2018-11-05T17:00:28.000Z","updated":"2019-12-13T06:08:19.950Z","comments":true,"path":"tags/index.html","permalink":"http://blog.guitoubing.top/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Hadoop与HDFS","slug":"分布式计算期末项目文档","date":"2020-11-02T15:33:40.000Z","updated":"2021-05-16T11:00:42.355Z","comments":true,"path":"2020/11/02/分布式计算期末项目文档/","link":"","permalink":"http://blog.guitoubing.top/2020/11/02/分布式计算期末项目文档/","excerpt":"组员：2031570 谈瑞2031568 薛锦伟 分布式计算期末项目简单记录，水一水~","text":"组员：2031570 谈瑞2031568 薛锦伟 分布式计算期末项目简单记录，水一水~ 一 系统架构1 Hadoop基本介绍Hadoop是一个高可靠的(reliable)，规模可扩展的(scalable)，分布式(distributed computing)的开源软件框架。它使我们能用一种简单的编程模型来处理存储于集群上的大数据集。 Hadoop是Apache基金会的一个开源项目，是一个提供了分布式存储和分布式计算功能的基础架构平台。可以应用于企业中的数据存储，日志分析，商业智能，数据挖掘等。其为应用提供可靠性和数据移动。它实现了名为 MapReduce 的编程范式：应用程序被分割成许多小部分，而每个部分都能在集群中的任意节点上执行或重新执行。此外，Hadoop 还提供了分布式文件系统HDFS，用以存储所有计算节点的数据，这为整个集群带来了非常高的带宽。MapReduce 和分布式文件系统的设计，使得整个框架能够自动处理节点故障。它使应用程序与成千上万的独立计算的电脑和 PB 级的数据。 由于Hadoop的架构与本次项目需求较吻合，因此我们直接使用Hadoop的HDFS和MapReduce框架来进行此次项目所需要的分布式系统的实现。 2 集群架构图 3 集群机器 主机名 内存 IP 软件 运行进程 node0 512MB 192.168.137.200 ZooKeeper QuorumPeerMain node1 512MB 192.168.137.201 ZooKeeper QuorumPeerMain node2 512MB 192.168.137.202 ZooKeeper QuorumPeerMain master 2GB 192.168.137.100 Hadoop,Hive,MySql JournalNode,NameNode,ResourceManager,DFSZKFailoverController,HiveServer2,MySql master1 2GB 192.168.137.10 Hadoop,Hive JournalNode,NameNode,ResourceManager,DFSZKFailoverController,HiveServer2 slave1 1GB 192.168.137.101 Hadoop JournalNode,DataNode,NodeManager slave2 1GB 192.168.137.102 Hadoop DataNode,NodeManager slave3 1GB 192.168.137.103 Hadoop DataNode,NodeManager host 8GB 192.168.137.1 应用服务器 4 集群搭建4.1 简介集群使用VirtualBox创建了8台虚拟机模拟真实环境中的分布式集群，虚拟机全部使用CentOS7-x86_64系统，其中3台ZooKeeper集群，5台Hadoop集群（2台Master，3台Slave），Windows本机作为应用程序服务器用于连接此集群。 4.2 虚拟机创建此集群中机器的系统基本配置几乎是一样的，只是在后期所担任的角色不同，因此这里我创建了一台虚拟机，而后将环境配好后复制了7台，而后针对其所担任角色进行针对性修改。 首先创建了一台裸机，要解决的第一个问题是虚拟机与主机的网络通信，这里我采用VirtualBox中的Host-Only连接方式，以保证虚拟机与主机之间正常的网络通信，同时需要在主机上共享网络，以保证虚拟机同时还能访问互联网。 在主机网络设置中共享网络： 在VirtualBox中执行以下操作设置主机连接方式： 在虚拟机终端执行以下操作： # 修改虚拟机的IP、子网掩码vim /etc/sysconfig/network-scripts/ifcfg-enp0s3# 修改为以下内容TYPE=EthernetIPADDR=192.168.137.100NETMASK=255.255.255.0# 保存退出# 修改网关地址vim /etc/sysconfig/network# 修改为以下内容NETWORKING=yesGATEWAY=192.168.137.1# 保存退出# 修改主机名为master，后续过程中访问本机只需要主机名而不用敲IPhostnamectl set-hostname master# 关闭并停用防火墙，由于这里使用的是局域网，因此无需太多考虑网络安全systemctl stop firewalldsystemctl disable firewalld# 重启网络服务systemctl restart network# 尝试从虚拟机ping网关以及从主机ping虚拟机hostname或者ip，若都能ping通说明网络配置成功ping 192.168.137.1# 从虚拟机ping外网查看是否可以连接互联网，这里测试百度IP：61.135.169.105ping 61.135.169.105# 修改hosts文件，添加局域网中其他主机的主机名与ip的映射vim /etc/hosts# 修改为以下内容192.168.137.100 master192.168.137.10 master1192.168.137.101 slave1192.168.137.102 slave2192.168.137.103 slave3192.168.137.200 node0192.168.137.201 node1192.168.137.202 node20.0.0.0 localhost# 保存退出 至此，虚拟机网络配置已完成，下面安装Hadoop 2.9.2及Hive 2.3.4（下载、解压步骤省略），并执行一些准备工作： # 首先添加Hadoop和Hive相关环境变量vim /etc/profile# 添加下列内容export HADOOP_MAPRED_HOME=/usr/local/hadoopexport HADOOP_HOME=/usr/local/hadoopexport HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoopexport HIVE_HOME=/usr/local/hiveexport PATH=$HIVE_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH# 保存退出，并使环境变量生效source /etc/profile Hadoop和Hive的配置需放到各台虚拟机上分别执行，因为不同虚拟机所需要的配置不同。 4.3 虚拟机复制上述步骤已经创建好了一个虚拟机，下面需要复制出7个，并对每台机器针对性的进行一些修改。 4.3.1 网络配置对于每台虚拟机需要执行以下几个步骤以保证9台机器之间形成一个网络： 修改IP vim /etc/sysconfig/network-scripts/ifcfg-enp0s3 针对集群机器中定义的IP将IPADDR项修改为对应的IP 修改主机名 hostnamectl set-hostname XXX 针对集群机器中定义的主机名执行以上命令修改为特定的主机名 重启网络服务 systemctl restart network ping各个节点测试是否成功 ping masterping master1ping slave1ping slave2ping slave3ping node0ping node1ping node2ping 192.168.137.1ping 61.135.169.105 4.3.2 Hadoop配置 修改core-site.xml vim $HADOOP_HOME/etc/hadoop/core-site.xml 作用：Hadoop集群的核心配置文件 需要修改的机器：master、master1、slave1、slave2、slave3 内容： &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://ns&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/var/hadoop&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zookeeper地址 --&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;node0:2181,node1:2182,node2:2181&lt;/value&gt; &lt;/property&gt; &lt;!-- 允许访问此hdfs的主机和群组，此处设置为任意 --&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改hdfs-site.xml vim $HADOOP_HOME/etc/hadoop/hdfs-site.xml 作用：hdfs集群配置文件 需要修改的机器：master、master1、slave1、slave2、slave3 内容： &lt;configuration&gt; &lt;!-- 指定dfs文件存储位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.data.dir&lt;/name&gt; &lt;value&gt;/var/hadoop-data&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定文件备份份数 --&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定机器运行情况检查时间间隔 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.heartbead.recheck-interval&lt;/name&gt; &lt;value&gt;3000000ms&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hdfs的nameservice为ns，和core-site.xml保持一致 --&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;ns&lt;/value&gt; &lt;/property&gt; &lt;!-- NS下面的NameNode --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.ns&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;!-- nn1的RPC通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ns.nn1&lt;/name&gt; &lt;value&gt;master:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- nn1的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.ns.nn1&lt;/name&gt; &lt;value&gt;master:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- nn2的RPC通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ns.nn2&lt;/name&gt; &lt;value&gt;master1:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- nn2的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.ns.nn2&lt;/name&gt; &lt;value&gt;master1:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定NameNode的元数据在JournalNode上的存放位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://master:8485;master1:8485;slave1:8485/ns&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定JournalNode在本地磁盘存放数据的位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/journaldata&lt;/value&gt; &lt;/property&gt; &lt;!-- 开启机器故障自动切换主从机器 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定failover切换的方法(java类的名称) --&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.ns&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定failover切换的方法，这里使用ssh通信方式交换 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt; sshfence shell(/bin/true) &lt;/value&gt; &lt;/property&gt; &lt;!-- ssh切换方法需要指定私钥文件位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 注意 假设当备份份数为2时，现在有三台DataNode机器，文件被分为2个block，block1位于1和2上，block2位于1和3上，这是若机器3宕机了，hdfs会在设定的dfs.namenode.heartbead.recheck-interval时间间隔内检查出机器3(在此时间间隔内可能会出现文件数量紊乱的现象)，此时block2数量变为1，hdfs会自动将1中的block2复制一份到另外一台可用机器上（此处为2）。当机器3恢复运行时，3中备份的block2会自动删除。 当使用jdbc访问hdfs时，不会使用hdfs-site.xml中的dfs.replication，而会默认使用3，可在java的configuration中配置为指定值 修改slaves文件 vim $HADOOP_HOME/etc/hadoop/slaves 作用：为各个master指定为其工作的slave 需要修改的机器：master、master1 内容 slave1slave2slave3 修改yarn-site.xml vim $HADOOP_HOME/etc/hadoop/yarn-site.xml 作用：yarn集群的核心配置文件 需要修改的机器：master、master1、slave1、slave2、slave3 内容： &lt;configuration&gt; &lt;!-- 启用yarn集群的高可用机制 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定ResourceManager集群id，可为任意字串 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;yrc&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定两台ResourceManager的名称 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定两台ResourceManager的主机名 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;master&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;master1&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定两台ResourceManager的web端口，正常情况为8088 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt; &lt;value&gt;master:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt; &lt;value&gt;master1:8088&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定管理集群的Zookeeper集群的地址及对应端口 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;node0:2181,node1:2181,node2:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.auxservices.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定jar包路径 --&gt; &lt;property&gt; &lt;name&gt;yarn.application.classpath&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/usr/local/hadoop/contrib/capacity-scheduler/*.jar&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改mapred-site.xml 作用：指定MapReduce操作的基本属性 需要修改的机器：master、master1 内容 &lt;configuration&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.application.classpath&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/usr/local/hadoop/contrib/capacity-scheduler/*.jar&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 注意： MapReduce是不一定依赖yarn的，但一般使用yarn框架来实现MapReduce 此项若是不配，一些job只会在本机跑，而不会分发给其他机器 修改hive-site.xml vim $HIVE_HOME/conf/hive-site.xml 作用：hive的基本配置 需要修改的机器：master、master1 内容： 修改hive.server2.webui.host &lt;property&gt; &lt;name&gt;hive.server2.webui.host&lt;/name&gt; &lt;value&gt;$&#123;hostname&#125;&lt;/value&gt; &lt;description&gt;The host address the HiveServer2 WebUI will listen on&lt;/description&gt;&lt;/property&gt; 其中${hostname}需要改成对应的主机名称(master与master1)，或者都改为0.0.0.0 修改hive.server2.bind.host &lt;property&gt; &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt; &lt;value&gt;$&#123;hostname&#125;&lt;/value&gt; &lt;description&gt;Bind host on which to run the HiveServer2 Thrift service.&lt;/description&gt;&lt;/property&gt; 其中${hostname}需要改成对应的主机名称(master与master1)，或者都改为0.0.0.0 修改hive.server2.zookeeper.namespace &lt;property&gt; &lt;name&gt;hive.server2.zookeeper.namespace&lt;/name&gt; &lt;value&gt;hiveserver2&lt;/value&gt; &lt;description&gt;The parent node in ZooKeeper used by HiveServer2 when supporting dynamic service discovery.&lt;/description&gt;&lt;/property&gt; 注意两个hiveserver节点的该值应设置为一样，指定了改值后，每当一个hiveserver节点启动时，在Zookeeper集群中，目录树下的hiveserver2文件夹下就可以看到该节点注册到Zookeeper中。 修改javax.jdo.option.ConnectionURL、javax.jdo.option.ConnectionPassword和javax.jdo.option.ConnectionDriverName &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://master:3306/hive?createDatabaseIfNotExist=true&amp;amp;useSSL=false&amp;amp;allowPublicKeyRetrieval=true&lt;/value&gt; &lt;description&gt; JDBC connect string for a JDBC metastore. To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL. For example, jdbc:postgresql://myhost/db?ssl=true for postgres database. &lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;&lt;/property&gt; 这里两个节点的数据库连接字符串也应该是一样的，需要知道的是这里只有master节点安装并运行了mysql服务，存储hive元数据的均在此mysql数据库中，意即存储元数据信息是与master1无关的，实际上mysql服务器可以在网络上的任意位置(此处我一开始也误解了，以为master和master1节点都需要存储hive的元数据)。 另外，mysql的连接jar包需要下载并复制到hive的lib目录下。 至此整个Hadoop集群已经搭建完毕，却未完，需要使用Zookeeper集群来实现集群的高可用性（HA）。 4.3.3 Zookeeper配置对于剩下的node0、node1、node2三台机器是用于搭建Zookeeper集群的，因此需要安装并配置Zookeeper-3.4.10： vim /usr/local/zookeeper/conf/zoo.cfg# 添加一下内容server.1=192.168.137.200:2888:3888server.2=192.168.137.201:2888:3888server.3=192.168.137.202:2888:3888# 保存并关闭 4.3.4 ssh免密登录配置 作用：保证任何一台机器都可通过ssh免密访问其他机器，这对于使用sshfence策略的failover机制是很必要的 需要修改的机器：所有机器 内容： # 进入用户目录下的`.ssh`目录cdcd .ssh/# 创建公钥私钥对ssh-keygen -t rsa# 将公钥发送给其他所有节点，hostname需对应每一台机器更改为其主机名执行一遍ssh-copy-id $&#123;hostname&#125; 至此，Hadoop集群与Zookeeper集群已搭建完毕，接下来需要启动之。 4.4 集群启动4.4.1 初始化数据库在master机器上执行以下操作以初始化数据库： cd $HIVE_HOMEschematool -initSchema -dbType mysql 此命令执行完之后将会在mysql中创建hive的元数据表，以存储hive的表结构及其他属性。 4.4.2 启动集群在master机器上执行以下操作以启动集群： start-all.sh 二 HDFS文件系统1 简介1.1 HDFS简介HDFS，是Hadoop Distributed File System的简称，是Hadoop抽象文件系统的一种实现。Hadoop抽象文件系统可以与本地系统、Amazon S3等集成，甚至可以通过Web协议（webhsfs）来操作。HDFS的文件分布在集群机器上，同时提供副本进行容错及可靠性保证。例如客户端写入读取文件的直接操作都是分布在集群各个机器上的，没有单点性能压力。 1.2 NameNode与DataNodeHDFS由四部分组成，HDFS Client、NameNode、DataNode和Secondary NameNode。整个HDFS集群由Namenode和Datanode构成master-worker（主从）模式。Namenode负责构建命名空间，管理文件的元数据等，而Datanode负责实际存储数据，负责读写工作。 Namenode存放文件系统树及所有文件、目录的元数据。元数据持久化为2种形式：Namespcae image和Edit log。但是持久化数据中不包括Block所在的节点列表，及文件的Block分布在集群中的哪些节点上，这些信息是在系统重启的时候重新构建（通过Datanode汇报的Block信息）。NameNode 的文件结构包含 edits、fsimage、seen_txid、VERSION 以及 in_use.lock。其中， edits 为编辑日志（edit log）， 当客户端执行写操作时， NameNode 会先在编辑日志中写入记录，并在内存中保存一个文件系统的元数据。这一描述 符会在编辑日志改动后更新。fsimage 为文件系统镜像，是文件系统元数据的持久检查点，包含以序列化 格式存储的文件系统目录和文件 inodes，每个 inodes 表征一个文件或目录的元数 据信息以及文件的副本数、修改和访问时间等信息。seen_txid 文件代表的是 NameNode 中 edits_*文件的尾数，当 NameNode 重 启后，会按照 seen_txid 所定义的数字，循序从头运行 edits_0000001~至 seen_txid。VERSION 文件是 java 的属性文件，保存了 HDFS 的版本号。in_use.lock 的作用是防止一台机器同时启动多个 NameNode 进程，导致目录 数据不一致。 在HDFS中，Namenode可能成为集群的单点故障，Namenode不可用时，整个文件系统是不可用的。HDFS针对单点故障提供了2种解决机制： 1）备份持久化元数据将文件系统的元数据同时写到多个文件系统， 例如同时将元数据写到本地文件系统及NFS。这些备份操作都是同步的、原子的。2）Secondary NamenodeSecondary节点定期合并主Namenode的namespace image和edit log， 避免edit log过大，通过创建检查点checkpoint来合并。它会维护一个合并后的namespace image副本， 可用于在Namenode完全崩溃时恢复数据。Secondary Namenode通常运行在另一台机器，因为合并操作需要耗费大量的CPU和内存。其数据落后于Namenode，因此当Namenode完全崩溃时，会出现数据丢失。 通常做法是拷贝NFS中的备份元数据到Second，将其作为新的主Namenode。 在HA（High Availability高可用性）中可以运行一个Hot Standby，作为热备份，在Active Namenode故障之后，替代原有Namenode成为Active Namenode。SecondaryNameNode 的文件结构主要包括 edits、fsimage、VERSION 以及 in_use.lock。其中edits、fsimage、VERSION的内容均与NameNode相同，in_use.lock 的作用是防止一台机器同时启动多个 SecondaryNameNode 进程导致目录数据不 一致。 而DataNode则会负责存储和提取Block，读写请求可能来自namenode，也可能直接来自客户端。数据节点周期性向Namenode汇报自己节点上所存储的Block相关信息。DataNode通常直接从磁盘读取数据，但是频繁使用的Block可以在内存中缓存。默认情况下，一个Block只有一个数据节点会缓存。但是可以针对每个文件可以个性化配置。 作业调度器可以利用缓存提升性能，例如MapReduce可以把任务运行在有Block缓存的节点上。 用户或者应用可以向NameNode发送缓存指令（缓存哪个文件，缓存多久）， 缓存池的概念用于管理一组缓存的权限和资源。DataNode 的文件结构主要包含 BP-前缀文件、blk前缀文件、VERSION 和 in_use.lock。 其中 BP-random integer-NameNode IP address-creation time 的 BP 代表 BlockPool，即 NameNode 的 VERSION 集群中唯一的 blockpoolID；finalized/rbw 目录用于存储 HDFS BLOCK 的数据， blk_前缀文件是 HDFS 中的文件块本身， 存储的是原始文件内容；VERSION 及 in_use.lock 的含义与上述类似。 我们知道NameNode的内存会制约文件数量，HDFS Federation提供了一种横向扩展NameNode的方式。在Federation模式中，每个NameNode管理命名空间的一部分，例如一个NameNode管理/user目录下的文件， 另一个NameNode管理/share目录下的文件。 每个NameNode管理一个namespace volumn，所有volumn构成文件系统的元数据。每个NameNode同时维护一个Block Pool，保存Block的节点映射等信息。各NameNode之间是独立的，一个节点的失败不会导致其他节点管理的文件不可用。 客户端使用mount table将文件路径映射到NameNode。mount table是在Namenode群组之上封装了一层，这一层也是一个Hadoop文件系统的实现，通过viewfs:协议访问。 2 分块机制2.1 分块机制简介物理磁盘中有块的概念，磁盘的物理Block是磁盘操作最小的单元，读写操作均以Block为最小单元，一般为512 Byte。文件系统在物理Block之上抽象了另一层概念，文件系统Block物理磁盘Block的整数倍。通常为几KB。Hadoop提供的df、fsck这类运维工具都是在文件系统的Block级别上进行操作。HDFS的Block块比一般单机文件系统大得多，默认为128M。HDFS的文件被拆分成block-sized的chunk，chunk作为独立单元存储。比Block小的文件不会占用整个Block，只会占据实际大小。例如， 如果一个文件大小为1M，则在HDFS中只会占用1M的空间，而不是128M。是为了最小化查找（seek）时间，控制定位文件与传输文件所用的时间比例。假设定位到Block所需的时间为10ms，磁盘传输速度为100M/s。如果要将定位到Block所用时间占传输时间的比例控制1%，则Block大小需要约100M。 但是如果Block设置过大，在MapReduce任务中，Map或者Reduce任务的个数 如果小于集群机器数量，会使得作业运行效率很低。Block的拆分使得单个文件大小可以大于整个磁盘的容量，构成文件的Block可以分布在整个集群， 理论上，单个文件可以占据集群中所有机器的磁盘。 Block的抽象也简化了存储系统，对于Block，无需关注其权限，所有者等内容（这些内容都在文件级别上进行控制）。Block作为容错和高可用机制中的副本单元，即以Block为单位进行复制。 2.2 块分配流程通常当一个客户端a机器发起请求分配块请求，NN接收到请求后，执行如下块分配流程：1） 如果a不是一个DataNode，则在集群范围内随机选择一个节点作为目标节点，否则执行下面的2,3步骤；2） 判断a机器是否符合存储数据块的目标节点，如果符合，第一个块副本分配完毕；3）如果a机器不符合作为目标节点，则在于与a机器同机架范围内寻找，如果找到目标节点，第一个块副本分配完毕；4）如果在同一个机架内未找到符合要求的目标节点，则在集群内随机查找，找到则第一个块副本分配完毕，否则未找到符合条件的块，块分配失败；5）如果已经成功分配第一个块副本，则与a不同机架的远程机架内寻找目标节点，如果符合，第二个块副本分配完毕；6）如果在远程机架内未找到符合要求的目标节点，在与a相同的本机架寻找，如果找到则第二个块副本分配完毕；否则未找到符合条件的块，第二份块分配失败；7）如果前2个块副本分配成功，则准备分配第三个副本的目标节点，首先会判断前两份是否在同一个机架，如果是，则在远程机架寻找目标节点，找到则第三份副本分配完毕；如果前两份在不同机架，则在与a相同机架内寻找，如果找到则第三份副本分配完毕，否则在集群范围寻找，找到则第三份分配完毕，否则第三份分配失败8）如果块副本大于三分，则在集群范围内随机寻找节点 当在一个范围内找到一个节点后，还需要经过如上的条件判断，才能确定一个DataNode进程是否可以作为目标节点：1） 如果没有节点机器被选择，则该节点可以作为备选节点，否则需要判断下一个DataNode是否符合要求；（这样就防止同一个块副本存储到同一台机器）2） 然后判断节点是否退役，存储空间是否足够，负载是否大于2倍平均负载，本机架选择的节点是否超过限制，如果均满足，则该datanode符合要求，否则需要判断下一个DataNode是否符合要求 3 备份策略3.1 备份策略简介对于HDFS而言，由Namenode负责这个集群的数据备份和分配，在分配过程中，主要考虑下面两个因素： 数据安全：在某个节点发生故障时，不会丢失数据备份； 网络传输开销：在备份数据同步过程中，尽量减少网络传输中的带宽开销； 这两个因素看起来是有些相互矛盾的：想要保证数据安全，那么就尽量把数据备份到多台节点上，但是就需要向多个节点传输数据；想要减少网络传输开销，那么就尽可能把数据备份到一个节点内部或者一个机架内部，因为系统内部的数据传输速度会远大于网络传输的速度。 3.2 数据块部署对于巨大的集群来说，把所有的节点都部署在一个平行的拓扑结构里是不太现实的。比较实际且通用的做法是，把所有的节点分布到多个Rack(服务器机架)上。每个Rack上的节点共享一个交换机，Rack之间可以使用一个或者多个核心交换机进行互联。在大多数情况下，同一Rack中的节点间通信的带宽肯定会高于不同Rack间节点的通信带宽。HDFS默认两个节点之间的网络带宽与他们的物理距离成正比。从一个节点到其父节点的距离被认为是常数1。这样，两个节点之间的距离可以通过将其到各级祖先节点的距离相加计算出来。两个节点之间的距离越短，就意味着他们之间传输数据的带宽越大。HDFS允许管理员通过配置脚本，返回一个节点的rack标识符，作为节点地址。NameNode位于整个结构的最中央，负责解析每一个DataNode的rack位置。当DataNode注册到NameNode时，NameNode会运行这些配置脚本，确定节点属于哪个rack。如果没有进行脚本配置，NameNode则会认为所有的节点都属于一个默认的Rack。 数据块备份的部署对于HDFS数据的可靠性和读写性能都有至关重要的影响。良好的数据块部署策略能够有效地改进数据的可靠性，可用性，甚至提高网络带宽的利用率。目前的HDFS系统提供了可配置的数据块部署策略接口，以此来让用户和研究人员能够对不同的部署策略进行测验，从而达到对系统应用进行优化的目的。 缺省的HDFS数据块部署策略企图在降低数据写入代价，最大化数据可靠性，可用性，以及整合读数据带宽等几个方面做出权衡。当一个新的数据块被创建，HDFS会把第一个数据开备份放到写入程序所在的位置。第二个和第三个数据块备份会被部署到不同rack的其他两个不同的节点。剩余的数据块备份则被放到随机的节点上，但是限制每个节点不会部署多于一个相同的数据块，每个rack不会部署都与两个相同的数据块（如果条件满足的话）。之所以要把第二个和第三个数据块备份放到不同的rack上，是为了考虑到一个集群上的文件所应当具有的分布性。如果头两个数据块备份放到相同的rack上，那么对于任何文件来说，其2/3的文件块会被存放在同一rack上。 在所有目标节点都被选择后，这些节点会被有组织地按照其亲近程度，以流水线的方式被传输到第一个备份上。数据会被以这个顺序推送到节点。在读取的时候，NameNode首先会检查客户端所对应的主机是否位于集群当中。如果是，那么数据块的位置会被返回到客户端，并以按照距离远近排序。然后数据块就会按照顺序从DataNode中进行读取。这一策略会降低rack之间以及节点之间的写入时间，普遍提高写入效率。因为rack故障的几率远低于节点故障的几率，所以该策略不会影响到数据的有效性和可用性。在大多数使用3数据块备份的情况下，该策略能够降低网络带宽的消耗，因为一个数据块只需要部署到两个不同的rack上，而不是3个。 3.3 备份策略NameNode会尽力保证们每个数据块都有所需的备份数量。当Block Report从DataNode提交上来的时候，NameNode会侦测到数据块的备份数量是少于所需还是超过所需。当超过时，NameNode会选择删除一个数据备份。NameNode倾向于不减少rack的数量，并在DataNode中选择一个剩余磁盘空间最小的节点进行备份移除。这样做的主要目的是平衡利用DataNode的存储空间，并其不降低到数据块的可用性。 当一个数据块处于低于其备份需求数的状态时，该数据块就会被放入到备份优先队列中。仅拥有一个数据备份的数据块处于最高优先级，其数据备份数高于其备份因子2/3的数据块则处于最低优先级。有一个后台进程专门负责定期对备份优先队列进行扫面，以确定将备份部署到何处。数据块备份遵循与数据块部署相似的策略。如果数据块当前只有一个备份，那么HDFS会把一个新的备份放到不同rack上。如果数据块当前有两个备份，并且连个备份都存在与相同的rack上，第三个备份就会被放到不同的rack上。否则，第三个备份就被放到同一rack的不同节点上。这么做的目的也是为了降低创建备份的代价。 NameNode也会确保不把所有的数据块备份都部署到同一个rack上。如果NameNode侦测到某数据块的所有备份都在一个rack上，那么它就会把这个数据块当做是mis-replicated(误备份),然后它就会用上面所提到的策略，在其他的rack上把这个数据块再备份一次。在NameNode收到异地rack备份成功后，该数据块就成为了“备份数量高于所需备份数”状态。此时NameNode会根据策略把本地的一个备份删除，因为策略规定不能减少rack的数量。 4 文件一致性4.1 一致性简介从客户端来看，一致性主要指的是多并发访问时更新过的数据如何获取的问题。从服务端来看，则是更新如何复制分布到整个系统，以保证数据最终一致。一致性是因为有并发读写才有的问题，因此在理解一致性的问题时，一定要注意结合考虑并发读写的场景。 从客户端角度，多进程并发访问时，更新过的数据在不同进程如何获取的不同策略，决定了不同的一致性。对于关系型数据库，要求更新过的数据能被后续的访问都能看到，这是强一致性。如果能容忍后续的部分或者全部访问不到，则是弱一致性。如果经过一段时间后要求能访问到更新后的数据，则是最终一致性 从服务端角度，如何尽快将更新后的数据分布到整个系统，降低达到最终一致性的时间窗口，是提高系统的可用度和用户体验非常重要的方面。对于分布式数据系统： N — 数据复制的份数 W — 更新数据时需要保证写完成的节点数 R — 读取数据的时候需要读取的节点数 如果W+R&gt;N，写的节点和读的节点重叠，则是强一致性。例如对于典型的一主一备同步复制的关系型数据库，N=2,W=2,R=1，则不管读的是主库还是备库的数据，都是一致的。 如果W+R&lt;=N，则是弱一致性。例如对于一主一备异步复制的关系型数据库，N=2,W=1,R=1，则如果读的是备库，就可能无法读取主库已经更新过的数据，所以是弱一致性。 对于分布式系统，为了保证高可用性，一般设置N&gt;=3。不同的N,W,R组合，是在可用性和一致性之间取一个平衡，以适应不同的应用场景。 如果N=W,R=1，任何一个写节点失效，都会导致写失败，因此可用性会降低，但是由于数据分布的N个节点是同步写入的，因此可以保证强一致性。 如果N=R,W=1，只需要一个节点写入成功即可，写性能和可用性都比较高。但是读取其他节点的进程可能不能获取更新后的数据，因此是弱一致性。这种情况下，如果W&lt;(N+1)/2，并且写入的节点不重叠的话，则会存在写冲突 4.2 一致性模型HDFS牺牲了一些POSIX的需求来补偿性能，所以有些操作可能会和传统的文件系统不同。当创建一个文件时，它在文件系统的命名空间中是可见的，代码如下: Path p = new Path(\"p\");fs.create(p);assertThat(fs.exists(p),is(true)); 但是对这个文件的任何写操作不保证是可见的，即使在数据流已经刷新的情况下，文件的长度很长时间也会显示为0 ： Path p = new Path(\"p\");OutputStream out = fs.create(p);out.write(\"content\".getBytes(\"UTF-8\"));out.flush();assertThat(fs.getFileStatus(p),getLen(),is(0L)); 一旦一个数据块写人成功，那么大家提出的新请求就可以看到这个块，而对当前写入的块，大家是看不见的。HDFS提供了使所有缓存和DataNode之间的数据强制同步的方法，这个方法是FSDataOutputStream中的sync()函数。当sync()函数返回成功时，HDFS就可以保证此时写入的文件数据是一致的并且对于所有新的用户都是可见的。即使HDFS客户端之间发生冲突，也会导致数据丢失，代码如下: Path p = new Path(\"p\");FSDataOutputStream out = fs.create(p);out.write(\"content\".getBytes(\"UTF-8\"));out.flush();out.sync();assertThat(fs.getFileStatus(p),getLen(),is(((long) \"content\" .length())); 这个操作类似于UNIX系统中的fsync系统调用，为一个文件描述符提交缓存数据，利用Java API写入本地数据，这样就可以保证看到刷新流并且同步之后的数据，代码如下: FileOutputStream out = new FileOutStream(localFile);out.write(\"content\".getBytes(\"UTF-8\"));out.flush(); // flush to operatig systemout.getFD().sync(); // sync to diskassertThat(fs.getFileStatus(p),getLen(),is(((long) \"content\" .length())); 在HDFS中关闭一个文件也隐式地执行了sync()函数，代码如下: Path p = new Path(\"p\");OutputStream out = fs.create(p);out.write(\"content\".getBytes(\"UTF-8\"));out.close();assertThat(fs.getFileStatus(p),getLen(),is(((long) \"content\" .length())); 文件系统的一致性与设计应用程序的方法有关。如果不调用sync()，那么需要做好因客户端或者系统发生故障而丢失部分数据做好准备。对大多数应用程序来说，这是不可接受的，所以需要在合适的地方调用sync()，比如在写入一定量的数据之后。尽管sync()被设计用来最大限度地减少HDFS的负担，但是它仍然有不可忽视的开销，所以需要在数据健壮性和吞吐最之间做好权衡，其中一个较好的参考平衡点就是:通过测试应用程序来选择不同sync()频率间的最佳平衡点。 5 文件读写5.1 写策略应用程序通过创建新文件以及向新文件写数据的方式，给HDFS系统添加数据。文件关闭以后，被写入的数据就无法再修改或者删除，只有以“追加”方式重新打开文件后，才能再次为文件添加数据。HDFS采用单线程写，多线程读的模式。 HDFS客户端需要首先获得对文件操作的授权，然后才能对文件进行写操作。在此期间，其他的客户端都不能对该文件进行写操作。被授权的客户端通过向NameNode发送心跳信号来定期更新授权的状态。当文件关闭时，授权会被回收。文件授权期限分为软限制期和硬限制期两个等级。当处于软限制期内时，写文件的客户端独占对文件的访问权。当软限制过期后，如果客户端无法关闭文件，或没有释放对文件的授权，其他客户端即可以预定获取授权。当硬限制期过期后（一小时左右），如果此时客户端还没有更新（释放）授权，HDFS会认为原客户端已经退出，并自动终止文件的写行为，收回文件控制授权。文件的写控制授权并不会阻止其他客户端对文件进行读操作。因此一个文件可以有多个并行的客户端对其进行读取。 HDFS文件由多个文件块组成。当需要创建一个新文件块时，NameNode会生成唯一的块ID，分配块空间，以及决定将块和块的备份副本存储到哪些DataNode节点上。DataNode节点会形成一个管道，管道中DataNode节点的顺序能够确保从客户端到上一DataNode节点的总体网络距离最小。文件的则以有序包（sequence of packets）的形式被推送到管道。应用程序客户端创建第一个缓冲区，并向其中写入字节。第一个缓冲区被填满后（一般是64 KB大小），数据会被推送到管道。后续的包随时可以推送，并不需要等前一个包发送成功并发回通知（这被称为“未答复发送”——译者注）。不过，这种未答复发送包的数目会根据客户端所限定的“未答复包窗口”(outstanding packets windows)的大小进行限制。 在数据写入HDFS文件后，只要文件写操作没有关闭，HDFS就不保证数据在此期间对新增的客户端读操作可见。如果客户端用户程序需要确保对写入数据的可见性，可以显示地执行hflush操作。这样，当前的包就会被立即推送到管道，并且hflush操作会一直等到所有管道中的DataNode返回成功接收到数据的通知后才会停止。如此就可以保证所有在执行hflush之前所写入的数据对试图读取的客户端用户均可见。 在一个集群的数千个节点里，节点失效（往往是因为存储故障造成的）每天都有可能发生。DataNode中所包含的文件块备份可能会因为内存、磁盘或者网络的错误而造成损坏。为了避免这种错误的形成，HDFS会为其文件的每个数据块生成并存储一份Checksum（总和检查）。Checksum主要供HDFS客户端在读取文件时检查客户端，DataNode以及网络等几个方面可能造成的数据块损坏。当客户端开始建立HDFS文件时，会检查文件的每个数据块的checksum序列，并将其与数据一起发送给DataNode。 DataNode则将checksum存放在文件的元数据文件里，与数据块的具体数据分开存放。当HDFS读取文件时，文件的每个块数据和checksum均被发送到客户端。客户端会即时计算出接受的块数据的checksum, 并将其与接受到的checksum进行匹配。如果不匹配，客户端会通知NameNode，表明接受到的数据块已经损坏，并尝试从其他的DataNode节点获取所需的数据块。 当客户端打开一个文件进行读取时，会从NameNode中获得一个文件数据块列表，列表中包含了每一个所需的数据块的具体位置。这些位置会按照与客户端的距离进行排序。当具体进行数据块读取时，客户端总是尝试首先从最近的位置获取数据。如果尝试失败，客户端会根据排序的顺寻，从下一个位置获取数据。下列情况可能会造成数据读取失败：DataNode不可用，节点不再包含所需数据块，或者数据块备份损坏，以及checksum验证失败。HDFS允许客户端从正在进行写操作的文件中读取数据。当进行这样的操作时，目前正在被写入的数据块对于NameNode来说是未知的。在这样的情况下，客户端会从所有数据块备份中挑选一个数据块，以这个数据块的最后长度作为开始读取数据之前的数据长度。HDFS I/O的设计是专门针对批处理系统进行优化的，比如MapReduce系统，这类系统对顺序读写的吞吐量都有很高的要求。针对于那些需要实时数据流以及随机读写级别的应用来说，系统的读/写响应时间还有待于优化，目前正在做这方面的努力。 5.2 读策略读相对于写，简单一些，详细步骤如下： client访问NameNode，查询元数据信息，获得这个文件的数据块位置列表，返回输入流对象。 就近挑选一台datanode服务器，请求建立输入流 。 DataNode向输入流中中写数据，以packet为单位来校验。 关闭输入流 三 MapReduce计算1 简介MapReduce是Google提出的一个软件架构，也是一种编程模型，用于大规模数据集（大于1TB）的并行运算。概念”Map（映射）”和”Reduce（归约）”，是它们的主要思想，都是从函数式编程语言里借来的，还有从矢量编程语言里借来的特性。它极大地方便了编程人员在不会分布式并行编程的情况下，将自己的程序运行在分布式系统上。 当前的软件实现是指定一个Map（映射）函数，用来把一组键值对映射成一组新的键值对，指定并发的Reduce（归约）函数，用来保证所有映射的键值对中的每一个共享相同的键组。 2 工作原理2.1 Map程序根据输入格式将输入文件划分为多个部分，每个部分均用作映射。任务输入，每个map任务都有一个内存缓冲区，输入数据在映射阶段进行处理，中间结果被写入存储缓冲区，并由写入数据的一方确定。当数据达到内存缓冲区阈值（默认值为0.8）时，会启动线程将内存中溢出的数据写入磁盘，并继续缓冲而不影响映射的中间结果。在溢出写入过程中，MapReduce框架对键进行排序。如果中间结果相对较大，则会形成多个溢出文件。最后缓冲区中的数据也被写入磁盘，从而形成一个溢出文件（至少一个溢出文件）。如果有多个溢出文件，则所有文件最后都会合并为一个文件。 2.2 Reduce当所有的映射任务完成后，每个映射任务会形成一个最终文件，并且该文件按区划分。reduce 任务启动之前，一个映射任务完成后，就会启动线程来拉取映射结果数据到相应的规约任务，不断地合并数据，为规约的数据输入做准备，当所有的射任务完成完成后，数据也拉取合并完毕后，规约任务启动，最终将输出结果存入HDFS 上。 3 MapReduce运行流程3.1 分析 MapReduce 执行过程MapReduce运行的时候，输入输出都是HDFS中的文件，首先，Mapper中运行的任务会去读取HDFS中的数据文件，通过调用map中的方法来处理数据，然后将处理结果输出给Reducer任务，Reducer将接收到的结果作为自己的输入数据，并且也调用自己的方法，将最后的结果输出到HDFS中。过程如下图所示： 3.2 Mapper 任务执行过程详解每个Mapper 任务是一个java 进程，它会读取HDFS 中的文件，解析成很多的键值对，经过我们覆盖的map 方法处理后，转换为很多的键值对再输出。整个Mapper 任务的处理过程又可以分为以下几个阶段： 第一阶段是把输入文件按照一定的标准分片(InputSplit)，每个输入片的大小是固定的。默认情况下，输入片(InputSplit)的大小与数据块(Block)的大小是相同的。如果数据块(Block)的大小是默认值64MB，输入文件有两个，一个是32MB，一个是72MB。那么小的文件是一个输入片，大文件会分为两个数据块，那么是两个输入片。一共产生三个输入片。每一个输入片由一个Mapper 进程处理。这里的三个输入片，会有三个Mapper 进程处理。 第二阶段是对输入片中的记录按照一定的规则解析成键值对。有个默认规则是把每一行文本内容解析成键值对。“键”是每一行的起始位置(单位是字节)，“值” 是本行的文本内容。 第三阶段是调用Mapper 类中的map 方法。第二阶段中解析出来的每一个键值对，调用一次map 方法。如果有1000 个键值对，就会调用1000 次map方法。每一次调用map 方法会输出零个或者多个键值对。 第四阶段是按照一定的规则对第三阶段输出的键值对进行分区。比较是基于键进行的。比如我们的键表示省份(如北京、上海、山东等)，那么就可以按照不同省份进行分区，同一个省份的键值对划分到一个区中。默认是只有一个区。分区的数量就是Reducer 任务运行的数量。默认只有一个Reducer 任务。 第五阶段是对每个分区中的键值对进行排序。首先，按照键进行排序，对于键相同的键值对，按照值进行排序。比如三个键值对&lt;2,2&gt;、&lt;1,3&gt;、&lt;2,1&gt;，键和值分别是整数。那么排序后的结果是&lt;1,3&gt;、&lt;2,1&gt;、&lt;2,2&gt;。如果有 第六阶段，那么进入第六阶段；如果没有，直接输出到本地的linux 文件中。第六阶段是对数据进行归约处理，也就是reduce 处理。键相等的键值对会调用一次reduce 方法。经过这一阶段，数据量会减少。归约后的数据输出到本地的linux文件中。本阶段默认是没有的，需要用户自己增加这一阶段的代码。 上述过程如下图所示： 3.3 Reduce任务执行过程详解每个Reducer 任务是一个java 进程。Reducer 任务接收Mapper 任务的输出，归约处理后写入到HDFS 中，可以分为如下几个阶段。 第一阶段是Reducer 任务会主动从Mapper 任务复制其输出的键值对。Mapper 任务可能会有很多，因此Reducer 会复制多个Mapper 的输出。 第二阶段是把复制到Reducer 本地数据，全部进行合并，即把分散的数据合并成一个大的数据。再对合并后的数据排序。 第三阶段是对排序后的键值对调用reduce 方法。键相等的键值对调用一次reduce 方法，每次调用会产生零个或者多个键值对。最后把这些输出的键值对写入到HDFS 文件中。 在整个MapReduce 程序的开发过程中，我们最大的工作量是覆盖map 函数和覆盖reduce 函数。上述过程如下图所示： 3.4 键值对的编号在对Mapper 任务、Reducer 任务的分析过程中，会看到很多阶段都出现了键值对，这里对键值对进行编号，方便理解键值对的变化情况，对于Mapper 任务输入的键值对，定义为key1 和value1。在map 方法中处理后，输出的键值对，定义为key2 和value2。reduce 方法接收key2 和value2，处理后，输出key3 和value3。具体如下图所示： 4 MapReduce实现MapReduce 在执行的时候，先执行Map 函数，再执行Reduce 函数，达到分布式并行运算效果。其中Map 函数与Reduce 函数需要用户自行设计，而这两个任务也定义了任务本身。 4.1 统计平均通话次数需要用到的数据为：主叫号码。仅需统计每个主叫号码在数据文件中出现的次数，用该数除以天数，即可得到平均值。与WordCount 类似，提取出第二列数据即可进行。具体代码如下图所示： 结果如下图所示，其中每一行有两个数据，第一列为主叫号码，第二列为平均每日通话次数： 4.2 统计不同通话类型下各运营商占比需要用到的数据为：通话类型，主叫号码运营商，被叫号码运营商。与第一个需求不同，本需求的key 值由两个数共同组成，需要定义一个新的类用来包含通话类型和运行商。定义如下： 其中需要注意，由于key 中数据的个数不唯一，需要对内部的数据优先级进行规定，所以要重写compareTo 函数，对key 进行排序。主函数中需要注意Map 阶段的输出类型和Reduce 阶段的输入输出类型，代码如下： 结果如下图，其中1代表电信，2 代表移动，3 代表联通： 4.3 统计用户在各个时间段通话时长所占比例所需用到的数据有：主叫电话号码，通话开始时间，通话结束时间。最后输出的形式是一长串的数组，因此同样需要设计一个新的类用于封装。与上一需求不同的是，该类用于value 的输入和输出。代码如下： 为了完成统计任务，还需要自己求出每个时间段的时间，才能统计占比。这需要设计统计时间段内持续时间的算法，代码如下： 主函数与上一需求类似，区别在于Reduce 阶段的结果输出为浮点型数据，需要再同最开始一样重新定义一个浮点结果类，才能输出。 结果如下图所示，每一行有9 个数据，第一列为主叫号码，第二到第八列为对应时间段中通话时长占一天的比例：","categories":[{"name":"分布式与云计算","slug":"分布式与云计算","permalink":"http://blog.guitoubing.top/categories/分布式与云计算/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://blog.guitoubing.top/tags/Hadoop/"},{"name":"HDFS","slug":"HDFS","permalink":"http://blog.guitoubing.top/tags/HDFS/"}]},{"title":"Tensorflow多GPU并行计算","slug":"Tensorflow-multi-GPU","date":"2020-10-02T15:33:40.000Z","updated":"2021-05-16T10:56:52.708Z","comments":true,"path":"2020/10/02/Tensorflow-multi-GPU/","link":"","permalink":"http://blog.guitoubing.top/2020/10/02/Tensorflow-multi-GPU/","excerpt":"笔者本科毕设做的是遥感图像分割相关的研究，当时采用的Potsdam数据集由于所有图像GSD(对地观测距离)固定且适中，因此将训练时的batch_size设置为了16，基本能满足每个批次中都包含所有地表要素的需求。然而NAIC2020数据集中包含的数据的GSD从0.1m-4m不等，0.1m的GSD意味着一张256x256的影像切片中甚至能够看到一根树干的轮廓，然而这也使得该切片包含的要素种类很少！以致于一个batch中的16张切片可能也无法包含所有的要素。一旦batch训练数据无法包含所有要素，就使得当次训练结果对于未被包含的要素基本不具备识别能力，表现为第n次训练和第n+1次训练结果梯度变化很大，有点像拆了东墙补西墙；而限制了batch_size的最根本原因是GPU限制，本模型完全跑起来需要约9G显存(输入大小为[16,256,256,3]，即16张256x256大小的RGB图像)，服务器上的GPU仅有11G，虽然显存不大，但是GPU多呀，4块2080Ti现在只用了一块。因此笔者考虑使用多GPU来增大batch_size，以此来使得每个batch训练的参数结果对所有要素都较为适配。","text":"笔者本科毕设做的是遥感图像分割相关的研究，当时采用的Potsdam数据集由于所有图像GSD(对地观测距离)固定且适中，因此将训练时的batch_size设置为了16，基本能满足每个批次中都包含所有地表要素的需求。然而NAIC2020数据集中包含的数据的GSD从0.1m-4m不等，0.1m的GSD意味着一张256x256的影像切片中甚至能够看到一根树干的轮廓，然而这也使得该切片包含的要素种类很少！以致于一个batch中的16张切片可能也无法包含所有的要素。一旦batch训练数据无法包含所有要素，就使得当次训练结果对于未被包含的要素基本不具备识别能力，表现为第n次训练和第n+1次训练结果梯度变化很大，有点像拆了东墙补西墙；而限制了batch_size的最根本原因是GPU限制，本模型完全跑起来需要约9G显存(输入大小为[16,256,256,3]，即16张256x256大小的RGB图像)，服务器上的GPU仅有11G，虽然显存不大，但是GPU多呀，4块2080Ti现在只用了一块。因此笔者考虑使用多GPU来增大batch_size，以此来使得每个batch训练的参数结果对所有要素都较为适配。 并行计算深度学习方法采用并行计算的方法一般包括两种：数据并行和模型并行。 数据并行考虑一个事实：深度学习方法在使用随机梯度下降法进行训练时，每次训练我们会尝试随机取出n个数据点去计算当前这个批次的梯度，然而当前这个批次计算出的梯度相对于整个数据集的真实梯度来说很不准确，同时GPU显存通常不足以容下我们所有的训练数据，因此随机梯度下降法通常需要①增大每个批次的数据点数量、②使用更多的训练轮次来更快、更准地进行模型的收敛。 NAIC2020数据集中包含了数十万张切片，由于multi-scale特性的存在，每个切片可能包含1-8个要素不等。由于先前训练时的batch_size为16，很难在每个batch里得到包含所有8个要素的16张切片，而由于GPU显存的限制，16已经达到了极限，因此考虑使用多GPU来解决batch_size这一瓶颈。 数据并行方法在操作时，允许输入模型的批次数量更大，而后将大批次分割为很多小批次，而后在每个GPU上计算一个小批次，每个GPU的梯度估计结果进行汇总后，进行加权平均，最终求和就得到了大批次的梯度估计结果。 证明过程： 当每个小批次是由大批次平均分配时，那么：m1=m2=···=mk=n/k 下图解释了数据并行的流程： 每个节点输入不同的Data Shards，即一个小批次，通过节点上的模型训练得到各自的梯度，而节点中的模型参数是相同的，这样的一个节点通常称作worker；而后每个worker会把各自计算得到的梯度送到ps server进行汇总操作（加权平均）并传回到各个节点进行模型的更新。 模型并行待更新 实际操作笔者代码使用的是TensorFlow-GPU 2.0+Keras 2.3，采用数据并行的方式增大batch_size；训练时，首先需要进行如下设置： os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0,1,2,3' 这句话是让4块GPU都对程序可见，然而可见并不是指成功调用了。 当设置os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &#39;0,1,2,3&#39;变量时，程序仍然可以正常训练，但是没有达到预期的多GPU效果，各GPU占用如下： GPU0 GPU1 GPU2 GPU3 显存占用(MB) 10870 155 155 155 GPU占用(%) 87 0 0 0 可以看到，还是GPU1-3虽然有显存占用，但是GPU基本无占用，说明这三个GPU仅加载了模型，并没有执行实际的计算任务；因此当增大batch_size时，程序并没有自动唤醒GPU1-3来帮助分担计算任务，而是报出内存不足的错误，即使GPU1-3均有空余容量。 在TensorFlow里，需要自行指定操作对应的device即GPU： # image:[64,256,256,3], label:[64,256,256]# small_batch_size = batch_size/N_GPUs = 64/4 = 16with tf.variable_scope(tf.get_variable_scope()): for i in tqdm(range(args.N_GPUs)): with tf.device('/gpu:%d' % i): start = i * small_batch_size end = (i + 1) * small_batch_size x = image[start:end, :, :, :] y = label[start:end, :, :] logits = model.forward(x) …… # calc loss with logits …… grads = optimizer.compute_gradients(loss=loss, var_list=tf.trainable_variables()) all_grads.append(grads)# merge gradientsgrads = average_gradients(all_grads) 上述代码中，我将batch_size设置为64，并将每个batch平均分配给4个GPU来进行计算任务，每个GPU需要前向传递更新自己的参数。一个训练伦次中，当4个GPU都计算完了梯度后，会传给主GPU(也可以是CPU，这里我使用GPU0为主GPU)进行梯度的合并，而后反向传递回并更新4个GPU的模型参数。训练过程中，GPU占用如下： GPU0 GPU1 GPU2 GPU3 显存占用(MB) 8793 8787 8787 8787 GPU占用(%) 30-55 30-50 30-50 30-50 可见，由于GPU0是主GPU，需要完成梯度的合并工作，因此需要占用稍微多一点的内存和GPU。 当增加了GPU数量以进行并行计算时，通常会由于GPU之间的通信、梯度汇总等原因导致批次的训练速度变慢：本机4个GPU并行计算时，训练速度约为1.2次/s，单GPU训练速度约为2次/s；然而由于4GPU每轮次输入了64个切片，是单GPU的4倍之多，因此4GPU1.25w次训练就能达到单GPU5w次的训练效果，总体训练时间还是缩短了。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://blog.guitoubing.top/categories/深度学习/"}],"tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://blog.guitoubing.top/tags/TensorFlow/"},{"name":"图像分割","slug":"图像分割","permalink":"http://blog.guitoubing.top/tags/图像分割/"},{"name":"遥感影像","slug":"遥感影像","permalink":"http://blog.guitoubing.top/tags/遥感影像/"}]},{"title":"Java - Spring学习笔记","slug":"Spring学习笔记","date":"2019-10-19T11:12:29.000Z","updated":"2021-05-16T10:56:33.611Z","comments":true,"path":"2019/10/19/Spring学习笔记/","link":"","permalink":"http://blog.guitoubing.top/2019/10/19/Spring学习笔记/","excerpt":"理解DIP、IOC、DI及IOC容器","text":"理解DIP、IOC、DI及IOC容器 DIPDIP(Dependence Inversion Principle)即依赖倒置原则，是一种抽象的软件架构设计的原则，其有两个特点： 高层模块不应依赖于低层模块，两者应该依赖于抽象 抽象不应该依赖于实现，实现应该依赖于抽象 举例子对比一下依赖倒置与依赖无倒置，代码使用SpringBoot风格： 依赖无倒置 这种明显高层模块已经依赖于低层模块，不满足高层模块不应依赖于低层模块，两者应该依赖于抽象的特点。 示例代码如下： controller层使用 依赖倒置","categories":[{"name":"Java","slug":"Java","permalink":"http://blog.guitoubing.top/categories/Java/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://blog.guitoubing.top/tags/Spring/"}]},{"title":"C# - 实现读写锁","slug":"C-ReadWriteLock","date":"2019-05-02T16:33:11.000Z","updated":"2021-05-16T10:53:13.998Z","comments":true,"path":"2019/05/03/C-ReadWriteLock/","link":"","permalink":"http://blog.guitoubing.top/2019/05/03/C-ReadWriteLock/","excerpt":"使用C#实现的可重入、非公平的读写锁，主要目的实现并发读写以及读写同步问题。为了减少读等待时间以及防止出现写饥饿现象，本锁使用了非同步锁的实现，允许读优先（提高效率）的同时使用一个阈值限定读者的最大数量（防止写饥饿）；重入机制允许某个线程可以获取锁多次（如多次函数调用导致的锁重入），每次获取都需要有对应的释放，否则会出错。","text":"使用C#实现的可重入、非公平的读写锁，主要目的实现并发读写以及读写同步问题。为了减少读等待时间以及防止出现写饥饿现象，本锁使用了非同步锁的实现，允许读优先（提高效率）的同时使用一个阈值限定读者的最大数量（防止写饥饿）；重入机制允许某个线程可以获取锁多次（如多次函数调用导致的锁重入），每次获取都需要有对应的释放，否则会出错。 本锁的目的是实现对读写线程队列的调度，而不是对线程获取锁的顺序进行调度。要注意读写线程进入队列的顺序是系统调度的（意即创建多个线程并Start时，其进入线程的时机是系统决定的，可能最后创建的线程最先执行），这里实现的是对进入队列后的线程进行阻塞、唤醒等操作。 本锁使用了C#中的lock原语，目的是为了实现一些队列操作的原子性。 实现本锁时作者参考了Java中AQS和ReentrantReadWriteLock的一些实现细节，予以理解仿效，设计了这个简单的读写锁，实现逻辑上可能仍有很多缺陷或矛盾，望各位看官批评指正！ Node节点Node节点是实现本读写锁的关键所在，读写线程队列也是基于Node节点实现的。Node内部持有了一个线程，同时有很多指向其他Node的引用。Node类实现如下： public class Node &#123; // 读节点 public static readonly Node SHARED = new Node(); // 写节点 public static readonly Node EXCLUSIVE = null; // 读链长度阈值 public static int Threshold = 3; // 节点状态 public static readonly int CANCELLED = 1; public static readonly int RUNNING = -1; public static readonly int WAITING = -2; public static readonly int SIGNAL = -3; public int waitStatus; // 前驱节点 public Node prev; // 后继节点 public Node next; // 持有线程 public Thread thread; //以下参数仅对读节点适用 // 读链头 public Node readerHead; // 后继读节点 public Node nextReader; // 读链长度 public int readerCount = 1; // 节点类型 public Node mode; // 是否是共享节点 public bool isShared() &#123; return mode == SHARED; &#125; public Node() &#123; &#125; // 创建共享或独占节点 public Node(Thread thread, Node mode) &#123; this.mode = mode; this.thread = thread; &#125; // 获取状态 public static string GetStatus(int status) &#123; switch (status) &#123; case 1: return &quot;CANCELLED&quot;; case -1: return &quot;RUNNING&quot;; case -2: return &quot;WAITING&quot;; case -3: return &quot;SIGNAL&quot;; &#125; return &quot;DEFAULT&quot;; &#125; &#125; 各属性及方法的涵义如上述代码注释所示，而由Node组成的等待队列结构（双向链表）如下图所示（可能出现的一种情况）： 对于写节点来说，锁是独占的，一次仅能有一个写线程在执行；而对于读节点来说，锁是共享的，在同一读链上的所有节点都可同时进行读。 重要变量头节点 head头节点是个傀儡节点，private volatile Node head，其无实际涵义，只是为了作为队列头而存在；其使用了惰性初始化的方法，仅在第一个节点入队列时初始化。 volatile关键字在是为了实现变量的内存可见性，使用该关键字修饰的变量的修改会直接反映到内存中而不是缓存中。 尾节点 tail尾节点是队列的最后一个节点，private volatile Node tail，引入尾节点是为了防止在新节点入列时遍历队列，提高了效率。 重入量 reentrants对于写锁（独占锁）来说，当一个写线程获取锁时，reentrants为1，后续每当锁重入一次，reentrants增加1；释放锁时，每释放一次reentrants减少1，直到reentrants为0时该线程释放当前锁，唤醒后续线程； 对于读锁（共享锁）来说，由于读链中可能会有不超过Node.Threshold个数的读节点，且每个读节点都可能会产生重入，这里会将reentrants初始化为读链长度，在Node.RUNNING`时读链每增加一个读节点会增加一个reentrants，读链中的每个节点多一次重入也会导致reentrants增加1`。 写锁获取 WriteLock()先看获取写锁的流程： /** * 获取写锁 */public void WriteLock()&#123; // 当前节点 Node currentNode = null; // 这里使用了惰性初始化，如果头节点为空，则初始化头节点 lock (this) &#123; if (head == null) head = new Node(); &#125; // 如果尾节点为空，说明当前队列为空，此时线程直接入队列设置为RUNNING状态 lock (this) &#123; if (tail == null) &#123; EnqWhenTailNull(Node.EXCLUSIVE); return; &#125; &#125; lock (this) &#123; // 如果当前线程就是持有线程，说明锁在重入，reentrants加1 if (owner == Thread.CurrentThread) &#123; reentrants += 1; return; &#125; // 否则，当前线程需要进入等待队列进行等待 else &#123; // 获取尾节点 Node t = tail; // 由于可能出现其他线程的介入，需要再次检测为节点是否为空 if (t == null) &#123; EnqWhenTailNull(Node.EXCLUSIVE); return; &#125; // 否则直接入队列等待 else currentNode = Enq(Node.EXCLUSIVE); &#125; &#125; // 检测当前节点是否可以被唤醒 while (currentNode.waitStatus != Node.SIGNAL) &#123; &#125; // 此时线程已被唤醒，设置重入量及状态等 reentrants = 1; currentNode.waitStatus = Node.RUNNING; owner = Thread.CurrentThread;&#125; 基本流程如下： 先判断队列是否已初始化，若未初始化，先初始化head节点 判断队列是否为空（即判断tail==null），若是直接进入队列，成功获取锁，返回；否则进入下一步 判断当前线程是否为持有锁的线程，若是说明线程在重入当前锁，直接reentrants加1，成功获取锁，返回；否则进入下一步 获取尾节点，由于第2步结束时可能有其他线程的介入，因此需再次判断tail==null，若是直接进入队列，成功获取锁，返回；否则以当前线程创建节点并入队列，同时阻塞 循环检测当前节点是否可以被唤醒，若可则解除阻塞状态，设置节点状态等信息，并成功获取独占锁 释放 WriteUnlock()释放写锁的逻辑很简单： /** * 释放写锁 */public void WriteUnlock()&#123; // 加锁是为了同步修改队列信息 lock (this) &#123; // 当前锁持有者重入量直接-1 reentrants -= 1; // 获取队列头（除了head之外的头） Node node = GetHolderNode(); // 如果后续节点为空，则释放完成，队列已空；若后续节点不为空，则可能需要唤醒后续节点 if (node != null) &#123; // 若当前锁的重入量为0，说明锁已经完全释放，则需要唤醒后继有效节点（否则可能只是释放了一个锁内部的锁） if (reentrants == 0) &#123; // 当前节点置为无效 node.waitStatus = Node.CANCELLED; // 唤醒后继有效节点 AwakeNext(); &#125; &#125; &#125;&#125; 逻辑如下： 锁reentrants减1 获取当前锁持有的节点 判断上述获取的节点是否为空，若为空说明队列已空，已无等待节点，直接返回；否则检查reentrants是否已减至0 若是则当前节点线程已完成，置为Node.CANCELLED状态，并唤醒后继有效节点 读锁获取 ReadLock()相比于写锁的获取，读锁的获取要复杂一点，因为涉及到读的共享以及阈值的控制： /** * 获取读锁 */public void ReadLock()&#123; Node reader = null; // 这里基本的初始化方式和读锁相似，不赘述 lock (this) &#123; if (head == null) head = new Node(); &#125; Thread current = Thread.CurrentThread; lock (this) &#123; if (tail == null) &#123; EnqWhenTailNull(Node.SHARED); return; &#125; &#125; Node currentNode = null; lock (this) &#123; // 读可重入 if (ReaderCanReentranted()) &#123; reentrants += 1; return; &#125; // 获取当前持有锁节点 Node node = head.next; if (node == null) &#123; EnqWhenTailNull(Node.SHARED); return; &#125; else &#123; // 队列中有写者的情况 if (HasWriter()) &#123; // 创建当前持有当前线程的读节点 reader = new Node(current, Node.SHARED); // 获取第一个读节点（可能为null） while (node != null &amp;&amp; (!node.isShared() || node.waitStatus == Node.CANCELLED)) &#123; if (node.isShared()) &#123; if (!CheckReadChainCancelled(node)) break; else node = node.next; &#125; else node = node.next; &#125; // 未找到读节点，说明队列中只有写节点，此时直接添加到队列尾 if (node == null) &#123; currentNode = Enq(Node.SHARED); &#125; // 否则说明找到了有效的读节点，此时此读节点为队列中第一个读节点，只需要判断此读节点链长是否达到了阈值， // 若未超过阈值，直接添加到读链中，并判断当前读链头是否正在读，则可以直接读，否则需要循环检测； // 若超过了阈值： // 若队尾为写节点，则添加到队列尾，并循环等待； // 若队尾为读节点，判断是否到达阈值： // 若是则添加到队列尾，并循环等待； // 否则添加到读链中。 else &#123; // 未超过阈值 if (node.readerCount &lt; Node.Threshold) &#123; // 添加到读链中 currentNode = AddReader(node, reader); // 链头正在读，此节点也直接读，且reentrants+1 if (node.waitStatus == Node.RUNNING) &#123; reader.waitStatus = Node.RUNNING; reentrants += 1; return; &#125; &#125; // 超过了阈值 else &#123; // 判断队尾节点类型 Node t = tail; // 队尾为“写节点”或“达到阈值的读节点” if (!t.isShared() || (t.isShared() &amp;&amp; t.readerCount &gt;= Node.Threshold)) currentNode = Enq(Node.SHARED); // 队尾为“未达到阈值的读节点” else &#123; // 添加到队尾节点所在的读链中 currentNode = AddReader(t, reader); // 链头正在读，此节点也直接读，且reentrants+1 if (t.waitStatus == Node.RUNNING) &#123; reader.waitStatus = Node.RUNNING; reentrants += 1; return; &#125; &#125; &#125; &#125; &#125; // 队列中无写者，说明队列中要么为空，要么全为读者，直接加到队尾或者队尾所在的读链 else &#123; // 队列为空，直接入队 if (tail == null) &#123; EnqWhenTailNull(Node.SHARED); return; &#125; // 队尾节点读链长度未达到阈值 if (tail.readerCount &lt; Node.Threshold) &#123; reader = new Node(current, Node.SHARED); // 添加到链尾 currentNode = AddReader(tail, reader); // 是否需要循环等待 if (tail.waitStatus == Node.RUNNING) &#123; reader.waitStatus = Node.RUNNING; reentrants += 1; return; &#125; &#125; // 队尾节点读链达到了阈值，直接加入队尾，并等待唤醒 else &#123; currentNode = Enq(Node.SHARED); &#125; &#125; &#125; &#125; while (currentNode.waitStatus != Node.SIGNAL) &#123; &#125; currentNode.waitStatus = Node.RUNNING; reentrants = currentNode.readerHead.readerCount; owner = currentNode.readerHead == null ? null : currentNode.readerHead.thread;&#125; 逻辑如下： 先判断队列是否已初始化，若未初始化，先初始化head节点 判断队列是否为空（即判断tail==null），若是直接进入队列，成功获取锁，返回；否则进入下一步 判断当前线程是否可重入锁（读线程的可重入判断与写不同，会在后面对ReaderCanReentranted()方法解释时介绍到），若是reentrants加1，返回；否则进入下一步 获取当前持有锁节点，若为空说明队列为空，直接进入队列，成功获取锁，返回；否则进入下一步 判断队列中是否有写者，若是跳至第6步，否则跳至第7步 队列中有写者，为防止出现写饥饿的情况，此读节点可能在写者前或后： 若队列中无读节点，即只有写节点，直接添加至队列尾，并阻塞 若找到了队列中的第一个有效读节点，判断其读链是否达到阈值 若未超过阈值，则直接添加到读链中，是否阻塞由读链头的状态决定 若超过了阈值，则有两种情况： 队尾为写节点或者队尾为读节点且读链长度达到阈值，此时节点添加至队列尾，并阻塞 队尾为读节点且读链长度未达到阈值，将节点加入队尾的读链中，与队尾节点状态一致 队列中无写者，说明队列中要么为空，要么全为读者，则根据队尾节点读链长度来决定是直接加到队尾还是加到队尾所在的读链 循环检测当前节点是否可以被唤醒，若可则解除阻塞状态，设置节点状态等信息，设置reentrants为读链长度，并成功获取共享锁 释放 ReadUnlock()/** * 释放读锁 */public void ReadUnlock()&#123; lock (this) &#123; // 读重入量-1 reentrants -= 1; // 获取到锁持有节点 Node node = head.next; while (node != null) &#123; if (CheckReadChainCancelled(node)) &#123; node = node.next; head.next = node; if (node != null) node.prev = head; &#125; else break; &#125; // 获取到有效读节点了 if (node != null) &#123; // 通过一个读节点（通常是链头）获取其读链中持有当前运行线程的节点 node = GetCurrentNodeByReader(node); // 当前节点置为取消 node.waitStatus = Node.CANCELLED; // 重入量为0，需要判断读链是否已全部读完成，若是则需要唤醒后续线程 if (reentrants == 0) AwakeNext(); &#125; &#125;&#125; 释放读锁的逻辑与释放写锁逻辑基本类似，无需多提。 其余辅助方法Enq(Node mode)以当前线程为基础，指定类型创建节点，并入队列并阻塞，同时返回此节点 /** * 读写节点入队列操作，入队列后需等待 */private Node Enq(Node mode)&#123; Node t = tail; // 以当前线程创建独占节点 Node node = new Node(Thread.CurrentThread, mode); // 连接尾节点与当前节点 t.next = node; node.prev = t; // 重置尾节点为当前节点 tail = node; // 设置节点等待状态为WAITING node.waitStatus = Node.WAITING; // 若为读节点，需要设置读链头 if (node.isShared()) node.readerHead = node; return node;&#125; EnqWhenTailNull(Node mode)以当前线程为基础，指定类型创建节点，在队列为空时，此节点直接入队列且当前线程继续执行 /** * 读写节点入队列操作，此时队列尾为空，入队列之后继续运行，无需等待 */private void EnqWhenTailNull(Node mode)&#123; // 以当前线程创建独占节点 Node node = new Node(Thread.CurrentThread, mode); // 设置等待类型为RUNNING node.waitStatus = Node.RUNNING; // 当前节点加入队列，尾节点为当前节点 tail = node; // 设置锁的持有者为当前线程 owner = Thread.CurrentThread; // 重入量初始为1 reentrants = 1; // 连接头节点与此节点 head.next = node; node.prev = head; // 若为读节点，需要设置读链头 if (node.isShared()) node.readerHead = node;&#125; AwakeNext()当某个节点置为Node.CANCELLED状态时，会唤醒其后一个有效节点，主要是将节点状态修改为Node.SIGNAL状态，以被各自节点监测到。 对于写节点来说，由于是独占的，只需要修改该节点；对于读节点来说，由于是共享的，需要遍历该节点所在的读链并修改每个节点的状态，才能达到唤醒整条读链的目的。 /** * 唤醒后一个节点 */private void AwakeNext()&#123; // 获取持有当前锁的节点，此时锁刚好被上个节点释放，获取的节点应处于Node.WAITING状态 Node node = GetHolderNode(); // 当前节点为空，说明队列中无有效节点，直接返回即可 if (node == null) &#123; tail = null; owner = null; reentrants = 0; return; &#125; // 否则需要唤醒此有效节点 else &#123; // 连接到头节点 node.prev = head; // 对于写节点，readerCount为1，即为reentrants初始值1 // 对于读节点，readerCount即为reentrants的初始值 reentrants = node.readerCount; // 对于读节点来说，需唤醒读链中的所有节点；对于写节点来说，无读链，只会唤醒当前节点 while (node != null) &#123; // 等待类型设置为SIGNAL，会被捕捉从而唤醒相关线程 node.waitStatus = Node.SIGNAL; // 若为读节点，可获取下一个读者；否则获取了null node = node.nextReader; // 下一个读者不为空，则也将其前驱节点设置为头节点 if (node != null) node.prev = head; &#125; &#125;&#125; GetHolderNode()/** * 删除无效节点（可能由于主动中断或者其他因素导致的线程失效），获取第一个有效节点或空节点， * 或者称作获取锁持有节点 */private Node GetHolderNode()&#123; Node node = head.next; // 此循环的判断条件可能出现歧义，因存在读链头为取消状态时，读链中仍然有读节点读未完成，但是此函数只在释放写锁(WriteUnlock)和唤醒后续节点(AwakeNext)中使用到，当AwakeNext被触发时，读链头的状态便可以代表整个读链的状态了。 while (node != null &amp;&amp; node.waitStatus == Node.CANCELLED) node = node.next; // 连接头节点和持有节点，无效节点全部交由垃圾收集器回收 head.next = node; if (node != null) node.prev = head; return node;&#125; HasWriter()在读节点获取锁时，有一步是需要判断队列中是否存在写者，即用到此函数 /** * 返回当前队列中是否存在写者，用在读者入队列时的决策 */private bool HasWriter()&#123; bool exclusive = false; Node excluNode = head.next; while (excluNode != null) &#123; if (!excluNode.isShared()) &#123; exclusive = true; break; &#125; excluNode = excluNode.next; &#125; return exclusive;&#125; GetCurrentNodeByReader(Node node)在释放读锁时，我们无法直接获取到当前节点，只能通过线程名称来确定当前线程的持有者在node所在的读链中的位置，返回获取到的节点，以在释放锁时修改状态。 而由于是通过线程名称来判断线程是否相等，可能会因线程重名导致异常，待改进。 /** * 通过一个读节点（通常是链头）获取其读链中持有当前运行线程的节点 */private Node GetCurrentNodeByReader(Node node)&#123; Node nextReader = node.readerHead; // 以线程名称为依据，这里可能出现多个线程同名的情况，待改进 while (nextReader != null) &#123; if (nextReader.thread.Name.Equals(Thread.CurrentThread.Name)) return nextReader; else nextReader = nextReader.nextReader; &#125; return null;&#125; CheckReadChainCancelled(Node node)对于写节点来说，只要节点状态为Node.CANCELLED，说明写已经完成；而对于读节点，需要遍历读链中所有节点的状态才能确定读是否完成，此方法便是如此。 /** * 通过一个读节点判断其所在读链是否已全部读完成 */private bool CheckReadChainCancelled(Node node)&#123; Node nextReader = node.readerHead; while (nextReader != null) &#123; if (nextReader.waitStatus != Node.CANCELLED) return false; else nextReader = nextReader.nextReader; &#125; return true;&#125; AddReader(Node readerHead, Node node)/** * 添加读者到指定读链头所在的读链中 */private Node AddReader(Node readerHead, Node node)&#123; // 获取到读链尾 Node reader = readerHead; while (reader.nextReader != null) &#123; reader = reader.nextReader; &#125; // 读链头的前驱节点也是读链中任何节点的前驱节点 node.prev = reader.prev; // 连接到读链尾 reader.nextReader = node; node.readerHead = readerHead; node.waitStatus = readerHead.waitStatus; // 读链头记录的读链长度+1 readerHead.readerCount++; return node;&#125; ReaderCanReentranted()对于来说写者，只需判断当前线程是否为持有锁线程即可判断是否可以重入；而对于读者来说，只要当前线程与正在读的读链中的任意一个线程相等即可重入。 同样线程同名的情况也不可避免。 /** * 判断读者是否可以重入，判断依据为当前线程是否被正在执行读的读链中的某一个节点持有 */private bool ReaderCanReentranted()&#123; Node node = head.next; if (node == null || !node.isShared()) return false; else &#123; Node reader = node; while (reader != null) &#123; if (reader.thread == Thread.CurrentThread) &#123; return true; &#125; reader = reader.nextReader; &#125; &#125; return false;&#125; ToString()/** * 覆盖ToString方法，格式化输出当前队列情况 */public override string ToString()&#123; StringBuilder sb = new StringBuilder(); if (head != null) &#123; if (owner != null) sb.Append(&quot;Head(持有线程：&quot; + owner.Name + &quot;，重入量：&quot; + reentrants + &quot;)\\n&quot;); else sb.Append(&quot;Head(持有线程：未持有线程，重入量：&quot; + reentrants + &quot;)\\n&quot;); Node node = head.next; while (node != null) &#123; Node reader = node; if (node.isShared()) &#123; while (reader != null) &#123; sb.AppendFormat(&quot;-&gt;【线程名称：&#123;0&#125;，类型：读，状态：&#123;1&#125;】&quot;, reader.thread.Name, Node.GetStatus(reader.waitStatus)); reader = reader.nextReader; &#125; sb.AppendLine(); &#125; else &#123; sb.AppendFormat(&quot;-&gt;【线程名称：&#123;0&#125;，类型：写，状态：&#123;1&#125;】&quot;, node.thread.Name, Node.GetStatus(node.waitStatus)); sb.AppendLine(); &#125; node = node.next; &#125; &#125; return sb.ToString();&#125; 测试测试代码： using System;using System.Threading;namespace ReadWriteLock&#123; class MainClass &#123; public static void Add(ReadWriteLock readWriteLock) &#123; for (int i = 0; i &lt; 100000000; i++) &#123; readWriteLock.WriteLock(); N++; readWriteLock.WriteUnlock(); &#125; &#125; public static void TestReentrantWriter(ReadWriteLock readWriteLock) &#123; readWriteLock.WriteLock(); readWriteLock.WriteLock(); readWriteLock.WriteLock(); readWriteLock.WriteLock(); Thread.Sleep(1000); readWriteLock.WriteUnlock(); readWriteLock.WriteUnlock(); readWriteLock.WriteUnlock(); readWriteLock.WriteUnlock(); &#125; public static void TestWriter(ReadWriteLock readWriteLock) &#123; readWriteLock.WriteLock(); Thread.Sleep(500); //Console.WriteLine(Thread.CurrentThread.Name + &quot;执行完毕&quot;); readWriteLock.WriteUnlock(); &#125; public static void TestReader(ReadWriteLock readWriteLock) &#123; readWriteLock.ReadLock(); Thread.Sleep(500); //Console.WriteLine(Thread.CurrentThread.Name + &quot;执行完毕&quot;); readWriteLock.ReadUnlock(); &#125; public static void Main(string[] args) &#123; ReadWriteLock readWriteLock = new ReadWriteLock(); for (int i = 1; i &lt;= 10; i++) CreateThread(false, i, readWriteLock); for (int i = 1; i &lt;= 20; i++) CreateThread(true, i, readWriteLock); for (int i = 0; i &lt; 100; i++) &#123; Thread.Sleep(500); readWriteLock.PrintQueue(); &#125; Console.ReadKey(); &#125; static void CreateThread(bool share, int i, ReadWriteLock readWriteLock) &#123; Thread thread; if (share) &#123; thread = new Thread(() =&gt; TestReader(readWriteLock)); thread.Name = &quot;Reader-&quot; + i; &#125; else &#123; thread = i == 5 ? new Thread(() =&gt; TestReentrantWriter(readWriteLock)) : new Thread(() =&gt; TestWriter(readWriteLock)); thread.Name = &quot;Writer-&quot; + i; &#125; thread.Start(); &#125; &#125;&#125; 首先是两个线程同时增加1亿次的时间：约30s，比Monitor大概慢了5倍。 其次，测试创建了10个写线程、20个读线程，假定每个线程持续时间为500毫秒，每隔500毫秒输出等待队列的情况如下： Head(持有线程：Writer-2，重入量：1)-&gt;【线程名称：Writer-2，类型：写，状态：RUNNING】-&gt;【线程名称：Writer-8，类型：写，状态：WAITING】-&gt;【线程名称：Reader-10，类型：读，状态：WAITING】-&gt;【线程名称：Reader-6，类型：读，状态：WAITING】-&gt;【线程名称：Reader-4，类型：读，状态：WAITING】-&gt;【线程名称：Writer-6，类型：写，状态：WAITING】-&gt;【线程名称：Writer-7，类型：写，状态：WAITING】-&gt;【线程名称：Writer-3，类型：写，状态：WAITING】-&gt;【线程名称：Writer-5，类型：写，状态：WAITING】-&gt;【线程名称：Reader-3，类型：读，状态：WAITING】-&gt;【线程名称：Reader-5，类型：读，状态：WAITING】-&gt;【线程名称：Reader-7，类型：读，状态：WAITING】-&gt;【线程名称：Reader-8，类型：读，状态：WAITING】-&gt;【线程名称：Writer-9，类型：写，状态：WAITING】-&gt;【线程名称：Reader-9，类型：读，状态：WAITING】-&gt;【线程名称：Writer-4，类型：写，状态：WAITING】-&gt;【线程名称：Writer-1，类型：写，状态：WAITING】-&gt;【线程名称：Reader-11，类型：读，状态：WAITING】-&gt;【线程名称：Reader-2，类型：读，状态：WAITING】-&gt;【线程名称：Reader-1，类型：读，状态：WAITING】-&gt;【线程名称：Reader-12，类型：读，状态：WAITING】-&gt;【线程名称：Reader-13，类型：读，状态：WAITING】-&gt;【线程名称：Reader-14，类型：读，状态：WAITING】-&gt;【线程名称：Reader-15，类型：读，状态：WAITING】-&gt;【线程名称：Reader-16，类型：读，状态：WAITING】-&gt;【线程名称：Reader-17，类型：读，状态：WAITING】-&gt;【线程名称：Reader-18，类型：读，状态：WAITING】-&gt;【线程名称：Reader-19，类型：读，状态：WAITING】-&gt;【线程名称：Reader-20，类型：读，状态：WAITING】Head(持有线程：Writer-8，重入量：1)-&gt;【线程名称：Writer-8，类型：写，状态：RUNNING】-&gt;【线程名称：Reader-10，类型：读，状态：WAITING】-&gt;【线程名称：Reader-6，类型：读，状态：WAITING】-&gt;【线程名称：Reader-4，类型：读，状态：WAITING】-&gt;【线程名称：Writer-6，类型：写，状态：WAITING】-&gt;【线程名称：Writer-7，类型：写，状态：WAITING】-&gt;【线程名称：Writer-3，类型：写，状态：WAITING】-&gt;【线程名称：Writer-5，类型：写，状态：WAITING】-&gt;【线程名称：Reader-3，类型：读，状态：WAITING】-&gt;【线程名称：Reader-5，类型：读，状态：WAITING】-&gt;【线程名称：Reader-7，类型：读，状态：WAITING】-&gt;【线程名称：Reader-8，类型：读，状态：WAITING】-&gt;【线程名称：Writer-9，类型：写，状态：WAITING】-&gt;【线程名称：Reader-9，类型：读，状态：WAITING】-&gt;【线程名称：Writer-4，类型：写，状态：WAITING】-&gt;【线程名称：Writer-1，类型：写，状态：WAITING】-&gt;【线程名称：Reader-11，类型：读，状态：WAITING】-&gt;【线程名称：Reader-2，类型：读，状态：WAITING】-&gt;【线程名称：Reader-1，类型：读，状态：WAITING】-&gt;【线程名称：Reader-12，类型：读，状态：WAITING】-&gt;【线程名称：Reader-13，类型：读，状态：WAITING】-&gt;【线程名称：Reader-14，类型：读，状态：WAITING】-&gt;【线程名称：Reader-15，类型：读，状态：WAITING】-&gt;【线程名称：Reader-16，类型：读，状态：WAITING】-&gt;【线程名称：Reader-17，类型：读，状态：WAITING】-&gt;【线程名称：Reader-18，类型：读，状态：WAITING】-&gt;【线程名称：Reader-19，类型：读，状态：WAITING】-&gt;【线程名称：Reader-20，类型：读，状态：WAITING】Head(持有线程：Reader-10，重入量：3)-&gt;【线程名称：Reader-10，类型：读，状态：RUNNING】-&gt;【线程名称：Reader-6，类型：读，状态：RUNNING】-&gt;【线程名称：Reader-4，类型：读，状态：RUNNING】-&gt;【线程名称：Writer-6，类型：写，状态：WAITING】-&gt;【线程名称：Writer-7，类型：写，状态：WAITING】-&gt;【线程名称：Writer-3，类型：写，状态：WAITING】-&gt;【线程名称：Writer-5，类型：写，状态：WAITING】-&gt;【线程名称：Reader-3，类型：读，状态：WAITING】-&gt;【线程名称：Reader-5，类型：读，状态：WAITING】-&gt;【线程名称：Reader-7，类型：读，状态：WAITING】-&gt;【线程名称：Reader-8，类型：读，状态：WAITING】-&gt;【线程名称：Writer-9，类型：写，状态：WAITING】-&gt;【线程名称：Reader-9，类型：读，状态：WAITING】-&gt;【线程名称：Writer-4，类型：写，状态：WAITING】-&gt;【线程名称：Writer-1，类型：写，状态：WAITING】-&gt;【线程名称：Reader-11，类型：读，状态：WAITING】-&gt;【线程名称：Reader-2，类型：读，状态：WAITING】-&gt;【线程名称：Reader-1，类型：读，状态：WAITING】-&gt;【线程名称：Reader-12，类型：读，状态：WAITING】-&gt;【线程名称：Reader-13，类型：读，状态：WAITING】-&gt;【线程名称：Reader-14，类型：读，状态：WAITING】-&gt;【线程名称：Reader-15，类型：读，状态：WAITING】-&gt;【线程名称：Reader-16，类型：读，状态：WAITING】-&gt;【线程名称：Reader-17，类型：读，状态：WAITING】-&gt;【线程名称：Reader-18，类型：读，状态：WAITING】-&gt;【线程名称：Reader-19，类型：读，状态：WAITING】-&gt;【线程名称：Reader-20，类型：读，状态：WAITING】Head(持有线程：Writer-6，重入量：1)-&gt;【线程名称：Writer-6，类型：写，状态：RUNNING】-&gt;【线程名称：Writer-7，类型：写，状态：WAITING】-&gt;【线程名称：Writer-3，类型：写，状态：WAITING】-&gt;【线程名称：Writer-5，类型：写，状态：WAITING】-&gt;【线程名称：Reader-3，类型：读，状态：WAITING】-&gt;【线程名称：Reader-5，类型：读，状态：WAITING】-&gt;【线程名称：Reader-7，类型：读，状态：WAITING】-&gt;【线程名称：Reader-8，类型：读，状态：WAITING】-&gt;【线程名称：Writer-9，类型：写，状态：WAITING】-&gt;【线程名称：Reader-9，类型：读，状态：WAITING】-&gt;【线程名称：Writer-4，类型：写，状态：WAITING】-&gt;【线程名称：Writer-1，类型：写，状态：WAITING】-&gt;【线程名称：Reader-11，类型：读，状态：WAITING】-&gt;【线程名称：Reader-2，类型：读，状态：WAITING】-&gt;【线程名称：Reader-1，类型：读，状态：WAITING】-&gt;【线程名称：Reader-12，类型：读，状态：WAITING】-&gt;【线程名称：Reader-13，类型：读，状态：WAITING】-&gt;【线程名称：Reader-14，类型：读，状态：WAITING】-&gt;【线程名称：Reader-15，类型：读，状态：WAITING】-&gt;【线程名称：Reader-16，类型：读，状态：WAITING】-&gt;【线程名称：Reader-17，类型：读，状态：WAITING】-&gt;【线程名称：Reader-18，类型：读，状态：WAITING】-&gt;【线程名称：Reader-19，类型：读，状态：WAITING】-&gt;【线程名称：Reader-20，类型：读，状态：WAITING】Head(持有线程：Writer-7，重入量：1)-&gt;【线程名称：Writer-7，类型：写，状态：RUNNING】-&gt;【线程名称：Writer-3，类型：写，状态：WAITING】-&gt;【线程名称：Writer-5，类型：写，状态：WAITING】-&gt;【线程名称：Reader-3，类型：读，状态：WAITING】-&gt;【线程名称：Reader-5，类型：读，状态：WAITING】-&gt;【线程名称：Reader-7，类型：读，状态：WAITING】-&gt;【线程名称：Reader-8，类型：读，状态：WAITING】-&gt;【线程名称：Writer-9，类型：写，状态：WAITING】-&gt;【线程名称：Reader-9，类型：读，状态：WAITING】-&gt;【线程名称：Writer-4，类型：写，状态：WAITING】-&gt;【线程名称：Writer-1，类型：写，状态：WAITING】-&gt;【线程名称：Reader-11，类型：读，状态：WAITING】-&gt;【线程名称：Reader-2，类型：读，状态：WAITING】-&gt;【线程名称：Reader-1，类型：读，状态：WAITING】-&gt;【线程名称：Reader-12，类型：读，状态：WAITING】-&gt;【线程名称：Reader-13，类型：读，状态：WAITING】-&gt;【线程名称：Reader-14，类型：读，状态：WAITING】-&gt;【线程名称：Reader-15，类型：读，状态：WAITING】-&gt;【线程名称：Reader-16，类型：读，状态：WAITING】-&gt;【线程名称：Reader-17，类型：读，状态：WAITING】-&gt;【线程名称：Reader-18，类型：读，状态：WAITING】-&gt;【线程名称：Reader-19，类型：读，状态：WAITING】-&gt;【线程名称：Reader-20，类型：读，状态：WAITING】Head(持有线程：Writer-3，重入量：1)-&gt;【线程名称：Writer-3，类型：写，状态：RUNNING】-&gt;【线程名称：Writer-5，类型：写，状态：WAITING】-&gt;【线程名称：Reader-3，类型：读，状态：WAITING】-&gt;【线程名称：Reader-5，类型：读，状态：WAITING】-&gt;【线程名称：Reader-7，类型：读，状态：WAITING】-&gt;【线程名称：Reader-8，类型：读，状态：WAITING】-&gt;【线程名称：Writer-9，类型：写，状态：WAITING】-&gt;【线程名称：Reader-9，类型：读，状态：WAITING】-&gt;【线程名称：Writer-4，类型：写，状态：WAITING】-&gt;【线程名称：Writer-1，类型：写，状态：WAITING】-&gt;【线程名称：Reader-11，类型：读，状态：WAITING】-&gt;【线程名称：Reader-2，类型：读，状态：WAITING】-&gt;【线程名称：Reader-1，类型：读，状态：WAITING】-&gt;【线程名称：Reader-12，类型：读，状态：WAITING】-&gt;【线程名称：Reader-13，类型：读，状态：WAITING】-&gt;【线程名称：Reader-14，类型：读，状态：WAITING】-&gt;【线程名称：Reader-15，类型：读，状态：WAITING】-&gt;【线程名称：Reader-16，类型：读，状态：WAITING】-&gt;【线程名称：Reader-17，类型：读，状态：WAITING】-&gt;【线程名称：Reader-18，类型：读，状态：WAITING】-&gt;【线程名称：Reader-19，类型：读，状态：WAITING】-&gt;【线程名称：Reader-20，类型：读，状态：WAITING】Head(持有线程：Writer-5，重入量：4)-&gt;【线程名称：Writer-5，类型：写，状态：RUNNING】-&gt;【线程名称：Reader-3，类型：读，状态：WAITING】-&gt;【线程名称：Reader-5，类型：读，状态：WAITING】-&gt;【线程名称：Reader-7，类型：读，状态：WAITING】-&gt;【线程名称：Reader-8，类型：读，状态：WAITING】-&gt;【线程名称：Writer-9，类型：写，状态：WAITING】-&gt;【线程名称：Reader-9，类型：读，状态：WAITING】-&gt;【线程名称：Writer-4，类型：写，状态：WAITING】-&gt;【线程名称：Writer-1，类型：写，状态：WAITING】-&gt;【线程名称：Reader-11，类型：读，状态：WAITING】-&gt;【线程名称：Reader-2，类型：读，状态：WAITING】-&gt;【线程名称：Reader-1，类型：读，状态：WAITING】-&gt;【线程名称：Reader-12，类型：读，状态：WAITING】-&gt;【线程名称：Reader-13，类型：读，状态：WAITING】-&gt;【线程名称：Reader-14，类型：读，状态：WAITING】-&gt;【线程名称：Reader-15，类型：读，状态：WAITING】-&gt;【线程名称：Reader-16，类型：读，状态：WAITING】-&gt;【线程名称：Reader-17，类型：读，状态：WAITING】-&gt;【线程名称：Reader-18，类型：读，状态：WAITING】-&gt;【线程名称：Reader-19，类型：读，状态：WAITING】-&gt;【线程名称：Reader-20，类型：读，状态：WAITING】Head(持有线程：Writer-5，重入量：4)-&gt;【线程名称：Writer-5，类型：写，状态：RUNNING】-&gt;【线程名称：Reader-3，类型：读，状态：WAITING】-&gt;【线程名称：Reader-5，类型：读，状态：WAITING】-&gt;【线程名称：Reader-7，类型：读，状态：WAITING】-&gt;【线程名称：Reader-8，类型：读，状态：WAITING】-&gt;【线程名称：Writer-9，类型：写，状态：WAITING】-&gt;【线程名称：Reader-9，类型：读，状态：WAITING】-&gt;【线程名称：Writer-4，类型：写，状态：WAITING】-&gt;【线程名称：Writer-1，类型：写，状态：WAITING】-&gt;【线程名称：Reader-11，类型：读，状态：WAITING】-&gt;【线程名称：Reader-2，类型：读，状态：WAITING】-&gt;【线程名称：Reader-1，类型：读，状态：WAITING】-&gt;【线程名称：Reader-12，类型：读，状态：WAITING】-&gt;【线程名称：Reader-13，类型：读，状态：WAITING】-&gt;【线程名称：Reader-14，类型：读，状态：WAITING】-&gt;【线程名称：Reader-15，类型：读，状态：WAITING】-&gt;【线程名称：Reader-16，类型：读，状态：WAITING】-&gt;【线程名称：Reader-17，类型：读，状态：WAITING】-&gt;【线程名称：Reader-18，类型：读，状态：WAITING】-&gt;【线程名称：Reader-19，类型：读，状态：WAITING】-&gt;【线程名称：Reader-20，类型：读，状态：WAITING】Head(持有线程：Reader-3，重入量：3)-&gt;【线程名称：Reader-3，类型：读，状态：RUNNING】-&gt;【线程名称：Reader-5，类型：读，状态：RUNNING】-&gt;【线程名称：Reader-7，类型：读，状态：RUNNING】-&gt;【线程名称：Reader-8，类型：读，状态：WAITING】-&gt;【线程名称：Writer-9，类型：写，状态：WAITING】-&gt;【线程名称：Reader-9，类型：读，状态：WAITING】-&gt;【线程名称：Writer-4，类型：写，状态：WAITING】-&gt;【线程名称：Writer-1，类型：写，状态：WAITING】-&gt;【线程名称：Reader-11，类型：读，状态：WAITING】-&gt;【线程名称：Reader-2，类型：读，状态：WAITING】-&gt;【线程名称：Reader-1，类型：读，状态：WAITING】-&gt;【线程名称：Reader-12，类型：读，状态：WAITING】-&gt;【线程名称：Reader-13，类型：读，状态：WAITING】-&gt;【线程名称：Reader-14，类型：读，状态：WAITING】-&gt;【线程名称：Reader-15，类型：读，状态：WAITING】-&gt;【线程名称：Reader-16，类型：读，状态：WAITING】-&gt;【线程名称：Reader-17，类型：读，状态：WAITING】-&gt;【线程名称：Reader-18，类型：读，状态：WAITING】-&gt;【线程名称：Reader-19，类型：读，状态：WAITING】-&gt;【线程名称：Reader-20，类型：读，状态：WAITING】Head(持有线程：Reader-8，重入量：1)-&gt;【线程名称：Reader-8，类型：读，状态：RUNNING】-&gt;【线程名称：Writer-9，类型：写，状态：WAITING】-&gt;【线程名称：Reader-9，类型：读，状态：WAITING】-&gt;【线程名称：Writer-4，类型：写，状态：WAITING】-&gt;【线程名称：Writer-1，类型：写，状态：WAITING】-&gt;【线程名称：Reader-11，类型：读，状态：WAITING】-&gt;【线程名称：Reader-2，类型：读，状态：WAITING】-&gt;【线程名称：Reader-1，类型：读，状态：WAITING】-&gt;【线程名称：Reader-12，类型：读，状态：WAITING】-&gt;【线程名称：Reader-13，类型：读，状态：WAITING】-&gt;【线程名称：Reader-14，类型：读，状态：WAITING】-&gt;【线程名称：Reader-15，类型：读，状态：WAITING】-&gt;【线程名称：Reader-16，类型：读，状态：WAITING】-&gt;【线程名称：Reader-17，类型：读，状态：WAITING】-&gt;【线程名称：Reader-18，类型：读，状态：WAITING】-&gt;【线程名称：Reader-19，类型：读，状态：WAITING】-&gt;【线程名称：Reader-20，类型：读，状态：WAITING】Head(持有线程：Reader-8，重入量：1)-&gt;【线程名称：Writer-9，类型：写，状态：SIGNAL】-&gt;【线程名称：Reader-9，类型：读，状态：WAITING】-&gt;【线程名称：Writer-4，类型：写，状态：WAITING】-&gt;【线程名称：Writer-1，类型：写，状态：WAITING】-&gt;【线程名称：Reader-11，类型：读，状态：WAITING】-&gt;【线程名称：Reader-2，类型：读，状态：WAITING】-&gt;【线程名称：Reader-1，类型：读，状态：WAITING】-&gt;【线程名称：Reader-12，类型：读，状态：WAITING】-&gt;【线程名称：Reader-13，类型：读，状态：WAITING】-&gt;【线程名称：Reader-14，类型：读，状态：WAITING】-&gt;【线程名称：Reader-15，类型：读，状态：WAITING】-&gt;【线程名称：Reader-16，类型：读，状态：WAITING】-&gt;【线程名称：Reader-17，类型：读，状态：WAITING】-&gt;【线程名称：Reader-18，类型：读，状态：WAITING】-&gt;【线程名称：Reader-19，类型：读，状态：WAITING】-&gt;【线程名称：Reader-20，类型：读，状态：WAITING】Head(持有线程：Writer-9，重入量：1)-&gt;【线程名称：Writer-9，类型：写，状态：RUNNING】-&gt;【线程名称：Reader-9，类型：读，状态：WAITING】-&gt;【线程名称：Writer-4，类型：写，状态：WAITING】-&gt;【线程名称：Writer-1，类型：写，状态：WAITING】-&gt;【线程名称：Reader-11，类型：读，状态：WAITING】-&gt;【线程名称：Reader-2，类型：读，状态：WAITING】-&gt;【线程名称：Reader-1，类型：读，状态：WAITING】-&gt;【线程名称：Reader-12，类型：读，状态：WAITING】-&gt;【线程名称：Reader-13，类型：读，状态：WAITING】-&gt;【线程名称：Reader-14，类型：读，状态：WAITING】-&gt;【线程名称：Reader-15，类型：读，状态：WAITING】-&gt;【线程名称：Reader-16，类型：读，状态：WAITING】-&gt;【线程名称：Reader-17，类型：读，状态：WAITING】-&gt;【线程名称：Reader-18，类型：读，状态：WAITING】-&gt;【线程名称：Reader-19，类型：读，状态：WAITING】-&gt;【线程名称：Reader-20，类型：读，状态：WAITING】Head(持有线程：Reader-9，重入量：1)-&gt;【线程名称：Reader-9，类型：读，状态：RUNNING】-&gt;【线程名称：Writer-4，类型：写，状态：WAITING】-&gt;【线程名称：Writer-1，类型：写，状态：WAITING】-&gt;【线程名称：Reader-11，类型：读，状态：WAITING】-&gt;【线程名称：Reader-2，类型：读，状态：WAITING】-&gt;【线程名称：Reader-1，类型：读，状态：WAITING】-&gt;【线程名称：Reader-12，类型：读，状态：WAITING】-&gt;【线程名称：Reader-13，类型：读，状态：WAITING】-&gt;【线程名称：Reader-14，类型：读，状态：WAITING】-&gt;【线程名称：Reader-15，类型：读，状态：WAITING】-&gt;【线程名称：Reader-16，类型：读，状态：WAITING】-&gt;【线程名称：Reader-17，类型：读，状态：WAITING】-&gt;【线程名称：Reader-18，类型：读，状态：WAITING】-&gt;【线程名称：Reader-19，类型：读，状态：WAITING】-&gt;【线程名称：Reader-20，类型：读，状态：WAITING】Head(持有线程：Writer-4，重入量：1)-&gt;【线程名称：Writer-4，类型：写，状态：RUNNING】-&gt;【线程名称：Writer-1，类型：写，状态：WAITING】-&gt;【线程名称：Reader-11，类型：读，状态：WAITING】-&gt;【线程名称：Reader-2，类型：读，状态：WAITING】-&gt;【线程名称：Reader-1，类型：读，状态：WAITING】-&gt;【线程名称：Reader-12，类型：读，状态：WAITING】-&gt;【线程名称：Reader-13，类型：读，状态：WAITING】-&gt;【线程名称：Reader-14，类型：读，状态：WAITING】-&gt;【线程名称：Reader-15，类型：读，状态：WAITING】-&gt;【线程名称：Reader-16，类型：读，状态：WAITING】-&gt;【线程名称：Reader-17，类型：读，状态：WAITING】-&gt;【线程名称：Reader-18，类型：读，状态：WAITING】-&gt;【线程名称：Reader-19，类型：读，状态：WAITING】-&gt;【线程名称：Reader-20，类型：读，状态：WAITING】Head(持有线程：Writer-1，重入量：1)-&gt;【线程名称：Writer-1，类型：写，状态：RUNNING】-&gt;【线程名称：Reader-11，类型：读，状态：WAITING】-&gt;【线程名称：Reader-2，类型：读，状态：WAITING】-&gt;【线程名称：Reader-1，类型：读，状态：WAITING】-&gt;【线程名称：Reader-12，类型：读，状态：WAITING】-&gt;【线程名称：Reader-13，类型：读，状态：WAITING】-&gt;【线程名称：Reader-14，类型：读，状态：WAITING】-&gt;【线程名称：Reader-15，类型：读，状态：WAITING】-&gt;【线程名称：Reader-16，类型：读，状态：WAITING】-&gt;【线程名称：Reader-17，类型：读，状态：WAITING】-&gt;【线程名称：Reader-18，类型：读，状态：WAITING】-&gt;【线程名称：Reader-19，类型：读，状态：WAITING】-&gt;【线程名称：Reader-20，类型：读，状态：WAITING】Head(持有线程：Reader-11，重入量：3)-&gt;【线程名称：Reader-11，类型：读，状态：RUNNING】-&gt;【线程名称：Reader-2，类型：读，状态：RUNNING】-&gt;【线程名称：Reader-1，类型：读，状态：RUNNING】-&gt;【线程名称：Reader-12，类型：读，状态：WAITING】-&gt;【线程名称：Reader-13，类型：读，状态：WAITING】-&gt;【线程名称：Reader-14，类型：读，状态：WAITING】-&gt;【线程名称：Reader-15，类型：读，状态：WAITING】-&gt;【线程名称：Reader-16，类型：读，状态：WAITING】-&gt;【线程名称：Reader-17，类型：读，状态：WAITING】-&gt;【线程名称：Reader-18，类型：读，状态：WAITING】-&gt;【线程名称：Reader-19，类型：读，状态：WAITING】-&gt;【线程名称：Reader-20，类型：读，状态：WAITING】Head(持有线程：Reader-12，重入量：3)-&gt;【线程名称：Reader-12，类型：读，状态：RUNNING】-&gt;【线程名称：Reader-13，类型：读，状态：RUNNING】-&gt;【线程名称：Reader-14，类型：读，状态：RUNNING】-&gt;【线程名称：Reader-15，类型：读，状态：WAITING】-&gt;【线程名称：Reader-16，类型：读，状态：WAITING】-&gt;【线程名称：Reader-17，类型：读，状态：WAITING】-&gt;【线程名称：Reader-18，类型：读，状态：WAITING】-&gt;【线程名称：Reader-19，类型：读，状态：WAITING】-&gt;【线程名称：Reader-20，类型：读，状态：WAITING】Head(持有线程：Reader-15，重入量：3)-&gt;【线程名称：Reader-15，类型：读，状态：RUNNING】-&gt;【线程名称：Reader-16，类型：读，状态：RUNNING】-&gt;【线程名称：Reader-17，类型：读，状态：RUNNING】-&gt;【线程名称：Reader-18，类型：读，状态：WAITING】-&gt;【线程名称：Reader-19，类型：读，状态：WAITING】-&gt;【线程名称：Reader-20，类型：读，状态：WAITING】Head(持有线程：Reader-18，重入量：3)-&gt;【线程名称：Reader-18，类型：读，状态：RUNNING】-&gt;【线程名称：Reader-19，类型：读，状态：RUNNING】-&gt;【线程名称：Reader-20，类型：读，状态：RUNNING】Head(持有线程：未持有线程，重入量：0)Head(持有线程：未持有线程，重入量：0) 如上述结果，每一行代表一个节点（读节点有不超过读链阈值的节点个数），Head开头说明打印了一次线程队列中的执行和等待情况。结果说明该读写锁可支持： 多个线程的并发读访问 多个线程的写访问 避免了写饥饿","categories":[{"name":"C#","slug":"C","permalink":"http://blog.guitoubing.top/categories/C/"}],"tags":[{"name":"C#","slug":"C","permalink":"http://blog.guitoubing.top/tags/C/"}]},{"title":"C# - .NETFramework及EntityFramework","slug":"NET-Framework及EntityFramework","date":"2019-04-18T03:07:39.000Z","updated":"2021-05-16T10:55:48.167Z","comments":true,"path":"2019/04/18/NET-Framework及EntityFramework/","link":"","permalink":"http://blog.guitoubing.top/2019/04/18/NET-Framework及EntityFramework/","excerpt":"概述EntityFramework是一种对象关系映射器ORM，它使.NET开发人员能够使用.NET对象处理数据库。它消除了开发人员通常需要编写的大多数数据访问代码的需要。","text":"概述EntityFramework是一种对象关系映射器ORM，它使.NET开发人员能够使用.NET对象处理数据库。它消除了开发人员通常需要编写的大多数数据访问代码的需要。 环境配置本次实验我使用最新的Microsoft Visual Studio Community 2019版本进行实验，环境配置过程中也是历经坎坷，在此记录如下。 VS2019连接MySQL使用VS2019连接MySQL时需要两个必备的程序： mysql-for-visualstudio-1.2.8.msi：MySQL for Visual Studio是MySQL提供给Microsoft Visual Studio的驱动，用来实现对MySQL对象和数据的访问。 mysql-connector-net-6.10.8.msi：MySQL Connector / NET能够开发需要与MySQL进行安全，高性能数据连接的.NET应用程序。它实现了所需的ADO.NET接口，并集成到ADO.NET感知工具中。 下载并安装完成之后，启动VS2019，选择工具-&gt;连接到数据库： 选择MySQL Database数据源： 填入我在ADO.NET实验中创建的数据库参数如下： 至此，关于VS2019连接MySQL的配置已经完成。 关于创建的项目类型在.NET家族中有.NET Core和.NET Framework两大平台，他们的区别如下： .NET Framework：支持Windows和Web应用程序。今天，您可以使用Windows窗体，WPF和UWP在.NET Framework中构建Windows应用程序。ASP.NET MVC用于在.NET Framework中构建Web应用程序。 .NET Core：是一种新的开源和跨平台框架，用于为包括Windows，Mac和Linux在内的所有操作系统构建应用程序。.NET Core仅支持UWP和ASP.NET Core。UWP用于构建Windows 10目标Windows和移动应用程序。ASP.NET Core用于构建基于浏览器的Web应用程序。 而在VS2019中也有两大类项目类型： .NET Core应用 .NET Framework应用 而我们本次实验一定要选择.NET Framework应用，否则无法通过EntityFramework创建实体数据模型。 创建项目这里我选择的是控制台应用(.NET Framework)： 注意最后一项框架最好选择4.6.0以上的框架，这里我选择最新版本4.7.2。 NuGet添加程序包 使用NuGet添加程序包时最要紧的就是程序包的版本问题。 注意！！！：在前面安装MySQL Connector / NET时，选择的版本是6.10.8，因此我们这里的MySql.Data和MySql.Data.Entity两个程序包的版本也需要一致为v6.10.8，否则会出现各式各样的错误，我遇到的错误有以下两个： 在1问题解决后，点击下一步会闪退。 至此环境配置已完成，下面针对EntityFramework三种模型来说明。 DB First（来自数据库的EF设计器）右键项目解决方案，选择添加-&gt;新建项： 选择ADO.NET实体数据模型，单击下一步： 选择来自数据库的EF设计器，单击下一步： 点击是，在连接字符串中包含敏感数据，单击下一步： 在表选项前打勾，单击完成： 完成后会发现，项目解决方案中出现了Model1.edmx文件，里面有两种主要的类DBContext和DBSet： DbSetDbSet对应于数据库中的表，意即每个实体表对应一个DbSet实体类，相当于数据的集合，可通过此类间接对数据库表进行ACID操作再通过DbContext关联到数据库。具体操作再介绍完DbContext后一齐举例。 DbContext DbContext类是实体框架的重要组成部分。它是您的域或实体类与数据库之间的桥梁。 DbContext是负责与数据交互作为对象的主要类。DbContext负责以下活动： EntitySet： DbContext包含映射到数据库表的所有实体的实体集（DbSet &lt;TEntity&gt;）。 查询（Querying）： DbContext将LINQ-to-Entities查询转换为SQL查询并将其发送到数据库。 更改跟踪（Change Tracking）：跟踪实体在从数据库查询后发生的更改。 持久数据（Persisting Data）：它还根据实体的状态对数据库执行插入，更新和删除操作。 缓存（Caching）： DbContext默认进行一级缓存。它存储在上下文类生命周期中已经被检索的实体。 管理关系（Manage Relationship）： DbContext还使用DB-First或Model-First方法使用CSDL，MSL和SSDL或者使用Code-First方法使用流利的API来管理关系。 对象实现（Object Materialization）： DbContext将原始表数据转换为实体对象。 下面是使用DBContext的示例： using System;using System.Collections.Generic;using System.Linq;using System.Text;using System.Threading.Tasks;namespace EF&#123; class Program &#123; static void Main(string[] args) &#123; assignment4Entities entities = new assignment4Entities(); // 增 student student = new student &#123; Id = 4, Name = &quot;Tanrui&quot;, Age = 23 &#125;; entities.student.Add(student); student = new student &#123; Id = 5, Name = &quot;TanRui&quot;, Age = 23 &#125;; entities.student.Add(student); entities.SaveChanges(); // 删 foreach (var stu in entities.student.Where(stu =&gt; stu.Name == &quot;Tanrui1&quot;)) &#123; entities.student.Remove(stu); &#125; entities.SaveChanges(); // 改 foreach ( var stu in entities.student.Where(stu =&gt; stu.Name == &quot;TanRui&quot;) ) &#123; stu.Age = 25; &#125; entities.SaveChanges(); // 查（使用Linq） var query = from s in entities.student select new &#123; Name = s.Name, Age = s.Age &#125;; foreach(var item in query) &#123; Console.WriteLine(item); &#125; Console.ReadKey(); &#125; &#125;&#125; 可通过数据库查看数据变化情况如期望一样。 Code First（空Code First模型）创建步骤和DB First类似，区别如下： 在选择实体数据模型内容时选择空Code First模型，直接点击完成即可： 点击完成后我们发现解决方案中出现了Model1.cs文件，同时查看App.Config文件发现多了下面内容： &lt;connectionStrings&gt; &lt;add name=\"Model1\" connectionString=\"data source=(LocalDb)\\MSSQLLocalDB;initial catalog=EF.Model1;integrated security=True;MultipleActiveResultSets=True;App=EntityFramework\" providerName=\"System.Data.SqlClient\" /&gt; &lt;/connectionStrings&gt; VS2019默认使用SQLserver数据库文件格式存储Code First生成的数据模型，而此处我们要使用的是MySQL数据库，因此需要更改connectionStrings参数如下： &lt;connectionStrings&gt; &lt;add name=\"Model1\" providerName=\"MySql.Data.MySqlClient\" connectionString=\"server=localhost;userid=tanrui;password=tanrui;database=assignment4;persistsecurityinfo=True\" /&gt; &lt;/connectionStrings&gt; 配置文件修改好之后，我们还需要修改Model1.cs文件，先看自动生成的内容： namespace EF&#123; using System; using System.Data.Entity; using System.Linq; [DbConfigurationType(typeof(MySql.Data.Entity.MySqlEFConfiguration))] public class Model1 : DbContext &#123; //您的上下文已配置为从您的应用程序的配置文件(App.config 或 Web.config) //使用“Model1”连接字符串。默认情况下，此连接字符串针对您的 LocalDb 实例上的 //“EF.Model1”数据库。 // //如果您想要针对其他数据库和/或数据库提供程序，请在应用程序配置文件中修改“Model1” //连接字符串。 public Model1() : base(&quot;name=Model1&quot;) &#123; &#125; //为您要在模型中包含的每种实体类型都添加 DbSet。有关配置和使用 Code First 模型 //的详细信息，请参阅 http://go.microsoft.com/fwlink/?LinkId=390109。 public virtual DbSet&lt;MyEntity&gt; MyEntities &#123; get; set; &#125; &#125; public class MyEntity &#123; public int Id &#123; get; set; &#125; public string Name &#123; get; set; &#125; &#125;&#125; VS2019为我们自动生成的Model1.cs中已经给出了最基本的Code First范例，这里为了简便起见，我们没有新建MyEntity.cs文件，而是直接在Model1.cs中声明此Model类，并在Model1类中定义对应的DbSet对象。 注意，上述代码中第6行的 [DbConfigurationType(typeof(MySql.Data.Entity.MySqlEFConfiguration))]内容需要手动添加，自动生成时不会自动添加。 至此我们假设已经创建了一个简单的逻辑表，现在需要将其同步到数据库中： 如图，打开程序包管理器控制台，输入如下代码： # 为项目启用迁移PM&gt; Enable-Migrations# 为当前设计器代码模型生成一次快照，在搭建迁移基架时会使用最近一次的快照，意即更改代码后，需要重新执行此命令PM&gt; Add-Migration AddMyEntity# 将更改迁移到数据库PM&gt; Update-Database -Verbose 可能出现的错误： 未为提供程序“MySql.Data.MySqlClient”找到任何 MigrationSqlGenerator。请在目标迁移配置类中使用 SetSqlGenerator 方法以注册其他 SQL 生成器。 原因：在Model1类定义前未手动添加[DbConfigurationType(typeof(MySql.Data.Entity.MySqlEFConfiguration))]。 当上述迁移成功完成后，就可以测试数据库是否真正生效了，测试代码如下： using System;using System.Collections.Generic;using System.Linq;using System.Text;using System.Threading.Tasks;namespace EF&#123; class Program &#123; static void Main(string[] args) &#123; Model1 model = new Model1(); MyEntity entity = new MyEntity &#123; Id = 1, Name = &quot;Tanrui&quot; &#125;; model.MyEntities.Add(entity); model.SaveChanges(); Console.ReadKey(); &#125; &#125;&#125; 运行完之后查看数据库： 可以看到，myentities表已成功添加到数据库中，一行数据也成功插入到表中了。 Tips: 注意到Code First还有个来自数据库的Code First，它实际上是根据现有的表生成对应的代码设计器，用户需要修改表结构时可通过修改设计器，而后更新迁移再更新到数据库中，个人感觉这和DB First没什么区别，而且不如DB First来的灵活。","categories":[{"name":"C#","slug":"C","permalink":"http://blog.guitoubing.top/categories/C/"}],"tags":[{"name":".NET Framework","slug":"NET-Framework","permalink":"http://blog.guitoubing.top/tags/NET-Framework/"},{"name":"EntityFramework","slug":"EntityFramework","permalink":"http://blog.guitoubing.top/tags/EntityFramework/"}]},{"title":"C# - ADO.NET配置说明","slug":"Framework","date":"2019-04-18T03:04:15.000Z","updated":"2021-05-16T10:54:47.622Z","comments":true,"path":"2019/04/18/Framework/","link":"","permalink":"http://blog.guitoubing.top/2019/04/18/Framework/","excerpt":"概述ADO.NET提供对数据库如MySQL和XML这样的数据源以及通过OLE DB和ODBC公开的数据源的一致访问。共享数据的使用方应用程序可以使用ADO.NET连接到这些数据源，并可以检索、处理和更新其中包含的数据。","text":"概述ADO.NET提供对数据库如MySQL和XML这样的数据源以及通过OLE DB和ODBC公开的数据源的一致访问。共享数据的使用方应用程序可以使用ADO.NET连接到这些数据源，并可以检索、处理和更新其中包含的数据。 安装MySQL数据库创建数据库PS A:\\&gt; cd .\\MySQL\\mysql-5.7.25-winx64\\bin\\# 安装MySQL服务PS A:\\MySQL\\mysql-5.7.25-winx64\\bin&gt; .\\mysqld install# 初始化MySQL数据库，创建无root密码的root用户PS A:\\MySQL\\mysql-5.7.25-winx64\\bin&gt; mysqld --initialize-insecure# 启动MySQL服务PS A:\\MySQL\\mysql-5.7.25-winx64\\bin&gt; net start mysqlMySQL 服务正在启动 .MySQL 服务已经启动成功。# 创建用户，设置密码并赋予权限PS A:\\MySQL\\mysql-5.7.25-winx64\\bin&gt; mysql -urootWelcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 2Server version: 5.7.25 MySQL Community Server (GPL)Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.# 创建用户并设置密码mysql&gt; create user 'tanrui'@'localhost' identified by 'tanrui';Query OK, 0 rows affected (0.00 sec)# 赋予权限mysql&gt; grant all privileges on *.* to 'tanrui'@'localhost';Query OK, 0 rows affected (0.00 sec)mysql&gt; exitBye# 以新用户登录数据库PS A:\\MySQL\\mysql-5.7.25-winx64\\bin&gt; mysql -utanrui -ptanruimysql: [Warning] Using a password on the command line interface can be insecure.Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 7Server version: 5.7.25 MySQL Community Server (GPL)Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.# 创建数据库mysql&gt; create database Assignment4;Query OK, 1 row affected (0.00 sec)mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || assignment4 || mysql || performance_schema || sys |+--------------------+5 rows in set (0.00 sec) 使用DataGrip连接数据库设置连接参数如下，点击OK完成连接： 右键assignment4数据库点击New-&gt;Table创建Student和Score新表： Visual Studio Nuget包管理本次实验需要用到MySQL一些操作，因此需要在Visual Studio中添加相应的包，如下： 打开Nuget包管理器，下载Mysql.Data和System.Data.DataSetExtensions两个包，前者提供MySQL连接驱动，后者主要是提供C#中对于数据库表操作的一些语法支持。 ADONET基础编程一个小示例using System;using System.Collections.Generic;using System.Linq;using System.Text;using System.Data;using MySql.Data.MySqlClient;namespace ADONET&#123; class Program &#123; static void Main(string[] args) &#123; Console.WriteLine(&quot;Hello World!&quot;); //ADO.NET 几个关键对象 //(1) Connection对象 用于连接数据库 //(2) Command对象 对数据源执行命令 //(3) DataReader对象 用户从数据源读取数据 //(4) DataAdapter对象 从数据源读取数据并且填充数据集对象 //(5) DataSet对象 相当于内存数据库 //(6) DataTable对象 相当于内存数据库中的表格 string connString = &quot;server = 127.0.0.1; user id = tanrui; password = tanrui; persistsecurityinfo = True; database = assignment4&quot;; MySqlConnection conn = new MySqlConnection(connString); conn.Open(); MySqlCommand cmd = new MySqlCommand(); cmd.Connection = conn; //ExecuteNonQuery的简单使用 cmd.CommandText = &quot;insert into Student values (1,&apos;zhangsan1&apos;,20)&quot;; cmd.ExecuteNonQuery(); cmd.CommandText = &quot;insert into Student values (2,&apos;zhangsan2&apos;,19)&quot;; cmd.ExecuteNonQuery(); cmd.CommandText = &quot;insert into Student values (3,&apos;zhangsan3&apos;,22)&quot;; cmd.ExecuteNonQuery(); cmd.CommandText = &quot;insert into Score values (1,80,85)&quot;; cmd.ExecuteNonQuery(); cmd.CommandText = &quot;insert into Score values (2,90,95)&quot;; cmd.ExecuteNonQuery(); cmd.CommandText = &quot;insert into Score values (3,86,75)&quot;; cmd.ExecuteNonQuery(); //参数化查询的使用，是目前唯一可以预防SQL Injection的方法 cmd.CommandText = @&quot;INSERT INTO Student (Id,Name,Age) VALUES (@Id,@Name,@Age)&quot;; cmd.Parameters.Add(new MySqlParameter(&quot;@Id&quot;, 100)); cmd.Parameters.Add(new MySqlParameter(&quot;@Name&quot;, &quot;newstudent&quot;)); cmd.Parameters.Add(new MySqlParameter(&quot;@Age&quot;, 18)); cmd.ExecuteNonQuery(); //ExecuteReader的简单使用 cmd.CommandText = &quot;select Count(*) from Student&quot;; object nRows = cmd.ExecuteScalar(); Console.WriteLine(&quot;nRows: &#123;0&#125;&quot;, nRows);cmd.CommandText = &quot;select * from Student&quot;; MySqlDataReader dr = cmd.ExecuteReader(); while(dr.Read()) &#123; Console.WriteLine(&quot;One Row: &#123;0&#125;,&#123;1&#125;,&#123;2&#125;&quot;,dr.GetValue(0), dr[&quot;Name&quot;], dr.GetValue( dr.GetOrdinal(&quot;Age&quot;) )); &#125; dr.Close(); //使用MySqlDataAdapter填充DataSet DataSet ds = new DataSet(); MySqlDataAdapter adapter = new MySqlDataAdapter(cmd); cmd.CommandText = &quot;select * from Student&quot;; adapter.Fill(ds, &quot;Student&quot;); cmd.CommandText = &quot;select * from Score&quot;; adapter.Fill(ds, &quot;Score&quot;); DataTable Student = ds.Tables[&quot;Student&quot;]; DataTable Score = ds.Tables[&quot;Score&quot;]; //LINQ与DataTable的结合 var qry = from s in Student.AsEnumerable() join c in Score.AsEnumerable() on s.Field&lt;int&gt;(&quot;Id&quot;) equals c.Field&lt;int&gt;(&quot;StudentId&quot;) where c.Field&lt;int&gt;(&quot;English&quot;) &gt; 80 select new &#123; Name = s.Field&lt;string&gt;(&quot;Name&quot;), English = c.Field&lt;int&gt;(&quot;English&quot;), Maths = c.Field&lt;int&gt;(&quot;Maths&quot;) &#125;; foreach (var item in qry) &#123; Console.WriteLine(item); &#125; //可以使用using模式进行资源的释放 conn.Close(); &#125; &#125;&#125;/* Output:Hello World!nRows: 4One Row: 1,zhangsan1,20One Row: 2,zhangsan2,19One Row: 3,zhangsan3,22One Row: 100,newstudent,18&#123; Name = zhangsan2, English = 90, Maths = 95 &#125;&#123; Name = zhangsan3, English = 86, Maths = 75 &#125;*/~ 上述例子将ADONET中几个主要对象综合到了一起，下面逐一介绍各个对象的功能及用法。 ConnectionConnection对象用于和数据库交互，若要操作数据库必须与其连接。创建连接时需要指定数据库服务器、数据库名、用户名、密码以及其他所需参数。例如本例中的connString： string connString = &quot;server = 127.0.0.1; user id = tanrui; password = tanrui; persistsecurityinfo = True; database = assignment4&quot;; 每一种数据源都有特定的Connection类，例如本例中的MySqlConnection： MySqlConnection conn = new MySqlConnection(connString); 与Java不同，C#中创建的Connection对象是使用正常的构造函数创建一个连接，而Java中使用的是单例模式创建数据库连接。 CommandCommand对象针对Connection对象指定的数据源执行SQL语句和存储过程及函数。这个对象是架构在Connection对象上的，也就是说Command对象通过Connection对象操作数据源。 同样，对于每一种数据源都有特定的Command类，例如本例中： MySqlCommand cmd = new MySqlCommand();cmd.Connection = conn; 在创建了MySqlCommand对象后还需为其指定对应的Connection对象。 增删改操作ExecuteNonQuery()方法主要用于Command对象的增删改操作。 Command对象的基本使用方法如例中所示： cmd.CommandText = &quot;insert into Student values (1,&apos;zhangsan1&apos;,20)&quot;;cmd.ExecuteNonQuery(); 先指定其CommandText即SQL语句，而后调用ExecuteNonQuery()方法完成query。 上述SQL操作方法很容易被非法分子通过字符串拼接等方式进行SQL注入攻击，我们可以使用Parameters修饰SQL语句，以防止SQL注入攻击： cmd.CommandText = @&quot;INSERT INTO Student (Id,Name,Age) VALUES (@Id,@Name,@Age)&quot;;cmd.Parameters.Add(new MySqlParameter(&quot;@Id&quot;, 100));cmd.Parameters.Add(new MySqlParameter(&quot;@Name&quot;, &quot;newstudent&quot;));cmd.Parameters.Add(new MySqlParameter(&quot;@Age&quot;, 18));cmd.ExecuteNonQuery(); 查询操作查询由于返回的结果的复杂性，一般有ExecuteScalar()和ExecuteReader()两种方法。 ExecuteScalar()主要用于SQL查询语句只返回一个数据，例如Count操作、Max操作等的返回值： cmd.CommandText = &quot;select Count(*) from Student&quot;;object nRows = cmd.ExecuteScalar();Console.WriteLine(&quot;nRows: &#123;0&#125;&quot;, nRows); ExecuteReader()返回一个DataReader对象，即结果集，下面会重点讨论该对象。 DataReaderDataReader对象是对数据源的查询结果的基于流的、仅向前的只读检索，它不会更新数据。 它是基于Command对象的，意即只能通过调用特定Command对象的ExecuteReader()方法以获取DataReader对象： MySqlDataReader dr = cmd.ExecuteReader();while(dr.Read())&#123; Console.WriteLine(&quot;One Row: &#123;0&#125;,&#123;1&#125;,&#123;2&#125;&quot;, dr.GetValue(0), dr[&quot;Name&quot;], dr.GetValue(dr.GetOrdinal(&quot;Age&quot;) ));&#125;dr.Close(); 同样，特定的数据源也是有特定类型的DataReader类，其都是通过继承DbDataReader类实现的。 由于Read()方法返回是否有下一行的布尔值，因此很适合使用while循环遍历结果。 从代码中易知，DataReader对象的Read()方法是通过循环从数据源中每次获取一行数据，类似于plsql中的游标，他除了读取效率很高之外，牺牲了很多其他特性，例如对结果的排序、更改等。 DataSet和DataAdapterDataSet类包含数据的数据表集合。它用于在不与数据源交互的情况下获取数据，这就是为什么它也被称为断开数据访问方法。这是一个内存数据存储，可以同时容纳多个表。可以使用DataRelation对象来关联这些表。 DataSet也可以用来读写XML文档中的数据。 DataAdapter是ADO.NET数据提供程序的一部分。DataAdapter提供数据集和数据源之间的通信。我们可以将DataAdapter与DataSet对象结合使用。注意DataAdapter类也是各个数据源有各自的实现方法。 DataAdapter通过映射Fill()方法提供此组合，该方法更改DataSet中的数据以匹配数据源中的数据。也就是说，这两个对象组合起来以实现数据访问和数据操作功能： DataSet ds = new DataSet();SqlDataAdapter adapter = new SqlDataAdapter(cmd); cmd.CommandText = &quot;select * from Student&quot;; adapter.Fill(ds, &quot;Student&quot;);cmd.CommandText = &quot;select * from Score&quot;; adapter.Fill(ds, &quot;Score&quot;);DataTable Student = ds.Tables[&quot;Student&quot;]; DataTable Score = ds.Tables[&quot;Score&quot;]; 通过代码，我们可以看到DataSet是独立于数据源的数据集，即对于任何数据源，都提供一致的关系编程模型。 相比于DataReader，DataSet一次性将所有数据放入内存中，同时还提供了很多额外的数据集操作方法，因此速度很快且很方便，但是对内存资源会有很大的消耗。 DataTableDataTable类将关系数据表示为表格形式。ADO.NET提供了一个DataTable类来独立创建和使用数据表。一般与DataSet一起使用。 在创建DataTable之前，必须包含System.Data名称空间。同时由于本例中使用了AsEnumerable()方法，因此还必须使用Nuget添加System.Data.DataSetExtensions包，否则会报错。 我们可以使用LINQ语法对DataTable进行查询，实现代码如下： var qry = from s in Student.AsEnumerable() join c in Score.AsEnumerable() on s.Field&lt;int&gt;(&quot;Id&quot;) equals c.Field&lt;int&gt;(&quot;StudentId&quot;) where c.Field&lt;int&gt;(&quot;English&quot;) &gt; 80 select new &#123; Name = s.Field&lt;string&gt;(&quot;Name&quot;), English = c.Field&lt;int&gt;(&quot;English&quot;), Maths = c.Field&lt;int&gt;(&quot;Maths&quot;) &#125;;foreach (var item in qry)&#123; Console.WriteLine(item);&#125;/* Output:&#123; Name = zhangsan2, English = 90, Maths = 95 &#125;&#123; Name = zhangsan3, English = 86, Maths = 75 &#125;**//~","categories":[{"name":"C#","slug":"C","permalink":"http://blog.guitoubing.top/categories/C/"}],"tags":[{"name":".NET","slug":"NET","permalink":"http://blog.guitoubing.top/tags/NET/"},{"name":"ADO.NET","slug":"ADO-NET","permalink":"http://blog.guitoubing.top/tags/ADO-NET/"}]},{"title":"Java - 深入垃圾回收","slug":"深入GC","date":"2019-04-11T02:36:03.000Z","updated":"2021-05-16T10:42:46.893Z","comments":true,"path":"2019/04/11/深入GC/","link":"","permalink":"http://blog.guitoubing.top/2019/04/11/深入GC/","excerpt":"可达性分析算法算法的基本思路是通过一系列称为GC Roots的对象作为起始点，从这些节点向下搜索，搜索所走过的路径称作引用链。","text":"可达性分析算法算法的基本思路是通过一系列称为GC Roots的对象作为起始点，从这些节点向下搜索，搜索所走过的路径称作引用链。 可作为GC Roots的对象包括下面几种： 虚拟机栈(栈帧中的本地变量表)中引用的对象 方法区中类静态属性引用的对象 方法去中常量引用的对象 本地方法栈中JNI引用的对象 引用强引用程序代码中普遍存在的，类似于Object obj = new Object();这类的引用，只要强引用还存在，GC永远不会回收掉被引用的对象。 软引用用来描述一些还有用但并非必需的对象。对于软引用关联着的对象，在系统将要发生内存溢出异常之前，将会把这些对象列进回收范围之中进行第二次回收。可用SoftReference类来实现。 弱引用比软引用强度低，被弱引用关联的对象只能生存到下一次垃圾收集发生之前。可用WeakReference类来实现。 虚引用最弱的引用。一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用来取得一个对象实例。 再谈finalize()即使在上述可达性分析算法中不可达的对象，也并非是非死不可的。 过程要真正宣告一个对象死亡，至少要经历两次标记过程： 如果对象在进行可达性分析后没有与GC Roots相连接的引用链 进行筛选，条件是对象是否有必要执行finalize()方法。当对象没有覆盖finalize()方法，或者finalize()方法以及被调用过，则虚拟机认定没有必要执行，此时才宣判对象已死 再生当有必要执行finalize()方法时，则对象就会有拯救自己的机会，如下： import java.util.concurrent.TimeUnit;public class FinalizeEscapeGC &#123; public static FinalizeEscapeGC SAVE_HOOK = null; public void isAlive() &#123; System.out.println(\"yes, i am still alive.\"); &#125; @Override protected void finalize() throws Throwable &#123; super.finalize(); System.out.println(\"finalize method executed!\"); FinalizeEscapeGC.SAVE_HOOK = this; &#125; public static void main(String[] args) throws InterruptedException &#123; SAVE_HOOK = new FinalizeEscapeGC(); // 对象第一次成功拯救自己 SAVE_HOOK = null; System.gc(); // 因为finalize方法优先级很低，所以暂停0.5s以等待它 TimeUnit.MILLISECONDS.sleep(500); if (SAVE_HOOK != null)&#123; SAVE_HOOK.isAlive(); &#125; else &#123; System.out.println(\"no, i am dead!\"); &#125; // 下面的代码与上面完全相同，但是此次自救却失败了 SAVE_HOOK = null; System.gc(); TimeUnit.MILLISECONDS.sleep(500); if (SAVE_HOOK != null)&#123; SAVE_HOOK.isAlive(); &#125; else &#123; System.out.println(\"no, i am dead!\"); &#125; &#125;&#125;/**Output:finalize method executed!yes, i am still alive.no, i am dead!**/. 从上述代码及其结果可看到，SAVE_HOOK对象的finalize()方法确实被GC触发过，其本身也在垃圾收集之前成功逃脱了。但是注意，由于一个对象的finalize()只会被执行一遍，因此上述代码中第二次将逃脱失败，无法完成自救。 HotSpot的算法实现 关于GC的几种主流实现方法（简单记忆）： 保守式GC(Conservative GC)：指JVM不记录内存上的某个数据应该被解读为引用类型还是其他类型。 半保守式GC(Conservative with respect to the roots)：让对象带有足够的元数据 准确式GC(Exact GC)：提供特定数据结构保存对象引用 枚举根节点枚举根节点这一过程是必须要停顿所有Java执行线程，即Stop The World。因为要保证这段时间的引用不变性。 Java中使用OopMap来存储对象引用，以实现准确性GC，同时也避免了垃圾回收时需要遍历栈的每个位置。 安全点 SafepointHotspot虚拟机只在到达Safepoint位置暂停，以进行GC。 程序中指令序列复用的指令，例如方法调用、循环跳转、异常跳转等情况，才会产生Safepoint。 在多线程中，有两种中断方案可供选择： 抢先式中断：GC发生时，将所有线程中断，而后让不在安全点上的线程恢复，直到跑到安全点。 主动式中断：设置一个标志，各个线程主动轮询这个标志，发现中断标志为真时就自己中断挂起。 安全区域 Safe Region安全区域是为了解决程序不执行的时候，程序无法进入安全点的情况，例如线程处于Sleep或者Blocked状态时。 安全区域指的是一段代码片段之中，引用关系不会发生变化，因此在这个区域的任何地方开始GC都是安全的。 当线程执行到安全区域的代码中时： 首先标识自己已经进入安全区域，此时JVM发起GC时就无需询问处于安全区域状态的线程了，直接回收 在线程要离开安全区域时，需要检查JVM是否已经完成了根节点枚举，如果完成了则线程继续执行，否则必须要等待直到收到可以安全离开安全区域的信号为止。 垃圾收集器Serial收集器Serial收集器是一个单线程的收集器，它只会使用一个CPU或一条收集线程去完成垃圾收集工作，同时它进行垃圾收集时，必须暂停其他所有工作线程。 ParNew收集器Serial收集器的多线程版本。 目前只有Serial和ParNew能够与CMS收集器配合工作。 Parallel Scavenge收集器此收集器的侧重点放在吞吐量上，吞吐量就是CPU用于运行用户代码与CPU总消耗时间的比值，即吞吐量=运行用户代码时间/(运行用户代码时间+垃圾收集时间)。 注意，吞吐量与垃圾收集速度无太大关系。 同时采用此类收集器的虚拟机可根据系统运行状况手机性能监控信息，动态调整参数以提供最合适的停顿时间或最大的吞吐量。 Serial Old收集器Serial收集器的老年代版本，两大用途： 与Parallel Scavenge收集器搭配使用 作为CMS收集器的后备预案，在并发收集发生Concurrent Mode Failure时使用。 Parallel Old收集器Parallel Scavenge收集器的老年代版本。 CMS收集器CMS收集器是一种以获取最短回收停顿时间为目标的收集器。 收集过程CMS收集器收集过程分为4个步骤： 初始标记：需要Stop The World，标记GC Roots能直接关联到的对象，速度很快。 并发标记：不需要Stop The World，进行GC Roots Tracing。 重新标记：需要Stop The World，标记因用户程序继续运作而导致变动的那一部分对象的标记记录。 并发清除：不需要Stop The World，进行清除。 缺点 对CPU资源很敏感，当CPU资源紧张时，用户程序速度下降很明显。 无法处理浮动垃圾，即在标记之后出现的垃圾，只能留到下一次GC时再清理掉。同时使用CMS时，由于需要预留空间给用户线程，因此不能等到老年代几乎全部被填满了再进行收集。此时当CMS预留的内存无法满足程序需要，就会出现一次Concurrent Mode Failure失败，这是就使用后备收集器Serial Old。 标记-清除算法会产生空间碎片。 G1收集器(Garbage First)特点 并行和并发 分代收集 空间整合 可预测的停顿 G1收集器中新生代和老年代不再是物理隔离的，它将整个Java堆划分为多个大小相等的独立区域(Region)。 由于Region之间可能存在相互引用的关系，所以使用Remembered Set来记录从其他Region引用当前Region的引用信息，Remembered Set是一种抽象概念，Card Table是其一种实现方式。 实际上，G1相关算法是个很复杂的过程，见R大的帖子，需要进一步研究。 收集过程 初始标记 并发标记 最终标记 筛选回收 内存分配与回收策略对象优先在Eden分配大多数情况下，对象在新生代Eden区中分配，当Eden区没有足够空间进行分配时，虚拟机发起一次MinorGC。 大对象直接进入老年代大对象指的是需要大量连续内存空间的Java对象，例如很长的字符串以及数组。更糟糕的是产生一群朝生夕灭的短命大对象。 长期存活的对象将进入老年代虚拟机给每个对象定义了一个对象年龄计数器，当年龄增长到阈值时，就可以晋升到老年代。阈值默认为15，也可通过MaxTenuringThredhold参数设置。 动态对象年龄判断当Survivor空间中相同年龄所有对象大小总和大于Survivor空间的一半，年龄大于等于该年龄的对象就直接进入老年代。","categories":[{"name":"Java","slug":"Java","permalink":"http://blog.guitoubing.top/categories/Java/"}],"tags":[{"name":"垃圾回收","slug":"垃圾回收","permalink":"http://blog.guitoubing.top/tags/垃圾回收/"},{"name":"可达性分析","slug":"可达性分析","permalink":"http://blog.guitoubing.top/tags/可达性分析/"},{"name":"HotSpot","slug":"HotSpot","permalink":"http://blog.guitoubing.top/tags/HotSpot/"}]},{"title":"Java - Java类与类加载器","slug":"Java类与类加载器","date":"2019-03-27T15:54:46.000Z","updated":"2021-05-16T10:55:21.675Z","comments":true,"path":"2019/03/27/Java类与类加载器/","link":"","permalink":"http://blog.guitoubing.top/2019/03/27/Java类与类加载器/","excerpt":"类加载机制类加载机制个人认为是JVM中比较重要的一部分，因此在JVM系统学习之前就先学习了类加载机制的相关细节，以记之。","text":"类加载机制类加载机制个人认为是JVM中比较重要的一部分，因此在JVM系统学习之前就先学习了类加载机制的相关细节，以记之。 阶段 其中解析可能会发生在初始化之后，使用可能不会被使用。 上述流程指的是开始时间的顺序，比如说加载未结束可能验证就会开始。 类加载时机虚拟机严格规定了5种情况必须立即对类进行初始化(不是上述流程中的初始化，指的是初始化类对象)： 遇到new、getstatic、putstatic、invokestatic这4条字节码指令时，如果类没有进行初始化则需要先触发其初始化。 对类进行反射调用； 当初始化一个类时，若父类还没有被初始化需要先触发其父类的初始化； 当虚拟机启动时，包含main()方法的那个类需要被初始化； 当使用动态语言支持时，如果一个java.lang.invoke.MethodHandle实例最后的解析结果是REF_getStatic、REF_putStatic、REF_invokeStatic的方法句柄，并且句柄对应的类没有被初始化。 不会触发类的初始化的可能操作： 通过子类调用父类的静态字段，不会导致子类初始化 通过数组定义来引用类，不会触发该类的初始化 引用类的静态常量域或字段，不会导致该类的初始化 注意，接口也是会有初始化的过程，与类唯一不同的是上述第3点：接口在初始化时，并不要求其父接口全部都完成了初始化(原因应该是接口&lt;clinit&gt;()方法不需要调用父类的&lt;clinit&gt;()方法)，只有在真正使用到父接口的时候(如引用接口中定义的常量时)才会初始化。 加载加载阶段的3件事情： 通过一个类的全限定名来获取定义此类的二进制字节流 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构 在内存中生成一个代表这个类的Class对象，作为方法区这个类的各种数据的访问入口 第1件事情中的二进制字节流不一定是本地文件，可能是从ZIP获取、从网络获取(Applet)、动态代理、JSP生成、数据库读取等。 验证验证主要是为了虚拟机对自身保护的一项重要工作，大致会完成以下4个阶段的检验动作： 文件格式验证：检测字节流是否符合Class文件格式规范 元数据验证：语义分析，保证信息符合Java语言规范的要求，主要是数据类型 字节码验证：最复杂的一部分，主要是对类的方法体进行校验(控制流、跳转等) 符号引用验证：发生在解析阶段，主要是对符号引用进行匹配性校验(能否找到、是否可达等) 准备准备阶段是为类变量(静态变量)分配初始值的过程。 注意两点： 初始值通常情况下是数据类型的零值，比如语句public static int value = 123;会在准备阶段给value初始化为int的零值即0，而123会在后续的初始化阶段被赋值给value； 特殊情况下，常量类型会在准备阶段被赋值，比如语句public static final int value = 123; 解析解析阶段是将常量池内的符号引用替换为直接引用的过程。 符号引用是指以一组符号来描述所引用的目标，符号引用在使用时能无歧义地定位到目标。 直接引用是指可以直接指向目标的指针、相对偏移量或一个句柄。 初始化正式开始执行类中定义的Java代码(或者说是字节码)。记得准备阶段有为变量赋予初始值，这里就会为其赋予程序中制定的初始值。 初始化主要的过程是执行&lt;clinit&gt;()方法。 类与类加载器对于任意一个类，都需要由加载它的类加载器和类本身一同确立其在JVM中的唯一性。 在使用instanceof关键字、Class对象的equal()、isAssignableFrom()、isInstance()方法时，都需要判定上述两方面是否相等。自定义的类加载器和系统自带的类加载器加载的同一个类生成的对象使用相等方法验证是得不到相等结果的。 双亲委派模型类加载器划分： 启动类加载器：负责将&lt;JAVA_HOME&gt;\\lib目录下的能被虚拟机识别的类库加载到虚拟机内存中，程序无法直接引用。 扩展类加载器：负责将&lt;JAVA_HOME&gt;\\lib\\ext目录下的能被虚拟机识别的类库加载到虚拟机内存中，程序可直接使用。 应用程序类加载器：负责加载用户类路径(ClassPath)上的类库，程序可直接使用。 双亲委派模型如下图所示： 其中每一层与其父层关系一般不是继承(Inheritance)而是组合(Composition)来复用父加载器的代码。 工作过程：如果一个类加载器收到了类加载的请求，它首先不会尝试加载这个类，而是把这个请求委派给父类加载器去加载，每个层次都是这样，直到请求被传递到顶层的启动类加载器中；而只有父加载器反馈自己无法完成此请求时，子加载器才回去尝试加载。 双亲委托模型在ClassLoader类中的loadClass()方法中实现。","categories":[{"name":"Java","slug":"Java","permalink":"http://blog.guitoubing.top/categories/Java/"}],"tags":[{"name":"ClassLoader","slug":"ClassLoader","permalink":"http://blog.guitoubing.top/tags/ClassLoader/"}]},{"title":"Java - 基础知识","slug":"深入学习Java（更新中）","date":"2019-03-06T13:01:30.000Z","updated":"2021-05-16T10:42:02.363Z","comments":true,"path":"2019/03/06/深入学习Java（更新中）/","link":"","permalink":"http://blog.guitoubing.top/2019/03/06/深入学习Java（更新中）/","excerpt":"垃圾回收器—GC众所周知，Java中的GC负责回收无用对象占用的内存资源，但会有特殊情况：假定对象获得了一块”特殊”的内存区域（不是使用new创建的），由于GC只释放那些经由new分配的内存，所以GC不知道如何释放该对象的这块”特殊”内存区域。","text":"垃圾回收器—GC众所周知，Java中的GC负责回收无用对象占用的内存资源，但会有特殊情况：假定对象获得了一块”特殊”的内存区域（不是使用new创建的），由于GC只释放那些经由new分配的内存，所以GC不知道如何释放该对象的这块”特殊”内存区域。 作为应对，Java允许在类中定义finalize()方法，它使得在GC回收该对象内存之前先调用finalize()方法，并在下一次GC回收发生时，真正回收对象内存。举个例子：某个对象创建时会在屏幕上绘出一些图像，当没有明确将其从屏幕擦除时，图像便可能会永远存在在屏幕上，若在finalize()指定擦除的方法，那么在GC回收该对象时将会同时将其图像从屏幕上擦除。 关键点： 对象可能不被垃圾回收 垃圾回收并不等于”析构” 垃圾回收只与内存有关 避免使用finalize() “终结函数无法预料，常常是危险的，总之是多余的。”《Effective Java》，第20页 在Java中一切皆为对象，且创建对象的方法只有new，那么必然存在通过某种创建对象以外的方式为对象分配了存储空间。 Native Method(本地方法)是Java中调用非Java代码的方式，此时非Java代码中可能使用了malloc()等分配内存的函数而未使用free()对其释放，此时GC也不会去管这块内存，这就使得需要指定特定的finalize()方法来实现内存的释放。 可见，finalize()不是进行普遍的清理工作的合适方式，因此需要避免使用。 终结条件的验证但是finalize()有个有趣的用法——终结条件。看如下代码： class Book&#123; // Book类，约定其在被回收前必须被签入。 boolean checkedOut = false; Book(boolean checkedOut)&#123; checkedOut = checkedOut; &#125; void checkIn()&#123; checkedOut = false; &#125; protected void finalize()&#123; // 终结条件，对象未被签入 if (checkedOut) &#123; System.out.println(\"Error: checked out\"); &#125; &#125;&#125;public class Main&#123; public static void main(String[] args)&#123; // 创建一个Book对象-novel Book novel = new Book(true); // 将其签入 novel.checkIn(); // 创建一个Book对象，此时该对象未被签入 new Book(true); // 强制执行垃圾回收，此时会先执行finalize System.gc(); &#125;&#125;/* 输出：Error: checked out*/ 我们约定所有的Book对象在创建之前都必须被签入，但是在main中，由于疏忽有个新创建的对象未执行签入操作，此时执行垃圾回收，finalize()中的终结条件被激活，把错误反馈给使用者。 注意这里使用的System.gc()强制调用垃圾回收器 若没有finalize()将很难实现这种操作。 GC如何工作引用计数（未被使用过）对象创建时便有引用计数，当引用计数变为0时，GC回收该对象内存空间。 缺陷：循环引用不适用，即出现”对象应该被回收，但引用计数不为0”的情况，称作”交互自引用的对象组”。如下所示： public class Main &#123; public static void main(String[] args) &#123; // object1指向的对象引用计数器：1 MyObject object1 = new MyObject(); // object2指向的对象引用计数器：1 MyObject object2 = new MyObject(); // object1指向的对象引用计数器：2 object1.object = object2; // object2指向的对象引用计数器：2 object2.object = object1; // object1指向的对象引用计数器减少为1 object1 = null; // object2指向的对象引用计数器减少为1 object2 = null; &#125;&#125; 我们将object1和object2赋值为null，意即我们已经不需要该对象，但由于此时对象的引用计数器不为0导致这两个对象永远不会被回收。 停止-复制（stop-and-copy）遍历所有引用找到所有”活”的对象，将堆中所有存活的对象复制到另一个堆中，没有被复制的便都是垃圾了。 这种策略避免了上述”交互自引用的对象组”无法回收的情况，因为这两个对象不会被看作是存活的对象，即遍历的过程中根本找不到这两个对象（他们不在从GC Root出发连接所有存活结点构成的图中）。 缺陷：效率低 复制需要在两个堆之间操作，即需要维护多一倍的空间； 当程序进入稳定状态之后，可能只产生少量垃圾，此时此策略仍然需要进行复制操作，很浪费。 针对第2个情况，有另外一种策略，如下。 标记-清扫（mark-and-sweep）同样遍历所有引用找到所有”活”的对象，同时会给该对象进行标记，当全部标记工作完成后，开始进行清理工作。没有被标记的对象将会被释放，因此剩下的堆空间是不连续的，此时GC需要使用其他整理的方法来清理内存碎片，称作”标记-整理”。 注意，上面两种垃圾回收机制都不是在后台进行的，意即进行垃圾回收时会暂停程序。 许多文献中有关于”垃圾回收器是低优先级的后台进程”的说法，事实上早期版本的JVM使用这两种策略时并非如此。当可用内存不足时，垃圾回收器会暂停运行程序，而后开展”停止-复制”或”标记-清扫”工作。 “标记-清扫”方式速度相当慢，但是当垃圾很少时，就很快了。 自适应技术JVM会进行监视，如果所有对象都很稳定，GC的效率降低的话，就切换到”标记-清扫”方式；同样，JVM也会跟踪”标记-清扫”方式，若堆空间出现很多碎片，就会切换回”停止-复制”方式。这就是自适应技术。 这是早期Sun版本的垃圾回收器。 分代垃圾收集（Generational Garbage Collection）上述无论是”停止-复制”、”标记-清扫”还是”标记-整理”对于日益增长的对象列表，效率会逐渐低下。 堆被分为三代： 年轻代(Young Generation) 内存空间：eden:S0:S1 = 8:1:1 S0和S1没有先后顺序，任何一个都可能是From survivor space和To survivor space 年老代(Old Generation) 内存空间：年老代:年轻代 ≈ 2:1 持久代(Permanent Generation) 用于存放静态文件，如Java类、方法等。持久代对垃圾回收没有显著影响。有些应用可能动态生成或者调用一些class，例如Hibernate等，在这种时候需要设置一个比较大的持久代空间来存放这些运行过程中新增的类。 下面说明一下对象在分配内存、老化、回收的过程： 首先，任何新对象创建时内存都会分配在年轻代的eden space中，S0和S1两个幸存者空间(survivor space)起初都是空的 当eden space满时，会触发第一次较小的垃圾回收过程(minor garbage collection，minor GC)\u0001 实际上MinorGC不一定要等到eden space满了才触发 eden space中所有存活对象(referenced objects)被复制到S0，其余对象(unreferenced objects)被视作垃圾，随eden space一起被回收 当下一次minor GC被触发时，eden space执行与第3点中相同的步骤，不过此时存活对象会被复制到S1，同时S0中的存活对象也会被复制到S1，此时S0和eden space都被回收。注意到此时S1有不同老化程度的对象\u0001 再当下一次minor GC被触发时，重复上述操作，幸存者空间变为S0，eden和S1中的存活对象都被复制到S0，同时老化，此时S1和eden space都被回收 当minor GC持续触发到对象老化程度达到一个阈值(此处为8)时，这些对象从年轻代提升到年老代 以上过程涵盖了整个年轻代老化的过程，最终，会在年老代触发完全的垃圾回收(major gabarge collector, major GC)，清理并压缩该块内存空间。 major GC被触发的原因： 年老代（Tenured）被写满 持久代（Permanent）被写满 System.gc()被显式调用 上一次GC之后Heap的各域分配策略动态变化 HotSpot JVM的垃圾收集器Serial收集器（复制算法)：新生代单线程收集器，标记和清理都是单线程，优点是简单高效。 Serial Old收集器(标记-整理算法)：老年代单线程收集器，Serial收集器的老年代版本。 ParNew收集器(停止-复制算法)：新生代收集器，可以认为是Serial收集器的多线程版本,在多核CPU环境下有着比Serial更好的表现。 Parallel Scavenge收集器(停止-复制算法)：并行收集器，追求高吞吐量，高效利用CPU。吞吐量一般为99%， 吞吐量= 用户线程时间/(用户线程时间+GC线程时间)。适合后台应用等对交互相应要求不高的场景。 Parallel Old收集器(停止-复制算法)：Parallel Scavenge收集器的老年代版本，并行收集器，吞吐量优先 CMS(Concurrent Mark Sweep)收集器(标记-清扫算法)：高并发、低停顿，追求最短GC回收停顿时间，cpu占用比较高，响应时间快，停顿时间短，多核cpu 追求高响应时间的选择 【参考：深入理解JVM(3)——7种垃圾收集器】 可变参数列表Java中的可变参数列表（JSE5之后）的使用与C的使用类似，如下： public class test&#123; public static void main(String[] args)&#123; Integer a = 1; Integer b = 2; Integer c = 3; Other.main(a, b); Other.main(a, b, c); Other.main(); Other.main(new Object[]&#123;a, b&#125;); Other.main(new Object[]&#123;a, b, c&#125;); &#125;&#125;class Other&#123; public static void main(Object... args)&#123; for (Object s : args)&#123; System.out.println(s + \" \"); &#125; &#125;&#125; 如上所示，当输入不同个数参数时，编译器会自动将其转换成数组，当参数本身就是数组时，编译器又不会进行转换，直接传递给函数。参数为空时编译器便直接传递一个空Object数组。 可变参数列表的重载public class test&#123; static void f(Character... args)&#123; System.out.println(\"first\"); &#125; static void f(String... args)&#123; System.out.println(\"second\"); &#125; public static void main(String[] args)&#123; f('a', 'b'); f(\"a\", \"b\"); f(); &#125;&#125; 如上，函数有f(Character... args)和f(String... args)两种重载方式，此时f(&#39;a&#39;, &#39;b&#39;)和f(&quot;a&quot;, &quot;b&quot;)都可正常调用，但是f()会报错，即两种重载都匹配。 此时可通过为其中一个重载函数添加一个非可变参数（可变参数必须位于参数列表最后）。但这样又会产生新的问题，如下： public class test&#123; static void f(float i, Character... args)&#123; System.out.println(\"first\"); &#125; static void f(Character... args)&#123; System.out.println(\"second\"); &#125; public static void main(String[] args)&#123; f(1, 'a'); f('a', 'b'); &#125;&#125; 如上，编译器也会报错，f(&#39;a&#39;, &#39;b&#39;)可匹配两个函数，(可能是)因为char类型可提升至float类型从而匹配第一个重载函数。 此时可为第二个重载函数也添加一个非可变参数，问题可得到解决。 public class test&#123; static void f(float i, Character... args)&#123; System.out.println(\"first\"); &#125; static void f(char i, Character... args)&#123; System.out.println(\"second\"); &#125; public static void main(String[] args)&#123; f(1, 'a'); f('a', 'b'); &#125;&#125; 这种用法比较奇怪，因此”你应该总是只在重载方法的一个版本上使用可变参数列表，或者压根就不是用它”（《Java编程思想》105页）。 内部类内部类对象对外围类对象的访问当外围类对象创建了一个内部类对象时，此内部类对象必定会秘密地捕获一个指向外围类对象的引用，因此内部类对象可以访问外部类对象的所有成员。 interface Selector &#123; boolean end(); Object current(); void next();&#125;public class Sequence &#123; private Object[] items; private int next = 0; public Sequence(int size) &#123; items = new Object[size]; &#125; public void add(Object x)&#123; if (next &lt; items.length)&#123; items[next++] = x; &#125; &#125; private class SequenceSelector implements Selector &#123; private int i = 0; public boolean end() &#123; return i == items.length; &#125; public Object current() &#123; return items[i]; &#125; public void next() &#123; if(i &lt; items.length) i++; &#125; &#125; public Selector selector()&#123; return new SequenceSelector(); &#125; public static void main(String[] args)&#123; Sequence sequence = new Sequence(10); for (int i = 0; i &lt; 10; i++)&#123; sequence.add(Integer.toString(i)); &#125; Selector selector = sequence.selector(); while(!selector.end())&#123; System.out.print(selector.current() + \" \"); selector.next(); &#125; &#125;&#125; Sequence中的内部类SequenceSelector可以访问Sequence的全部成员，就像SequenceSelector自己拥有这些成员一样。 内部类与静态内部类（嵌套类）创建方法// 内部类：DotNew.javapublic class DotNew &#123; class Inner &#123; Inner()&#123; System.out.println(\"创建内部类\"); &#125; &#125; public static void main(String [] args)&#123; DotNew dn = new DotNew(); DotNew.Inner dni = dn.new Inner(); &#125;&#125;// 静态内部类：DotNewStatic.javapublic class DotNewStatic &#123; static class Inner &#123; Inner() &#123; System.out.println(\"创建静态内部类\"); &#125; &#125; public static void main(String[] args)&#123; DotNewStatic.Inner inner = new DotNewStatic.Inner(); &#125;&#125; 匿名内部类Java支持创建一个继承自某基类的匿名类的对象，通过new表达式返回的引用被自动向上转型为对基类的引用。 匿名内部类可以使用默认构造器生成，也可以使用有参数的构造器。 注意，在匿名内部类中若想使用外部定义的对象，该外部对象的参数引用必须是final，如下： // Destination.javapublic interface Destination&#123; String readLabel();&#125;// Parcel9.javapublic class Parcel9 &#123; public Destination destination(final String dest)&#123;// 外部变量dest被引用时需声明为final，否则产生编译时错误 return new Destination()&#123; private String label = dest; @Override public String readLabel() &#123; return label; &#125; &#125;; &#125; public static void main(String[] args)&#123; Parcel9 p = new Parcel9(); Destination d = p.destination(\"Tasmania\"); System.out.println(d.readLabel()); &#125;&#125; 但是我使用的Java 10中，当dest不声明为final时也不会报错，虽然不会报错，但是当更改dest引用时会报前面所述的编译时错误（Local variable dest defined in an enclosing scope must be final or effectively final）。 为什么匿名内部类访问外部变量必须是final的？ 为了避免外部方法修改引用导致内部类得到的引用值不一致和内部类修改引用而导致外部方法的参数值在修改前和修改后不一致 保证回调函数回调时可访问到变量（待研究） 反编译查看其实现细节： &gt; // 源代码&gt; public interface MyInterface &#123;&gt; void doSomething();&gt; &#125;&gt; public class TryUsingAnonymousClass &#123;&gt; public void useMyInterface() &#123;&gt; final Integer number = 123;&gt; System.out.println(number);&gt; &gt; MyInterface myInterface = new MyInterface() &#123;&gt; @Override&gt; public void doSomething() &#123;&gt; System.out.println(number);&gt; &#125;&gt; &#125;;&gt; myInterface.doSomething();&gt; &gt; System.out.println(number);&gt; &#125;&gt; &#125;&gt; &gt; // 反编译结果&gt; class TryUsingAnonymousClass$1&gt; implements MyInterface &#123;&gt; private final TryUsingAnonymousClass this$0;&gt; private final Integer paramInteger;&gt; &gt; TryUsingAnonymousClass$1(TryUsingAnonymousClass this$0, Integer paramInteger) &#123;&gt; this.this$0 = this$0;&gt; this.paramInteger = paramInteger;&gt; &#125;&gt; &gt; public void doSomething() &#123;&gt; System.out.println(this.paramInteger);&gt; &#125;&gt; &#125;&gt; 注意到，number在实际使用时是作为构造函数的参数传入到匿名内部类的，也就是说匿名类内部在使用外部变量时实际上是做了个”拷贝”或者说“赋值”。若可以更改，则会造成数据不一致。 RTTIRTTI(Run-Time Type Identifier)是Java能在运行时自动识别出某个类型的保证（RTTI在Java运行时维护类的相关信息），是多态的基础，由Class类实现。 Class对象每当编写并且编译一个类时，在与类同名的.class文件中会自动产生一个Class对象。实现此过程的JVM子系统被称作类加载器。 Class对象仅在需要的时候才被加载，也就是所有的类都是只在对其第一次使用时，动态加载到JVM中的。所谓第一次使用指的是对类的非常量静态域的第一次引用。 要注意，类的构造器是隐性非常量静态域，所以使用new操作符生成对象也是产生这样的Class类引用。 与此同时，还可以使用Class.forName(类名)产生Class对象的引用，告诉JVM去加载这个类。当JVM未找到这个类，会抛出异常ClassNotFoundException。比如在JDBC连接数据库时常常用到的Class.forName(&quot;com.mysql.jdbc.Driver&quot;)，就是告诉JVM去加载MySQL驱动。 当已经拥有某个类型的对象（实例）时，可通过调用getClass()方法来获取该类型的Class引用。 另一种方法，使用类字面变量。通过使用类名.class可获取此类的Class对象的引用，但是注意，此时此Class对象还未被初始化，还需要等到上述的对类的非常量静态域的第一次引用这一操作执行时才被初始化。 使用.class方法获取Class对象引用实际包含三个步骤： 加载：类加载器创建Class对象 链接 初始化：如果该类具有超类，则对其初始化，执行静态初始化器和静态初始化块 考虑如下代码： &gt; import java.util.Random;&gt; &gt; class Initable &#123;&gt; static final int staticFinal = 1;&gt; static final int staticFinal2 = ClassInitialization.rand.nextInt(1000);&gt; static &#123;&gt; System.out.println(\"Initializing Initable\");&gt; &#125;&gt; &#125;&gt; &gt; class Initable2 &#123;&gt; static int staticNonFinal = 2;&gt; static &#123;&gt; System.out.println(\"Initializing Initable2\");&gt; &#125;&gt; &#125;&gt; &gt; class Initable3 &#123;&gt; static int staticNonFinal = 3;&gt; static &#123;&gt; System.out.println(\"Initializing Initable3\");&gt; &#125;&gt; &#125;&gt; &gt; public class ClassInitialization &#123;&gt; public static Random rand = new Random(47);&gt; public static void main(String[] args) throws ClassNotFoundException &#123;&gt; // 创建Initable的Class对象的引用，Class对象未初始化&gt; Class initable = Initable.class;&gt; // 仍然未初始化，因Initable.staticFinal是常数&gt; System.out.println(Initable.staticFinal);&gt; // 触发了Initable的Class对象的初始化&gt; System.out.println(Initable.staticFinal2);&gt; // 触发了Initable2的Class对象的初始化&gt; System.out.println(Initable2.staticNonFinal);&gt; // 创建Initable3的Class对象的引用，同时会初始化此Class对象&gt; Class initable3 = Class.forName(\"Initable3\");&gt; // 此时已初始化，无需再次初始化&gt; System.out.println(Initable3.staticNonFinal);&gt; &#125;&gt; &#125;&gt; 另外，当我拥有某个Class对象c的时候，我虽然不知道它确切类型，但是可以使用c.newInstance()来正确地获取c代表的类型的实例。但是此方法要求对应的类。 泛化的Class对象引用Class对象可以通过Class&lt;Type&gt;的方法产生特定类型的类引用，创建了使用类型限定后的Class对象引用不能再赋值给除本身和子类的其他的Class对象。 注意这里的子类指的是Class对象的继承关系，而不是类本身的继承关系，如Integer继承自Number，而Integer Class对象却不是Number Class对象的子类。 使用通配符Class&lt;?&gt;优于平凡的Class（实际上是等价的），而且会免除编译器警告，看图： 一种更好的用法，Class&lt;? extends Type&gt;，这种类型限定比直接Class&lt;Type&gt;好的地方在于他产生的Class对象引用可赋值给Type本身及子类的Class对象，这种继承关系是Type所属的继承关系而不是对应的Class对象的继承关系。 转型语法（不常用）class Building&#123;&#125;class House extends Building &#123;&#125;public class ClassCasts &#123; public static void main(String[] args)&#123; Building b = new Building(); Class&lt;House&gt; houseType = House.class; House h = houseType.cast(b); h = (House) b; &#125;&#125; 如上，使用houseType.cast(b)和(House) b效果一样，但是执行的工作却不同，具体内部实现尚未学习到。 动态的类型检测obj instanceof ClassType返回一个布尔值，告诉我们某个对象是不是某个特定类型的实例。 ClassType.isInstance()返回一个布尔值，告诉我们某个对象的类型是不是可以被强转为某个特定类型。 区别区别主要是后者与前者动态等价，看代码： class Father&#123;&#125;class Son extends Father&#123;&#125;public class DynamicEqual &#123; public static void main(String[] args)&#123; Father father = new Father(); Son son = new Son(); // instanceof关键词后面必须跟类型的名称，意即其必须首先知道类型名称 // if (son instanceof father.getClass())&#123; // ... // &#125; // isInstance()方法是类对象的方法，任何一种类型的类对象的引用都可调用该方法，简言之，其前面的Class类对象是可动态的。 if (father.getClass().isInstance(son))&#123; System.out.println(\"isInstance is Dynamic\"); &#125; &#125;&#125; 优点isInstance()的存在可以替代instanceof，而且可使得代码更简洁。比如说有多个类{A1,A2,A3,…}都继承自A，现有一个A对象实例，要判断其为子类中的哪一个从而产生不同响应时： 使用instanceof时可能需要使用switch-case语句；当需要添加一个子类时，需要修改switch-case内部代码。 而使用isInstance()时，可创建一个列表存储所有的子类类型，主程序只需要使用一个循环检测该实例即可；当需要添加一个子类时，只需要修改子类类型列表而不用修改程序代码。 反射机制反射与RTTI的区别 RTTI：编译器在编译时打开和检查.class文件（获取类的Class类对象信息） 反射：JVM在运行时打开和检查.class文件（编译时可能没有此文件，但是在运行时必须在本地机器或者网络上获取.class文件） 类方法提取器通过Class对象引用：调用getMethods()方法获取该类及其父类的方法列表，调用getConstructors()方法获取该类的构造方法列表。要注意能获得的方法与该类的访问权限有关，一个非public类的非public方法是无法被获取的。 接口与类型信息interface关键字的一种重要目标就是允许程序员隔离构件，进而降低耦合性。 包权限安全吗？直接看例子： // A.javapublic interface A&#123; void f();&#125;// HiddenC.javaclass C implements A&#123; @Override public void f() &#123; System.out.println(\"public C.f()\"); &#125; public void g()&#123; System.out.println(\"public C.g()\"); &#125; void u()&#123; System.out.println(\"package C.u()\"); &#125; protected void v()&#123; System.out.println(\"protected C.v()\"); &#125; private void w()&#123; System.out.println(\"private C.w()\"); &#125;&#125;public class HiddenC &#123; public static A makeA()&#123; return new C(); &#125;&#125;// HiddenImplementation.javaimport java.lang.reflect.Method;public class HiddenImplementation &#123; public static void main(String[] args) throws Exception &#123; A a = HiddenC.makeA(); a.f(); System.out.println(a.getClass().getName()); callHiddenMethod(a, \"g\"); callHiddenMethod(a, \"u\"); callHiddenMethod(a, \"v\"); callHiddenMethod(a, \"w\"); &#125; static void callHiddenMethod(Object a, String methodName) throws Exception&#123; // 获取a中的方法 Method g = a.getClass().getDeclaredMethod(methodName); // 修改该方法的权限 g.setAccessible(true); // 调用该方法 g.invoke(a); &#125;&#125;/* outputpublic C.f()Cpublic C.g()package C.u()protected C.v()private C.w()*///:) 当我知道一个类中有哪些方法时，哪怕是private方法仍然可以在使用setAccessble(true)后被调用。 只发布.class文件也是没办法避免此问题，javap -private命令可以反编译.class文件，-private参数约定显示所有的成员 同样，内部类和匿名内部类也是没办法避免此情况 泛型指定类型有保证吗？ 在泛型代码内部，无法获得任何有关泛型参数类型的信息。 例如对于ArrayList&lt;String&gt;和ArrayList&lt;Integer&gt;，二者的实例调用.getClass()获取的Class对象时相同的，如下： import java.util.ArrayList;public class Erase&#123; public static void main(String[] args)&#123; Class&lt;?&gt; s = new ArrayList&lt;String&gt;().getClass(); Class&lt;?&gt; i = new ArrayList&lt;Integer&gt;().getClass(); System.out.println(s == i); System.out.println(s.getName()); System.out.println(s.getTypeParameters()); &#125;&#125;/* Output:truejava.util.ArrayList[Ljava.lang.reflect.TypeVariable;@68f7aae2*/// 但是，如果在一个ArrayList&lt;String&gt;类型的实例中添加Integer会报编译期错误，这个很容易理解（静态类型检查）。但是上述的Class对象相同有给了我们可乘之机： import java.lang.reflect.Method;import java.util.ArrayList;import java.util.Arrays;class Apple&#123; @Override public String toString()&#123; return \"This is an apple\"; &#125;&#125;public class ReflectAdd&#123; public static void main(String[] args) throws Exception&#123; ArrayList&lt;String&gt; strings = new ArrayList&lt;&gt;(); Class&lt;?&gt; s = strings.getClass(); Method method = s.getMethod(\"add\", Object.class); method.invoke(strings, 1); method.invoke(strings, \"2\"); method.invoke(strings, 3); method.invoke(strings, new Apple()); System.out.println(Arrays.toString(strings.toArray())); for (Object o : strings)&#123; System.out.println(o.getClass()); &#125; &#125;&#125;/* Output:[1, 2, 3, This is an apple]class java.lang.Integerclass java.lang.Stringclass java.lang.Integerclass Apple*/// 我们可以看到上述代码使用反射机制成功的在ArrayList&lt;String&gt;里面添加了Integer，原因在于ArrayList的泛型实现ArrayList&lt;E&gt;使其被擦除为ArrayList&lt;Object&gt;，从而通过反射机制找到其add(E e)方法时，实际上是add(Object o)，而我们代码中的Method method = s.getMethod(&quot;add&quot;, Object.class);恰好可以找到包含这样一个参数列表的add方法，后面也就理所当然的可以添加任意类型(甚至是自定义的Apple类)的实例了。 与C++的区别C++: #include &lt;iostream&gt;using namespace std;template&lt;class T&gt; class Manipulator &#123; T obj;public : Manipulator(T x) &#123; obj = x; &#125; void manipulate() &#123; obj.f(); &#125; void manipulate2() &#123; obj.noF(); &#125;&#125;;class HasF &#123;public: void f()&#123; cout &lt;&lt; \"HasF()::f()\" &lt;&lt; endl; &#125;&#125;;class DontHaveF&#123;public: void noF()&#123; cout &lt;&lt; \"Don't have f()\" &lt;&lt; endl; &#125;&#125;;int main()&#123; HasF hf; Manipulator&lt;HasF&gt; manipulator(hf); manipulator.manipulate(); // manipulator.manipulate2(); 无法编译 DontHaveF dhf; Manipulator&lt;DontHaveF&gt; manipulator2(dhf); // manipulator2.manipulate(); 无法编译 manipulator2.manipulate2();&#125;/* Output:HasF()::f()Don't have f()*/// 模板类Manipulator在编译时期便可以检测到函数f()、noF()是在类型参数中存在的，这是在编译器看到声明Manipulator&lt;HasF&gt; manipulator(hf)和Manipulator&lt;DontHaveF&gt; manipulator2(dhf)所产生的结果。 然而Java中却无法实现这样的操作： Java: // HasF.javapublic class HasF&#123; public void f()&#123; System.out.println(\"HasF.f();\"); &#125;&#125;// Manipulation.javaimport java.lang.reflect.Method;class Manipulator&lt;T&gt; &#123; private T obj; public Manipulator(T x) &#123; obj = x; &#125; public void manipulate()&#123; obj.f() // 会报编译错误 &#125;&#125;public class Manipulation&#123; public static void main(String[] args)&#123; HasF hf = new HasF(); Manipulator&lt;HasF&gt; manipulation = new Manipulator&lt;HasF&gt;(hf); manipulation.manipulate(); &#125;&#125; 由于Java在编译过程中，Manipulator&lt;T&gt;是无法确定其类型参数，只知道他是一个Object实例，因此obj只能调用Object基类所有的公开方法。若想实现C++的操作有两种办法(目前我已知的只有这两种)。 为T限定参数类型（给定边界），即声明时指定其所继承的基类： class Manipulator&lt;T extends HasF&gt;&#123; ...&#125; 使用反射机制调用f() import java.lang.reflect.Method;class Manipulator&lt;T&gt; &#123; private T obj; public Manipulator(T x) &#123; obj = x; &#125; public void manipulate() throws Exception&#123; Class&lt;?&gt; oc = obj.getClass(); Method method = oc.getMethod(\"f\"); method.invoke(obj); &#125;&#125;public class Manipulation&#123; public static void main(String[] args) throws Exception&#123; HasF hf = new HasF(); Manipulator&lt;HasF&gt; manipulation = new Manipulator&lt;HasF&gt;(hf); manipulation.manipulate(); &#125;&#125;/* Output:HasF.f();*/// 擦除带来的问题 擦除的主要正当理由是从非泛化代码到繁华代码的转变过程，以及在不破坏现有类库的情况下，将泛型融入Java语言。 泛型不能用于显式地引用运行时类型的操作之中，例如转型、instanceof、new表达式，因为在静态类型检测之后，泛型就已经被擦除了。 也就是说，需要时刻提醒自己，我只是看起来好像拥有有关参数的类型信息而已。实际上，它只是一个Object！ 边界既然编译器会擦除类型信息，那么擦除发生的地点是在哪儿呢？便是所谓的边界：对象进入和离开方法的地点，也就是编译器在执行类型检查并插入转型代码的地点。 通配符import java.util.Arrays;import java.util.List;class Fruit&#123;&#125;class Apple extends Fruit&#123;&#125;class Jonathan extends Apple&#123;&#125;class Orange extends Fruit&#123;&#125;public class CompilerIntelligence&#123; public static void main(String[] args)&#123; List&lt;? extends Fruit&gt; flist = Arrays.asList(new Apple()); Apple a = (Apple) flist.get(0); // Orange o = (Orange) flist.get(0); 运行时错误 // flist.add(new Fruit()); 编译错误 // flist.add(new Apple()); 编译错误 System.out.println(flist.contains(a)); System.out.println(flist.contains(new Apple())); System.out.println(flist.indexOf(new Apple())); &#125;&#125;/*truefalse-1*/// 对于使用了通配符的List&lt;? extends Fruit&gt; flist来说，其需要用到类型参数的方法例如add()参数也变成了&lt;? extends Fruit&gt;，然而编译器并不能知道这里需要哪一个具体的子类型，于是编译器拒绝了所有对参数列表中涉及到了通配符的方法的调用，除了构造器。 容器完整的容器分类法： HashMapHashMap采用了链地址法，也就是数组+链表的方式。主干是一个Entry数组，链表是为了解决哈希冲突而存在的。HashMap中的链表越少，性能越好。 Entry数组长度为2的次幂 由于在计算key的插入位置时用到了hash &amp; (length-1)，hash是key计算出来的哈希值，想象一下当length不为2的次幂时，length-1的二进制必然有0位，那么意味着该位为0的位置永远不可能被当做插入位置，造成了严重的空间浪费。 由于刚才的原因，数组可以使用的位置比数组长度小了很多，意味着进一步增加了碰撞的几率，意即equal()操作多了起来，效率也就慢了。 resizeHashMap当Entry数组元素超过数组大小*loadFactor时，就会进行数组扩容。loadFactor默认值为0.75。此时Entry数组大小会扩大一倍，保证了2的次幂大小。 扩容的时候所有的key需要重新计算哈希值。 JDK1.8优化由于1.8之前的HashMap在hash冲突很大时，遍历链表将会效率很低，于是1.8中采用了红黑树部分代替链表，当链表长度到达阈值时，就会改用红黑树存储。 HashTableHashTable在结构上与HashMap基本相同，下面总结其不同点： HashMap可有null key，HashTable获取null key会报空指针异常 HashTable有synchronized方法同步，线程安全；HashMap线程不安全 Hash值计算方法不同 HashTable初始大小为11，扩容机制为2*old+1；HashMap初始大小为16，扩容机制为2*old ConcurrentHashMapJDK1.7版本中的ConcurrentHashMap比HashMap多了一层Segment，其中Segment继承于ReentrantLock：一次put操作会调用scanAndLockForPut()方法自旋获取锁；而一次get操作则不需要加锁，value用volatile关键词修饰的，保证了内存可见性，每次获取的必定是新值，由于不用加锁，所以很高效。 JDK1.8版本移除了segment，有一个Node数组相当于HashMap中的Entry数组。同时采用了CAS+synchronized关键字进行put操作。put操作步骤如下： 根据key计算出hashcode； 判断是否需要进行初始化； f 即为当前 key 定位出的 Node，如果为空表示当前位置可以写入数据，利用 CAS 尝试写入，失败则自旋保证成功； 判断是否需要进行扩容； 如果都不满足，则利用 synchronized 锁写入数据； 如果数量大于 TREEIFY_THRESHOLD 则要转换为红黑树。 线程Brian Goetz的线程同步规则 如果你正在写一个变量，他可能接下来将被另一个线程读取，或者正在读取一个上一次已经被另一个线程写过的变量，那么必须使用同步，并且，读写线程都必须用相同的监视器锁同步。 ExecutorExecutor用来管理Thread对象，简化了并发编程，允许管理异步任务的执行，而无须显式管理线程的声明周期。 import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;public class CachedThreadPool &#123; public static void main(String[] args)&#123; ExecutorService exec = Executors.newCachedThreadPool(); for (int i = 0; i &lt; 5; i++)&#123; exec.execute(new LiftOff()); &#125; exec.shutdown(); &#125;&#125;/* Output:#4(9).#2(9).#1(9).#2(8).#3(9).#4(8).#0(9).#1(8).#2(7).#3(8).#4(7).#0(8).#1(7).#2(6).#3(7).#4(6).#0(7).#1(6).#2(5).#3(6).#4(5).#0(6).#1(5).#2(4).#3(5).#4(4).#0(5).#1(4).#2(3).#3(4).#4(3).#0(4).#1(3).#2(2).#3(3).#4(2).#0(3).#1(2).#2(1).#3(2).#4(1).#0(2).#1(1).#2(LiftOff!).#3(1).#4(LiftOff!).#0(1).#1(LiftOff!).#0(LiftOff!).#3(LiftOff!).*/// 线程池线程池的作用是限制系统中执行线程的数量，根据系统情况可以自动或手动设置线程数量，达到最佳运行效果。线程池中的线程若出现异常，会自动补充一个新线程以代替。 newSingleThreadExecutor()：创建一个单线程的线程池，所有的任务在等待队列中等待该线程。 newFixedThreadPool()：创建固定大小的线程池。 newCachedThreadPool()：创建一个可缓存的线程池。会根据任务数量自动添加和回收线程，线程池的大小依赖于JVM能够创建的最大线程大小。 newScheduledThreadPool()：创建一个大小无限的线程池，此线程支持定时以及周期性执行任务的需求。 任务的返回值通常实现Runnable接口的类是没有返回值的，要想任务在完成时返回一个值可实现Callable&lt;T&gt;接口，其泛型类型参数表示方法call()的返回值，并且需要使用ExecutorService.submit()方法调用他。 import java.util.ArrayList;import java.util.concurrent.*;class TaskWithResult implements Callable&lt;String&gt; &#123; private int id; public TaskWithResult(int id) &#123; this.id = id; &#125; public String call() &#123; return \"result of TaskWithResult\" + id; &#125;&#125;public class CallableDemo &#123; public static void main(String[] args) &#123; ExecutorService exec = Executors.newCachedThreadPool(); ArrayList&lt;Future&lt;String&gt;&gt; results = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 10; i++) &#123; results.add(exec.submit(new TaskWithResult(i))); &#125; for (Future&lt;String&gt; fs : results) &#123; try &#123; System.out.println(fs.get()); &#125; catch (InterruptedException | ExecutionException e) &#123; System.err.println(e); &#125; finally &#123; exec.shutdown(); &#125; &#125; &#125;&#125;/* Output：result of TaskWithResult0result of TaskWithResult1result of TaskWithResult2result of TaskWithResult3result of TaskWithResult4result of TaskWithResult5result of TaskWithResult6result of TaskWithResult7result of TaskWithResult8result of TaskWithResult9*/// ExecutorService对象的submit()方法会返回一个Future&lt;T&gt;对象，泛型类型参数即是实现Callable&lt;T&gt;的类型参数。get()方法会返回结果，若任务未完成，get()会阻塞。 优先级优先权不会导致死锁，优先级较低的线程仅仅是执行的频率较低。 但是注意优先级高的线程也有几率比优先级底的线程执行的少。 优先级是否起作用也与操作系统及虚拟机版本相关联，会随着不同的线程调度器而产生不同的含义。 Thread.yield()可靠吗？Thread.yield()源码中提及了该方法的效果：当前线程会给线程调度器一个暗示，说明我愿意让出当前资源供你调度，但是线程调度器可自由的选择是否忽略其暗示。意即此处的让步只是一厢情愿，发出让步的线程同样可以继续执行。 后台线程后台线程并不属于程序中不可或缺的部分。当所有的非后台线程结束时，程序也就终止了，同时会杀死进程中的所有后台线程。 执行main()就是一个非后台线程，当main()没有执行结束时，程序就不会终止。 后台线程创建的线程也将是后台线程。 同时要注意在后台线程的run()方法中若有finally子句，其中的语句也不一定会执行。因为随着非后台线程的结束，后台线程会突然终止。 Thread还是Runnable创建多线程任务可以继承Thread类重写其run()方法，也可以实现Runnable接口实现其run()方法。 实际应用中，Runnable还是比较有优势的： 避免了由于Java的单继承体系带来的局限（实际上继承Thread也是可以避免，使用内部类） 多个线程区处理同一资源，而非独立处理（这句话有问题） 注意，一开始在理解这里的时候我出现了误解，什么叫处理同一资源，意思指的是Thread类无法达到资源共享的目的，而Runnable可以。但是在使用线程池的时候，Thread又可以了(待确认)，如下： import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;class TestThread extends Thread &#123; private int val = 10; public void run()&#123; while(true)&#123; System.out.println(Thread.currentThread() + \"-- val: \" + val--); Thread.yield(); if (val &lt;= 0) return; &#125; &#125;&#125;class TestRunnable implements Runnable &#123; private int val = 10; @Override public void run() &#123; while(true)&#123; System.out.println(Thread.currentThread() + \"-- val: \" + val--); Thread.yield(); if (val &lt;= 0) return; &#125; &#125; &#125;public class TestRunnableAndThread &#123; public static void main(String[] args)&#123; Runnable runnable = new TestRunnable(); Thread thread = new TestThread(); // a.只有1个线程处理一个数据 thread.start(); thread.start(); thread.start(); thread.start(); thread.start(); // b.5个不同线程处理不同数据 new TestThread().start(); new TestThread().start(); new TestThread().start(); new TestThread().start(); new TestThread().start(); // c.5个不同线程处理相同数据 new Thread(runnable).start(); new Thread(runnable).start(); new Thread(runnable).start(); new Thread(runnable).start(); new Thread(runnable).start(); // d.5个不同线程处理相同数据 ExecutorService execRun = Executors.newCachedThreadPool(); for (int i = 0; i &lt; 5; i++) execRun.execute(runnable); // e.5个不同线程处理5个不同数据 ExecutorService execRun2 = Executors.newCachedThreadPool(); for (int i = 0; i &lt; 5; i++) execRun2.execute(new TestRunnable()); // f.5个不同线程处理相同数据 ExecutorService execThread = Executors.newCachedThreadPool(); for (int i = 0; i &lt; 5; i++)&#123; execThread.execute(thread); &#125; // g.5个不同线程处理5个不同数据 ExecutorService execThread2 = Executors.newCachedThreadPool(); for (int i = 0; i &lt; 5; i++) execThread2.execute(new TestThread()); // i.5个不同线程处理相同数据 ExecutorService execThread2 = Executors.newCachedThreadPool(); for (int i = 0; i &lt; 5; i++) execThread2.execute(new Thread(runnable)); execRun.shutdown(); execRun2.shutdown(); execThread.shutdown(); execThread2.shutdown(); &#125;&#125; 其中：c、d、i实际上是相同的，b、g是相同的，而a和f看起来相同，但是实际作用却差别很大，待研究。 实际上，a是错误的用法，b、c基本上不用，而且，注意当需要共享数据的时候，通常不会在类中定义共享变量，而需要一个线程安全的外部对象。 共享资源Synchronized冲突是多线程问题必须解决的任务，Java使用synchronized关键字标识访问共享资源的方法，JVM负责跟踪对象被加锁的次数，注意，当对象被解锁（完全释放时）其加锁计数为0，显然此时所有任务都有几率向其加锁，当某一个任务第一次给该对象加锁时，计数变为1，此后只有这个相同的任务能继续给该对象加锁，计数会递增；每当离开一个synchronized方法时，计数递减，直到计数变为0时，对象被解锁。要注意，每个访问该临界资源的方法都必须被同步，否则就不会正确地工作。 通常synchronized关键字标识方法时，是在this上面同步，也可在方法中使用synchronized(synObject){}域，以在特定的对象上同步，因此不同对象上的锁是相互无关的。 LockLock对象必须被显式地创建、锁定和释放。 import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;public class MutexEvenGenerator &#123; private int currentEvenValue = 0; // 显式声明 private Lock lock = new ReentrantLock(); public int next() &#123; // lock()方法创建临界资源 lock.lock(); try &#123; ++currentEvenValue; Thread.yield(); ++currentEvenValue; // return语句必须出现在try子句中 return currentEvenValue; &#125;finally &#123; // unlock()方法完成清理工作 lock.unlock(); &#125; &#125;&#125; 与synchronize相比，显式的Lock优点在于可以使用finally子句将系统维护在正常的状态，而在使用synchronize关键字时，某些事物失败了就会抛出异常。 import java.util.concurrent.TimeUnit;import java.util.concurrent.locks.ReentrantLock;public class AttemptLocking&#123; private ReentrantLock lock = new ReentrantLock(); public void untimed() &#123; boolean captured = lock.tryLock(); try &#123; System.out.println(\"untimed - tryLock(): \" + captured); System.out.println(\"untimed - isHeldByCurrentThread(): \" + lock.isHeldByCurrentThread()); &#125; finally &#123; if (captured) lock.unlock(); &#125; &#125; public void timed() &#123; boolean captured = false; try &#123; captured = lock.tryLock(2, TimeUnit.SECONDS); &#125; catch(InterruptedException e)&#123; throw new RuntimeException(e); &#125; try &#123; System.out.println(\"timed - tryLock(2, TimeUnit.SECONDS): \" + captured); System.out.println(\"timed - isHeldByCurrentThread(): \" + lock.isHeldByCurrentThread()); &#125; finally &#123; if (captured) lock.unlock(); &#125; &#125; public static void main(String[] args)&#123; final AttemptLocking al = new AttemptLocking(); al.untimed(); al.timed(); // 匿名内部类创建单独的Thread来获取锁，而未释放 new Thread()&#123; &#123;setDaemon(true);&#125; public void run()&#123; al.lock.lock(); System.out.println(\"acquired\"); System.out.println(\"main - isHeldByCurrentThread(): \" + al.lock.isHeldByCurrentThread()); &#125; &#125;.start(); Thread.yield(); al.untimed(); al.timed(); &#125;&#125;/* Output:untimed - tryLock(): trueuntimed - isHeldByCurrentThread(): truetimed - tryLock(2, TimeUnit.SECONDS): truetimed - isHeldByCurrentThread(): trueacquiredmain - isHeldByCurrentThread(): trueuntimed - tryLock(): falseuntimed - isHeldByCurrentThread(): falsetimed - tryLock(2, TimeUnit.SECONDS): falsetimed - isHeldByCurrentThread(): false*/// 看代码就很容易理解了。 原子性与易变性原子操作有可能无需同步机制，因为操作是不可分的，一次操作进行的时候不会有其他操作的介入，但是实现原子操作是很难的，或者说原子操作是较少存在的。同时，即使操作是原子性的，操作的修改也可能暂时性地存储在本地处理器的缓存中，对于其他任务有可能是不可视的，因此不同的任务对应用状态有不同的视图。 volatile关键字确保了前面提及的可视性，以及当一个域被声明为volatile时，那么只要对这个域产生了写操作，所有的读操作都可以看到这个修改。即使使用了本地缓存，volatile域的修改也会被立即写入到主存中。 所以非volatile域上的原子操作未刷新到主存中去，因此其他读操作未必会看到新值。 因此多个任务在同时访问某个域时，要么使用volatile关键字限定，要么经由同步机制访问，以保证一致性。 import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;public class AtomicityTest implements Runnable &#123; private int i = 0; public int getValue() &#123; return i; &#125; private void evenIncrement() &#123; i++; i++; &#125; @Override public void run() &#123; while (true) evenIncrement(); &#125; public static void main(String[] args)&#123; ExecutorService exec = Executors.newCachedThreadPool(); AtomicityTest at = new AtomicityTest(); exec.execute(at); while (true)&#123; int val = at.getValue(); if (val%2 != 0)&#123; System.out.println(val); System.exit(0); &#125; &#125; &#125;&#125; 看上面这个例子，程序找到奇数时便终止，理想状态下，通过evenIncrement()加2，i应该始终为偶数，但是由于缺少同步机制，可能导致不稳定的中间状态被读取即获取到奇数，同时i也不是volatile的，因此还存在可视性问题（当然，这里仅仅使用volatile限定i是不够的，因为i++操作不是原子性的）。下面使用Lock显式加锁以实现同步： import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;public class AtomicityTest implements Runnable &#123; private int i = 0; private Lock lock = new ReentrantLock(); public int getValue() &#123; try &#123; lock.lock(); return i; &#125; finally &#123; lock.unlock(); &#125; &#125; private void evenIncrement() &#123; try &#123; lock.lock(); i++; i++; &#125; finally &#123; lock.unlock(); &#125; &#125; @Override public void run() &#123; while (true) evenIncrement(); &#125; public static void main(String[] args)&#123; ExecutorService exec = Executors.newCachedThreadPool(); AtomicityTest at = new AtomicityTest(); exec.execute(at); while (true)&#123; int val = at.getValue(); System.out.println(val); if (val%2 != 0)&#123; System.out.println(val); System.exit(0); &#125; &#125; &#125;&#125; 原子类上面说到原子操作是较少的，而JSE5引入了AtomicInteger、AtomicLong、AtomicReference等特殊的原子性变量类，这些类的一些方法在某些机器上可以是原子的。通常用在性能调优方面。 ReetrantLockReentrantLock是一个可重入的互斥锁，又被称为”独占锁“。 可重入锁指的是某个线程获取锁之后，在执行相关的代码块时可继续调用加了同样的锁的方法，理解为嵌套锁。反之，不可重入锁称作自旋锁。 独占锁指的是同一时间点锁只能被一个线程获取。 同时ReentrantLock也分为公平锁和非公平锁，它们的区别体现在获取锁的机制是否公平。公平锁通过一个FIFO等待队列管理等待获取该锁的所有进程，而非公平锁不管是否在队列中，都直接获取该锁。 ReentrantReedWriteLock顾名思义，ReentrantReadWriteLock维护了读取锁和写入锁。 读取锁用于只读操作，是共享锁，能被多个线程获取； 写入锁用于写入操作，是独占锁，只能被一个线程获取。 线程状态 新建（new） 就绪（Runnable） 阻塞（Blocked） 调用sleep(milliseconds)方法使任务休眠 调用wait()方法挂起 等待输入输出完成 获取锁失败 死亡（Dead） 线程协作wait()与sleep()和yield()不同，调用wait()时需要释放当前线程获取的锁，由于某个条件不成立使得当前线程进入阻塞状态，直到其他修改使得此条件发生了变化调用了notifyAll()方法时，线程被唤醒。 但是要注意，使用wait()的时候需要用while循环包围： 为了检查线程是否被意外唤醒 notifyAll()notifyAll()用来唤醒等待某个锁的所有挂起的任务。等待某个锁指的是某些需要获取共同的锁的线程，notifyAll()可以唤醒这些线程，而不是程序中所有被挂起的线程。 死锁多个并发进程因争夺系统资源而产生相互等待的现象。 四个必要条件： 互斥 占有且等待 不可抢占 循环等待 免锁容器免锁容器的策略是：对容器的修改可以与读取操作同时发生，只要读取者只能看到完成修改的结果即可。修改时在容器数据结构的某个部分的一个单独的副本上执行的，并且这个副本在修改过程中是不可视的。只有当修改完成时，被修改的结构才会自动地与主数据结构进行交换，之后读取者就可以看到这个修改了。 这些容器允许并发的读取和写入，但是在任何修改完成之前，读取者仍然是不能够看到它们的。 乐观锁每次拿数据的时候认为别人不会修改，所以不会上锁，但是在更新的时候会判断此期间有没有别人更新这个数据。上述有提到的原子类就是使用了CAS实现的乐观锁。 悲观锁每次拿数据的时候都认为别人会修改，所以每次拿数据的时候都会上锁。synchronized关键字的实现就是悲观锁。 CAS(Compare And Swap)技术CAS是用来实现乐观锁的一种方法，原理见这里。 CAS机制使用3个基本操作数：内存地址V，旧的预期值A，要修改的新值B。 更新一个变量的时候，只有当A和V的实际值相同时，才会将V对应的值修改为B。 缺点： ABA问题：链表的头在变化了两次后恢复了原值，但是不代表链表就没有发生变化 循环时间长开销大 只能保证一个共享变量的原子性 未完~","categories":[{"name":"Java","slug":"Java","permalink":"http://blog.guitoubing.top/categories/Java/"}],"tags":[{"name":"垃圾回收","slug":"垃圾回收","permalink":"http://blog.guitoubing.top/tags/垃圾回收/"},{"name":"泛型编程","slug":"泛型编程","permalink":"http://blog.guitoubing.top/tags/泛型编程/"},{"name":"容器","slug":"容器","permalink":"http://blog.guitoubing.top/tags/容器/"}]},{"title":"数据库与内存数据库实验报告","slug":"数据库与内存数据库实验报告","date":"2019-03-05T12:34:59.000Z","updated":"2021-05-16T11:00:35.325Z","comments":true,"path":"2019/03/05/数据库与内存数据库实验报告/","link":"","permalink":"http://blog.guitoubing.top/2019/03/05/数据库与内存数据库实验报告/","excerpt":"一、实验前准备机器配置","text":"一、实验前准备机器配置 时间计算标准SQL执行过程首先，本实验的目的是优化数据库，减少数据库语句执行的时间，在此之前，我们要明白一点数据库执行时间这句话包含了哪些东西。我们从数据库执行一条SQL语句的过程来看，对于MySQL、Oracle、TimesTen这些具有内部优化的数据库来说，一般的执行步骤是： 而我们的关注点应放在语句执行这一步骤上。 语句执行步骤进一步深入MySQLMySQL的执行时间为以下项目的加和： State Desription 1. Checking permissions 检查用户的权限 2. Opening tables 打开表 3. Init 初始化过程 4. System lock 获取锁 5. Optimizing 优化SQL语句 6. Statistics 分析SQL语句 7. Preparing 准备执行条件 8. Executing 执行SQL语句 9. Sending data 进行磁盘的IO以及数据的发送返回 10. End 执行结束 11. Closing tables 关闭表 12. Freeing items 释放资源 13. Cleaning up 清理缓存以及临时空间 Oracle一条SQL语句在进入语句执行这一步骤之后，若不在高速缓存中，数据库会从数据文件中把所在位置移动到高速缓存中而后返回给客户端。这也就意味着，同一条语句在以后的执行中都只从高速缓存取数据（前提是高速缓存未被清除）。这样想的话，我们要做的优化应该是一条SQL语句在第一次进入数据库时数据库作出的应答。 那么，我们通过数据库工具来查看执行的SQL语句的时间应该是不准的：因为我们不知道这条语句是不是第一次执行，或者说我们不知道高速缓存中有没有我们需要的数据。这里我们选择使用Oracle的执行计划来看SQL语句的准确的执行过程以及其开销。如下图： 我们的关注点在上图中的COST，cost是Oracle里判定效率的唯一标准，Oracle的优化器会计算当前SQL语句的最低cost方案，而后为其选择执行计划。Oracle中定义了语句的一次执行开销cost = CPU cost + IO cost，对于cost，我们可以理解为一次过程所需要访问的Block数量，那么执行时间就是t = Block数量 * Block处理时间。 后续实验过程中的Oracle部分我们都是通过执行计划及cost来做对比。为此我们写了一个procedure来记录一条语句执行计划中记录的cost： -- 计算query的costcreate or replace procedure calc_cost(query_ varchar2, func_ number, desc_ varchar2) is cpu_cost number := 0; io_cost number := 0; cost_ number := 0; -- 一条SQL语句的唯一标识 hash_v number := 0; -- 获取上述标识 select_v_sql varchar2(255) := &apos;select hash_value into :x from v$sql a where a.SQL_TEXT like &apos;&apos;:y&apos;&apos;&apos;; -- 获取cost select_v_sql_plan varchar2(255) := &apos;select max(cpu_cost) , max(io_cost) into :x :y from V$SQL_PLAN a where hash_value=:z&apos;; -- 结果保存 insert_result varchar2(255) := &apos;insert into t_cost_record values(:x,:y,:z,:a,:b,:c)&apos;;begin execute immediate select_v_sql using hash_v, query_; execute immediate select_v_sql_plan using cpu_cost, io_cost, cost_; execute immediate insert_result using id_seq.nextval, func_, cpu_cost, io_cost, cost_, desc_;end; TimesTen对于TimesTen来说，不如Oracle的优化器来的智能，它完全靠速度制胜。Oracle中我们讨论了执行时间t = Block数量 * Block处理时间，TimesTen就是在Block处理时间上有很大的优势。遗憾的是TimesTen中没有作为本身的高速缓存这一说，这也就意味着一条SQL语句进入TimesTen时都要经过SQL Prepare -&gt; SQL Execution -&gt; SQL Fetch这一完整的过程，如下： 二、MySQL实验过程功能：查询电影评论平均分排行前一百的电影 SQL语句select m.name_, sum(c1.score) as movie_avg_comment_scorefrom movie m , comment_1 c1where m.id_ = c1.movie_idgroup by m.name_order by movie_avg_comment_score desclimit 100; 仅有主键索引执行之后得到如下的时间消耗： 这个时间相比其他数据库慢得多（oracle 约4s)，不符合预期的耗时，且在执行时mysqld的cpu占用率非常高。于是根据以下步骤查看sql执行慢的原因。 MySQL进程表使用show processlist命令查看正在执行的sql语句列表： 可以看到当前执行的语句就是我们的目标语句，并且没有其他语句在与当前查询语句竞争资源，所以应该把语句执行过慢点原因定位到查询语句本身。 解释执行计划通过查看process list得知对应语句有问题之后，使用describe命令查看当前SQL语句的执行计划，MySQL的执行计划与其他相关参数： 可以看到在执行计划中，movie表有可选的主码索引，但是在这个场景中mysql并没有选择使用主码索引，没有使用索引是导致时间过慢点一个原因，于是可以考虑在电影名字字段上建立索引。 执行过程为了进一步查看SQL语句具体的系统能耗分布，我们选择使用profiling来分析我们SQL语句的执行过程，在没有创建其他索引的情况下我们得到如下的时间消耗分析： 我们可以看到其中能耗占比最高的是 Sending data项，查看官方文档相关解释： The thread is reading and processing rows for a SELECT statement, and sending data to the client. Because operations occurring during this state tend to perform large amounts of disk access (reads), it is often the longest-running state over the lifetime of a given query. 该线程正在读取和处理SELECT语句的行，并将数据发送到客户端。 由于在此状态期间发生的操作往往会执行大量磁盘访问（读取），因此它通常是给定查询生命周期中运行时间最长的状态 所以这个与我们的磁盘IO的速度以及网络的传输速度有关，磁盘的IO除了受到硬件本身的限制之外还会与数据库的索引有关，更换性能更好的磁盘或者建立适当的索引以减少磁盘IO数量都可以提高查询语句的执行速度。 建立索引根据以上分析过程得到的结论，我们在电影表的名字字段上建立合适的索引，我们在mysql中选择了B-Tree索引。 建立索引之后再查看相同SQL语句的执行计划： key字段上的值从原来的null 变成了我们刚刚创建的索引。 执行该SQL语句，并在结束后使用Profiling查看优化后的执行时间： sending data: 从磁盘读取数据，将数据返回，表示磁盘IO create index：使用临时表来处理select语句 可以看到Sending data的值明显小于优化前，总的执行时间也变为优化前的1/5，所以增加索引能够在很大程度上加快查询的速度。 实验结论综合其他实验，在大数据的处理上MySQL数据库的性能远不如ORACLE及TIMESTEN数据库，有数十倍的耗时差距，而且MySQL作为一个轻量级的数据库，支持的索引类型也少于其他两个数据库，在SQL语句的优化方面也不如ORACLE数据库那般强大。所以在当前的实验环境下我们更倾向于使用ORACLE数据库与TIMESTEN数据库进行对比。 三、Oracle实验过程实验1：SQL各子句条件顺序对查询效率的影响查询语句SELECT T_MOVIE.NAME_, T_MOVIE.YEAR_ FROM T_MOVIE,T_MOVIE_REGION,T_REGIONWHERE T_REGION.ID_=T_MOVIE_REGION.REGION_ID AND T_MOVIE.ID_=T_MOVIE_REGION.MOVIE_IDAND T_REGION.NAME_='美国' AND T_MOVIE.SCORE_&gt;6; 实验方式通过对MySQL、Oracle、TimesTen中SQL语句中select、from、where子句的排序顺序进行调换，观察执行计划的改变 实验结果 SELECT子句中，结果集的排序方式不会影响执行计划 FROM子句中，各个表的排序方式不会影响执行计划 WHERE子句中，各个条件的排序方式不会影响执行计划，优化器会首先将筛选条件应用于表进行过滤，最后逐次执行表的连接。 分析自Oracle6以来，一直采用RBO（Rule-Based Optimization 基于规则的优化器），其基于一套严格死板的使用规则。由于其对于规则的崇尚性，SQL语句的写法则尤为重要。而自Oracle8以来，Oracle引入了一种新的优化方式，即CBO（Cost-Based Optimization 基于代价的优化器），从Oracle 10g开始RBO被完全舍弃。使用CBO优化器时，对SQL语句的要求变得没有那么苛刻，优化器会选择开销比较小的方式执行，而不由用户所写的表的顺序、条件的顺序决定。MySQL与TimesTen的优化器也是如此，有其自己的选择。 连接方式和连接顺序连接顺序：连接顺序表明以哪张表为驱动表来连接其他表的先后顺序。即以某张表为基点，根据其中的信息再去访问其他的表。 连接方式：简单来讲，就是两个表获得满足条件的数据时的连接过程。主要有三种表连接方式，嵌套循环（NESTED LOOPS）、哈希连接（HASH JOIN）和排序-合并连接（SORT MERGE JOIN）。 排序-合并连接假设有查询：select a.name, b.name from table_A a join table_B b on (a.id = b.id) 内部连接过程： a) 生成 row source 1 需要的数据，按照连接操作关联列（如示例中的a.id）对这些数据进行排序 b) 生成 row source 2 需要的数据，按照与 a) 中对应的连接操作关联列（b.id）对数据进行排序 c) 两边已排序的行放在一起执行合并操作（对两边的数据集进行扫描并判断是否连接） 延伸： 如果示例中的连接操作关联列 a.id，b.id 之前就已经被排过序了的话，连接速度便可大大提高，因为排序是很费时间和资源的操作，尤其对于有大量数据的表。 故可以考虑在 a.id，b.id 上建立索引让其能预先排好序。不过遗憾的是，由于返回的结果集中包括所有字段，所以通常的执行计划中，即使连接列存在索引，也不会进入到执行计划中，除非进行一些特定列处理（如仅仅只查询有索引的列等）。 排序-合并连接的表无驱动顺序，谁在前面都可以； 排序-合并连接适用的连接条件有： &lt; &lt;= = &gt; &gt;= ，不适用的连接条件有： &lt;&gt; like 嵌套循环内部连接过程： a) 取出 row source 1 的 row 1（第一行数据），遍历 row source 2 的所有行并检查是否有匹配的，取出匹配的行放入结果集中 b) 取出 row source 1 的 row 2（第二行数据），遍历 row source 2 的所有行并检查是否有匹配的，取出匹配的行放入结果集中 c) …… 若 row source 1 （即驱动表）中返回了 N 行数据，则 row source 2 也相应的会被全表遍历 N 次。 因为 row source 1 的每一行都会去匹配 row source 2 的所有行，所以当 row source 1 返回的行数尽可能少并且能高效访问 row source 2（如建立适当的索引）时，效率较高。 嵌套循环的表有驱动顺序，注意选择合适的驱动表。嵌套循环连接有一个其他连接方式没有的好处是：可以先返回已经连接的行，而不必等所有的连接操作处理完才返回数据，这样可以实现快速响应。 应尽可能使用限制条件（Where过滤条件）使驱动表（row source 1）返回的行数尽可能少，同时在匹配表（row source 2）的连接操作关联列上建立唯一索引（UNIQUE INDEX）或是选择性较好的非唯一索引，此时嵌套循环连接的执行效率会变得很高。若驱动表返回的行数较多，即使匹配表连接操作关联列上存在索引，连接效率也不会很高。 哈希连接哈希连接只适用于等值连接（即连接条件为 = ） HASH JOIN对两个表做连接时并不一定是都进行全表扫描，其并不限制表访问方式； 内部连接过程简述： a) 取出 row source 1（驱动表，在HASH JOIN中又称为Build Table） 的数据集，然后将其构建成内存中的一个 Hash Table（Hash函数的Hash KEY就是连接操作关联列），创建Hash位图（bitmap） b) 取出 row source 2（匹配表）的数据集，对其中的每一条数据的连接操作关联列使用相同的Hash函数并找到对应的 a) 里的数据在 Hash Table 中的位置，在该位置上检查能否找到匹配的数据 实验2：B树索引与位图索引的比较sql语句-- 小基数SELECT T_MOVIE.NAME_, T_MOVIE.YEAR_FROM T_MOVIE,T_MOVIE_REGION,T_REGIONWHERE T_REGION.ID_=T_MOVIE_REGION.REGION_IDAND T_MOVIE.ID_=T_MOVIE_REGION.MOVIE_IDAND T_REGION.NAME_='美国'AND T_MOVIE.SCORE_&gt;6;-- 大基数SELECT T_MOVIE.NAME_, T_MOVIE.YEAR_FROM T_ACTOR,T_ACT,T_MOVIEWHERE T_ACTOR.NAME_='Tom Byron'AND T_MOVIE.SCORE_&gt;6AND T_ACTOR.ID_=T_ACT.ACTOR_IDAND T_MOVIE.ID_=T_ACT.MOVIE_ID; 索引语句-- B树CREATE INDEX IX_MOVIE_SCORE ON T_MOVIE(SCORE_);CREATE INDEX IX_MOVIE_NAME ON T_MOVIE(NAME_);CREATE INDEX IX_ACTOR_NAME ON T_ACTOR(NAME_);-- BitMapCREATE BITMAP INDEX IXBM_MOVIE_NAME ON T_MOVIE(NAME_);CREATE BITMAP INDEX IXBM_MOVIE_SCORE ON T_MOVIE(SCORE_);CREATE BITMAP INDEX IXBM_ACTOR_NAME ON T_ACTOR(NAME_); 查询消耗B树索引（小基数） 位图索引（小基数） 不加索引（大基数） B树索引（大基数） 位图索引（大基数） 分析即使在字段基数较大的情况下，位图索引依然有比B树索引更好的表现。但是有个问题，创建位图索引时所需的时间更长。此外，由于表中该字段的更改都会导致对位图的修改，所以位图索引不适用于并发的情况。 实验3：Oracle优化器对索引的选择 关于索引索引类型 B树索引（默认的索引） &gt; CREATE INDEX IX_MOVIE_SCORE ON T_MOVIE(SCORE_);&gt; 位图索引：以位图的形式存储每个值对应的的一组rowid &gt; CREATE BITMAP INDEX IXBM_REGION_NAME ON T_REGION(NAME_);&gt; 基于函数的索引：利于对某个字段查询时需要同时使用函数或计算的情景 &gt; CREATE INDEX upper_ix ON employees (UPPER(last_name)); &gt; 分区索引：本地分区索引的分区完全依赖于其索引所在表，而全局分区索引的分区机制和表分区可能一样也可能不一样 range范围分区 &gt; CREATE INDEX cost_ix ON sales (amount_sold)&gt; GLOBAL PARTITION BY RANGE (amount_sold)&gt; (PARTITION p1 VALUES LESS THAN (1000),&gt; PARTITION p2 VALUES LESS THAN (2500),&gt; PARTITION p3 VALUES LESS THAN (MAXVALUE));&gt; hash哈希分区 &gt; CREATE INDEX cust_last_name_ix ON customers (cust_last_name)&gt; GLOBAL PARTITION BY HASH (cust_last_name)&gt; PARTITIONS 4;&gt; list列表分区：一个分区对应指定列的特定的值，以列举的方式进行分区 组合分区（range-hash，range-list） 什么时候用索引对于Oracle的CBO来说，只有在使用索引能提高效率（估算的效率）时才会使用索引。对于程序员自己进行数据库管理的时候，一般有： 需要使用索引来优化查询的情况： 一个属性的值分布非常广，变化的范围跨度很大。 一般来说，常常需要被用在SQL语句的where中的限制条件的属性最好为其建立索引。 表经常被访问且需要访问的数据量仅占一部分。 不适合用索引的情况： 表很小 表经常被更新 属性不经常作为where中的限制条件的属性存在 查询得到的数据占总量的很大部分 对于数据经常更新的情况，DBA要定时进行索引的重构（rebuild）以维持索引的可用性。 影响优化器决策的因素 进行全表扫描需要读取的数据块数量； 进行索引查询需要读取的数据块数量，这主要是基于对WHERE子句谓词返回的记录数目估计； 进行全表扫描时多块读的相关开销，以及为满足索引查询进行的单块读的开销； 内存中对缓存中的索引块和数据块数目的假设。 索引失效的可能原因以下是一些常见的定义了索引当Oracle并未使用的原因： 不等于情况，即“&lt;&gt;” 字符串匹配like中百分号在第一位的情况，即“%XXX” 表没有进行分析更新统计信息 使用复合索引但单独引用且非复合索引的第一属性 对索引进行计算，此时需要建立索引函数 属性为字符串但在where中没有加引号 使用not in，not exists 使用了其他索引 强制使用索引如果想要强制使用索引，则可以在查询语句的select单词后加上/*+index (tablename indexname)*/，这样可以规定Oracle选择使用indexname的索引的执行计划。该方法已在前面实验中使用，不再赘述。 sql语句SELECT T_MOVIE.NAME_, T_MOVIE.YEAR_FROM T_MOVIE,T_MOVIE_REGION,T_REGIONWHERE T_REGION.ID_=T_MOVIE_REGION.REGION_IDAND T_MOVIE.ID_=T_MOVIE_REGION.MOVIE_IDAND T_REGION.NAME_='美国'AND T_MOVIE.SCORE_&gt;6; 查询消耗不用索引（不论是B树索引还是位图索引都不使用） 强制使用B树索引 强制使用位图索引 sql语句SELECT T_MOVIE.NAME_, T_MOVIE.YEAR_FROM T_MOVIE,T_MOVIE_REGION,T_REGIONWHERE T_REGION.ID_=T_MOVIE_REGION.REGION_IDAND T_MOVIE.ID_=T_MOVIE_REGION.MOVIE_IDAND T_REGION.NAME_='美国'AND T_MOVIE.SCORE_&gt;9; 查询消耗B树索引（未使用） 位图索引 分析由此可见，即使在有索引的情况下，oracle优化器也可能选择不使用索引。CBO优化器会对每种执行计划计算一个COST，并采用COST最小的执行计划。如果一个表有索引或多种索引，其会选择最好的一种索引方式扫描表，或者甚至不用索引而用全局扫描方式。 另外对于符合筛选条件的数据，当占全表的比例越小、数据量越小时，使用索引的可能性越大。如在这次实验中，条件为”T_MOVIE.SCORE_ &gt;9”时会使用索引，而”T_MOVIE.SCORE_ &gt;6”时不会。 此外，由于位图索引导致的COST要小于B树索引，因此在相同的查询中，使用位图索引的可能性比B树索引更大。 实验4：Oracle分区索引sql语句SELECT T_COMMENT.SUMMARY_,T_COMMENT.SCORE_,T_COMMENT.TIME_FROM T_COMMENT,T_MOVIEWHERE T_MOVIE.ID_=T_COMMENT.MOVIE_IDAND T_COMMENT.SCORE_&gt;6AND T_MOVIE.NAME_='Blindsided'; 实验结果 未分区表+无索引 未分区表+B树索引 未分区表+位图索引 分区表+无索引 分区表+全局不分区B树索引 分区表+本地(哈希分区)B树索引 分区表+本地(哈希分区)位图索引 分区表+全局哈希分区索引 分区表+全局范围分区索引 分析 在未建索引时，分区表的COST是未分区表的十倍多。原因是分区所依据的键（字段）不是直接的查询条件——我们以评论表的movie_id字段为依据建哈希分区表，但在查询的时候并不直接以movie_id为查询条件。导致连接表的时候，需要访问多个分区，反而造成COST大大增长。 后来我们重新设计一个以movie_id为查询条件的sql语句，结果显示分区表的COST大约是未分区表的1/4（一共分了4个区），证明在以分区依据的字段为直接查询条件时，分区表能够体现比较好的性能，能够避免对一部分数据的访问。 在分区表上建索引比在未分区表上建索引后的开销更小，不论分区表上的索引是全局还是本地，不论是否是分区索引。在我们的实验场景中，尽管movie_id不是直接的查询条件而是join表的条件，但是在添加索引后，依然能够大大减少join表的开销从而提升效率。 在我们的实验场景中，全局的分区索引，不论是哈希分区还是范围分区，COST是一样的。 本地索引的效率略微比全局索引的效率好。根据查到的资料，本地索引的可维护性好，能够自动维护，不需要人工干预，但因把索引分成多个分区导致每次的索引访问都需要遍历所有索引分区，所以索引访问性能下降。因此比较适合OLAP系统。而全局索引的可维护性差，分区表发生改变时，需要用命令手动更新索引，但索引访问性能比本地分区索引要好。因此比较适合OLTP系统。 实验5：Oracle使用复合索引SQL语句select T_COMMENT_1.SUMMARY_, T_COMMENT_1.SCORE_from T_COMMENT_1, T_MOVIEwhere T_MOVIE.ID_ = T_COMMENT_1.MOVIE_ID and T_COMMENT_1.SCORE_ &gt; 7 and T_MOVIE.NAME_ = 'The Notebook'; 第一次查询：T_COMMENT_1上只有主键的唯一索引。 第二次查询：在MOVIE_ID上建立一个B-tree索引COMMENT_1_MOVIE。 create index COMMENT_1_MOVIE on T_COMMENT_1(MOVIE_ID); 第三次查询：使第二次的索引invisible，在SCORE_上建立一个B-tree索引T_COMMENT_SCORE_INDEX。 alter index COMMENT_1_MOVIE invisible;create index T_COMMENT_SCORE_INDEX on T_COMMENT_1(SCORE_); 第四次查询：将第二次和第三次的索引都保持为visible，在MOVIE_ID和SCORE_上建立一个复合索引COMMENT_1_MOVIE_SCORE。 alter index COMMENT_1_MOVIE visible;create index COMMENT_1_MOVIE_SCORE on T_COMMENT_1(MOVIE_ID, SCORE_); 实验结果第一次查询： 全表扫描，花销很大 第二次查询： 利用在MOVIE_ID上的索引，在T_COMMENT_1中访问的数据量和花销都大幅度下降。 第三次查询： 如果只有在SCORE_上的索引，根据CBO，Oracle并没有使用这个索引，而是依旧使用全表扫描，可知该索引并没有提升性能。 易知，如果在这个时候将MOVIE_ID上的索引设为visible，Oracle会使用MOVIE_ID上的索引。 第四次查询： Oracle使用了复合索引，尽管在当前问题下COST花销与只有MOVIE_ID的索引差不多，但是其访问的记录数（CARDINALITY）显著减小，体现了复合索引给查询带来的性能提升。 实验6：物化视图对SQL查询性能的提升SQL语句原始查询语句： select T_DIRECTOR.NAME_, T_MOVIE.NAME_ MOVIE_NAME, AVG(T_COMMENT_1.SCORE_) SCOREfrom T_DIRECTOR, T_DIRECT, T_MOVIE, T_COMMENT_1where T_DIRECTOR.ID_ = T_DIRECT.DIRECTOR_ID and T_DIRECT.MOVIE_ID = T_MOVIE.ID_ and T_COMMENT_1.MOVIE_ID = T_MOVIE.ID_ and T_DIRECTOR.NAME_ like '黑泽明%'group by T_DIRECTOR.NAME_, T_DIRECTOR.ID_, T_MOVIE.NAME_, T_MOVIE.ID_, T_MOVIE.YEAR_ ; 创建一个普通视图： CREATE VIEW DIRECTOR_MOVIE_FAKEASselect T_DIRECTOR.NAME_, T_DIRECTOR.ID_, T_MOVIE.NAME_ MOVIE_NAME, T_MOVIE.ID_ MOVIE_ID, T_MOVIE.YEAR_, AVG(T_COMMENT_1.SCORE_) SCOREfrom T_DIRECTOR, T_DIRECT, T_MOVIE, T_COMMENT_1where T_DIRECTOR.ID_ = T_DIRECT.DIRECTOR_ID and T_DIRECT.MOVIE_ID = T_MOVIE.ID_ and T_COMMENT_1.MOVIE_ID = T_MOVIE.ID_group by T_DIRECTOR.NAME_, T_DIRECTOR.ID_, T_MOVIE.NAME_, T_MOVIE.ID_, T_MOVIE.YEAR_; 使用普通视图进行查询： select NAME_, MOVIE_NAME, SCORE from DIRECTOR_MOVIE_FAKE where NAME_ like '黑泽明%'; 创建物化视图： CREATE MATERIALIZED VIEW DIRECTOR_MOVIEBUILD IMMEDIATEREFRESH FORCEON DEMANDENABLE QUERY REWRITEASselect T_DIRECTOR.NAME_, T_DIRECTOR.ID_, T_MOVIE.NAME_ MOVIE_NAME, T_MOVIE.ID_ MOVIE_ID, T_MOVIE.YEAR_, AVG(T_COMMENT_1.SCORE_) SCOREfrom T_DIRECTOR, T_DIRECT, T_MOVIE, T_COMMENT_1where T_DIRECTOR.ID_ = T_DIRECT.DIRECTOR_ID and T_DIRECT.MOVIE_ID = T_MOVIE.ID_ and T_COMMENT_1.MOVIE_ID = T_MOVIE.ID_group by T_DIRECTOR.NAME_, T_DIRECTOR.ID_, T_MOVIE.NAME_, T_MOVIE.ID_, T_MOVIE.YEAR_; 设置创建时生成数据，按需要刷新，刷新方式为FORCE。 根据视图进行如上查询： select NAME_, MOVIE_NAME, SCORE from DIRECTOR_MOVIE where NAME_ like '黑泽明%'; 由于物化视图与表类似，可以给其建立索引，以下给导演名建立索引： CREATE BITMAP INDEX DIRECTOR_MOVIE_INAME_INDEX ON DIRECTOR_MOVIE (NAME_); 再次使用物化视图查询： select NAME_, MOVIE_NAME, SCORE from DIRECTOR_MOVIE where NAME_ like '黑泽明%'; 实验结果使用原始查询： 具有极大的花销。 创建视图后的查询： 其执行计划与原始查询一致。 创建物化视图后的查询： 其直接在物化视图中进行查询，执行计划即为简单，花销大幅度减小。 给物化视图创建索引后的查询： 建立索引后通过范围索引扫描该物化视图进行查询，其COST数字小得令人惊奇。 分析1.建立普通视图并不能提升性能。因为普通是虚拟的，对视图的操作实际都转变为了对各表的SQL操作，其与原始查询完全一致。 2.物化视图是一种物理表，对于物化视图的查询是直接的，跟表一样。因此建立物化视图可以大幅度减小花销，但是同时，物化视图也会产生大量的维护成本。因此程序员应该根据实际情况建立物化视图以优化查询。 3.物化视图同样可以增添索引，增加索引后Oracle对物化视图可以通过索引进行扫描，进一步提高效率。 物化视图与普通视图视图只是一种虚拟表。实际上，对视图的查询真正转换成了相应的SQL语句再对各表进行连接查询，因此其性能提升有限，只是方便了使用。 而物化视图是实质化的视图，是物理表，可以像表一样进行查询，建立索引，占用真正的存储空间，需要被刷新。 刷新模式on demand：顾名思义，仅在该物化视图“需要”被刷新了，才进行刷新(REFRESH)，即更新物化视图，以保证和基表数据的一致性; on commit：提交触发，一旦基表有了commit，即事务提交，则立刻刷新，立刻更新物化视图，使得数据和基表一致。一般用这种方法在操作基表时速度会比较慢。 创建物化视图时未作指定，则Oracle按 on demand 模式来创建。 刷新方法完全刷新（COMPLETE）： 会删除表中所有的记录（如果是单表刷新，可能会采用TRUNCATE的方式），然后根据物化视图中查询语句的定义重新生成物化视图。 快速刷新（FAST）： 采用增量刷新的机制，只将自上次刷新以后对基表进行的所有操作刷新到物化视图中去。FAST必须创建基于主表的视图日志。对于增量刷新选项，如果在子查询中存在分析函数，则物化视图不起作用。 FORCE方式： 这是默认的数据刷新方式。Oracle会自动判断是否满足快速刷新的条件，如果满足则进行快速刷新，否则进行完全刷新。 实验7：Oracle In Memory性能分析Sql语句SELECT T_MOVIE.NAME_, SUM(T_COMMENT_2.SCORE_) s FROM T_MOVIE,T_COMMENT_2 WHERE T_MOVIE.ID_=T_COMMENT_2.MOVIE_ID GROUP BY T_MOVIE.NAME_ ORDER BY s DESC; 设置In MemoryALTER TABLE T_MOVIE.NAME_ IN MEMORY; 实验结果原始查询： In Memory查询： 结果分析遗憾的是与想象的不同，Oracle和Oracle In Memory在COST上面结果相同，但是事实上在我们同样的实验环境下测试二者时间时，In Memory确实会比Oracle好很多。其实简单思考一下，这是应该的，前面我们说过执行时间t = Block数量 * Block处理时间，不难知道差距还是出在Block处理时间上。 实验8：Oracle执行计划浅析(Oracle表的访问方式)对T_MOVIE表进行查询，其本身有在其主码(ID_)上的UNIQUE INDEX和LENGTH_上的B-tree INDEX。 根据UNIQUE INDEX（ID_）返回唯一记录select * from T_MOVIE where ID_ = 20050; 使用的是索引唯一扫描 根据ID_返回少部分记录select * from T_MOVIE where ID_ &lt; 10; 使用的是索引范围扫描 根据LENGTH_返回大量数据select * from T_MOVIE where LENGTH_ &lt;100; 全查询MOVIE_和TYPE_返回其ID_全查询MOVIE_： select ID_ from T_MOVIE; 采用的是索引快速扫描（因为数据量较多） 且返回结果无顺序（从578开始，一段有序，即代表是一个索引数据块）。 全查询TYPE_: select ID_ from T_TYPE; 采用的是索引全扫描（因为数据量较小） 返回结果有顺序 执行计划中的访问方式访问方式即分为全表扫描（TABLE ACCESS FULL）和各种类型索引扫描（TABLE INDEX SCAN）。Oracle会根据表和索引的信息，推算执行的SQL语句从表中取多少数据以及这些数据是怎么分布的。 TABLE ACCESS FULL（全表扫描）Oracle会读取表中所有的行，并检查每一行是否满足SQL语句中的 where限制条件。全表扫描时可以使用多块读（即一次I/O读取多块数据块）操作来提升吞吐量。数据量太大的表不建议使用全表扫描，除非本身需要取出的数据较多，占到表数据总量的 5% ~ 10% 或以上。 TABLE ACCESS ROWID（通过ROWID的表存取）ROWID是由Oracle自动加在表中每行最后的一列伪列，表中并不会物理存储ROWID的值。程序员可以像使用其它列一样使用它，但不能对该列的值进行增、删、改操作。一旦一行数据插入后，则其对应的ROWID在该行的生命周期内是唯一的，即使发生行迁移，该行的ROWID值也不变。 ROWID可以被视为每条记录的“指针”。它指出了该行所在的数据文件、数据块以及行在该块中的位置，所以通过ROWID可以快速定位到目标数据上，这也是Oracle中存取单行数据最快的方法。 TABLE ACCESS BY INDEX SCAN（索引扫描）在索引块中，既存储每个索引的键值，也存储具有该键值的行的ROWID。因此索引扫描其实分为两步：扫描索引得到对应的ROWID；通过ROWID定位到具体的行读取数据。 索引扫描主要分为以下几种： INDEX UNIQUE SCAN 索引唯一扫描对应UNIQUE INDEX（唯一性索引）的扫描方式，其只会应用在返回一条记录的情况下。该点在之前的实验中已经描述。 INDEX RANGE SCAN 索引范围扫描主要是使用在需要返回多行记录的情况下，常见为以下三种： 在唯一索引列上使用了范围操作符（如：&gt; &lt; &lt;&gt; &gt;= &lt;= between） 在组合索引上，只使用部分列进行查询（查询时必须包含前导列，否则会走全表扫描） 对非唯一索引列上进行的任何查询 如果在查询的过程中需要访问的记录数很多，分布很广，这个时候Oracle会根据CBO原则认为使用索引的花销可能比全表扫描大，会使用全表扫描。 INDEX FULL SCAN 索引全扫描进行全索引扫描时，查询出的数据都必须从索引中可以直接得到。其常发生在要查询的列包含唯一索引且需要对表中的所有数据都要查询。索引全扫描返回的结果有顺序。 INDEX FAST FULL SCAN 索引快速全扫描索引快速全扫描与索引全扫描类似，只是其在查找索引时会用一种更为快速的方式（简单来说是根据索引块的物理顺序而省去较为繁琐的逻辑顺序），其更适合于数据量大的表进行全查询，其一个特点就是返回的记录不按照顺序。 四、TimesTen实验过程实验概述调用自己改写的 AliTT11.sql，查看 SQLPrepare，SQLExecute，FetchLoop 的查询时间； 所有实验中，查询时间分为增加索引前、增加索引后、按照 timesten 建议添加索引三类，针对每一类时间分别有第一次执行时间和之后的平均查询时间两种； 在首次执行查询语句时，timesten首先需要对语句进行预编译，因此首次执行的 SQLPrepare 时间相比之后的时间较长，之后的准备时间就相应缩短了很多。 实验1实验内容某地区评分6以上的所有电影的名字和上映时间 查询语句SELECT DBIM.T_MOVIE.NAME_, DBIM.T_MOVIE.YEAR_FROM DBIM.T_MOVIE, DBIM.T_MOVIE_REGION, DBIM.T_REGIONWHERE DBIM.T_REGION.ID_ = DBIM.T_MOVIE_REGION.REGION_IDAND DBIM.T_MOVIE.ID_ = DBIM.T_MOVIE_REGION.MOVIE_IDAND DBIM.T_REGION.NAME_ = '美国'AND DBIM.T_MOVIE.SCORE_ &gt; 6; 添加的索引 表明 列名 索引类型 是否唯一 Movie id_ hash unique Movie score_ range Region id_ hash unique Movie_region region_id hash Movie_region movie_id hash 查询时间 时间类型 Before1 Before2 After1 After2 建议1 建议2 提高百分比 SQLPrepare 0.001845 0.000059 0.000878 0.000054 0.000807 0.000055 SQLExecute 0.075809 0.061819 0.000037 0.000025 0.000034 0.000025 99.96% FetchLoop 0.000004 0.000002 0.000002 0.000001 0.000003 0.000002 执行计划 (before) 执行计划 (after) 原因分析添加索引后速度大大提升，因为在 region 表中指定了查询条件，添加索引后可以快速从表项中匹配到指定条件的项；在添加之前，timesten 自动帮我们在 movie 表上的 id 字段上添加了临时哈希索引，除此之外，我们额外为几个 where 条件语句的查询字段都增加了索引， 因此提高了效率。 执行计划 before 在两层嵌套循环中，顺序执行在region表中的查询、region表与联系表的join，循环结束后生成一个指定地区内的所有电影联系表；内层嵌套完成后，通过散列索引匹配movie表与内存循环生成的联系表，join筛选后生成结果列表 after 添加索引之后，过程与添加之前相同，但由于内层循环内使用散列索引而不是顺序执行，因此查询速度比较快，加上没有临时创建索引的时间开销，所以相比之下大大提高了查询效率。 实验2实验内容所有地区全部电影的平均评分排行榜（前100） 查询语句SELECT * FROM (SELECT DBIM.T_REGION.NAME_, SUM(DBIM.T_MOVIE.SCORE_) sFROM DBIM.T_REGION, DBIM.T_MOVIE_REGION, DBIM.T_MOVIEWHERE DBIM.T_MOVIE.ID_ = DBIM.T_MOVIE_REGION.MOVIE_IDAND DBIM.T_REGION.ID_ = DBIM.T_MOVIE_REGION.REGION_IDGROUP BY DBIM.T_REGION.NAME_ORDER BY s DESC) WHERE ROWNUM &lt; 101; 添加的索引 表明 列名 索引类型 是否唯一 Movie id_ hash unique Movie score_ range Region name_ hash unique Region id_ hash unique Movie_region region_id hash Movie_region movie_id hash 查询时间 时间类型 Before(1) Before(2) After(1) After(2) 建议(1) 建议(2) 提高百分比 SQLPrepare 0.001253 0.000081 0.001004 0.000054 0.000985 0.000056 SQLExecute 0.353111 0.335902 0.337983 0.313458 0.313004 0.312695 7% FetchLoop 0.000045 0.000020 0.000018 0.000018 0.000018 0.000017 执行计划 (before) 执行计划 (after) 原因分析添加索引之前，timesten 自动在 movie 和 region 表的 id 字段上都设置了相应的哈希索引，而我们添加索引后与添加之前的执行计划中的索引项没有差别，因此效率几乎没有变化，加上 sum 聚合操作、group by、order by 操作都要进行费时间的全表扫描，所以需要较长时间完成查询。 实验1和2分析总结指定条件的查询： 建立索引之前 timesten在某个相对较小的表上建立临时索引（散列索引或范围索引），在其他表上进行顺序扫描，执行查询语句中的条件匹配，建立索引的过程会造成时间上的消耗； 建立索引之后 自己建立的索引覆盖timesten优化建立的索引，由于索引提前建立，因此没有建立索引带来的额外时间开销，而且在此类查询中我们在所有查询涉及字段上都建立了索引（tt自身优化通常只在一个表上建立索引），所以与建立索引之前相比有极大的性能提升。 聚合查询： 执行计划Before： 先顺序扫描关系表act的记录字段id，利用临时HASH索引 actor.id_，将act表中对应记录与act的记录通过字段相连；对(这些/该)拼接记录逐条利用临时HASH索引 movie.id ,接上movie表中符合条件的记录字段。 执行计划After： 先顺序扫描关系表movie的记录字段id ，利用HASH索引 act.id_，将act表中对应记录与act的记录通过字段相连；针对第一次hash检索出的 act.id，再对(这些/该)拼接记录逐条利用临时HASH索引 actor.id ,接上actor表中符合条件的记录字段。 主要原因在于：第一次顺序扫描的关系表act，外码引用actor表的主码(1:1)，movie表(1:1)，hash索引查询唯一记录快；第二次顺序扫描的表为movie表，将对应多条act表里的记录（1:many），对应多个演员(1:many)。 实验3：AWT创建 AWT 直写缓存组 缓存表 t_moive t_comment_1 选择理由 动态缓存组适用于不从 oracle 中预加载数据的场景 Movie 表和评论表体量较大，不需要从 oracle 中提前加载 测试 AWT 修改数据 修改电影评论表 修改语句 UPDATE DBIM.T_COMMENT_1SET SUMMARY_=&apos;A&apos;WHERE DBIM.T_COMMENT_1.SCORE_&gt;8AND DBIM.T_COMMENT_1.MOVIE_ID = 1; 踩坑 update语句指定修改的表名后，set字段不需要再次声明表名（否则报错） 修改数据前要开启 replication agent 执行update语句后要提交事务 实验4：查看不同数据类型对查询效率的影响表字节大小 表名 行数 字节大小 有数据类型映射的字节大小 节约百分比 Movie 292352 47301232（nomapping） 26293000（optimal） 45% Comment 9805336 2528884600（nomapping） 404967952（optimal） 84% 压缩设置 数据类型映射结果 表 字段 noMapping standardMapping aggressive Region id_ NUMBER(11,0) TT_BIGINT TT_SMALINT Region name_ VARCHAR(255 BYTE) VARCHAR(255 BYTE) VARCHAR(80 BYTE) 结果分析 对相同的表来说，从 oracle 导入 timesten 中如果不进行压缩（nomapping），与进行最优化数据类型映射+aggressive mapping + optimal compression 相比，大约浪费了45%的空间； 对于不同数量级的表来说，千万数量级的 comment 表不进行压缩时浪费84%所有的空间，比十万数量级的 movie 表浪费的空间多了接近一倍。 实验5：根据优化建议建立索引SQL语句Command&gt; call ttIndexAdviceCaptureOutput(0);&lt; 6, create index T_MOVIE_i1 on DBIM.T_MOVIE(ID_); &gt;&lt; 7, create index T_COMMENT_1_i2 on DBIM.T_COMMENT_1(MOVIE_ID,SCORE_); &gt;2 rows found. 实验对象 实验3.2语句 实验结果 Before：自己建立索引后的查询时间 After：根据 timesten 查询优化建议建立索引后的查询时间 时间类型 Before(1) Before(2) After(1) After(2) 提高百分比 SQLPrepare 0.001527 0.000049 0.000822 0.000049 SQLExecute 4.139655 3.556375 3.301318 3.302092 7.18% FetchLoop 0.000047 0.000027 0.000018 0.000017","categories":[{"name":"数据库","slug":"数据库","permalink":"http://blog.guitoubing.top/categories/数据库/"}],"tags":[{"name":"Oracle","slug":"Oracle","permalink":"http://blog.guitoubing.top/tags/Oracle/"},{"name":"Timesten","slug":"Timesten","permalink":"http://blog.guitoubing.top/tags/Timesten/"}]},{"title":"数据仓库期末项目文档","slug":"数据仓库期末项目文档","date":"2018-12-31T09:32:45.000Z","updated":"2021-05-16T11:00:38.152Z","comments":true,"path":"2018/12/31/数据仓库期末项目文档/","link":"","permalink":"http://blog.guitoubing.top/2018/12/31/数据仓库期末项目文档/","excerpt":"简介本项目我们基于Stanford University中的Amazon Movie Comment数据，利用爬虫技术爬取了数十万的电影信息数据以及数百万计的电影评论数据，并通过搭建Neo4j图数据库、MySQL关系型数据库、Influx时序数据库及Hive分布式数据库对数据进行存储、分析及实现功能，同时对于部分功能需求针对这4种数据库进行效率对比分析。","text":"简介本项目我们基于Stanford University中的Amazon Movie Comment数据，利用爬虫技术爬取了数十万的电影信息数据以及数百万计的电影评论数据，并通过搭建Neo4j图数据库、MySQL关系型数据库、Influx时序数据库及Hive分布式数据库对数据进行存储、分析及实现功能，同时对于部分功能需求针对这4种数据库进行效率对比分析。 系统架构Neo4j 操作系统：macOS Mojave 10.14.1 硬件：Core i5 &amp; 16GB RAM 软件：Neo4j Desktop Version 1.1.10 (1.1.10.436) 选择理由： 高性能：Neo4j以图的遍历算法来帮助查询数据，查询时从一个节点开始，根据其连接的关系，快速和方便地找出它的邻近节点。这种查找数据的方法并不受数据量的大小所影响，因为邻近查询始终查找的是有限的局部数据，不会对整个数据库进行搜索。所以，Neo4j具有非常高效的查询性能，相比于RDBMS可以提高数倍乃至数十倍的查询速度。而且查询速度不会因数据量的增长而下降。 灵活性：图数据结构的自然伸展特性及其非结构化的数据格式让Neo4j的数据库设计可以具有很大的伸缩性和灵活性，使其可以随着需求的变化而增加的节点、关系及其属性并不会影响到原来数据的正常使用，因此在项目后期的推进中，我们也可以不断的快速修改neo4j数据库中的内容来满足我们的查询需求。 直观性：图数据库使用图的形式作为数据库最主要的展现形式，可以更清楚的帮助我们理解整个数据库中数据之间的联系，Cypher语言的灵活性也帮助我们更轻松的操控数据库 存储模型简介： 本项目中主要建立了Neo4j的两个不同的库，一个库是围绕电影的相关信息，我们在其中存储了和电影有关的所有信息，包括导演，制片人，演员，类别，语言，字幕，编剧等等，节点与节点之间通过不同的关系相连接。第二个库针对合作关系，分别存储了导演，演员，以及类别，通过节点与节点之间的关系，记录他们彼此的合作次数，类别的引入也帮助我们分析导演的执导风格。 存储模型：图 性能对比分析： 数据存储：Neo4j对于图的存储自然是经过特别优化的。不像传统数据库的一条记录一条数据的存储方式，Neo4j的存储方式是：节点的类别，属性，边的类别，属性等都是分开存储的，这将大大有助于提高图形数据库的性能。在Neo4j 中属性，关系等文件是以数组作为核心存储结构；同时对节点，属性，关系等类型的每个数据项都会分配一个唯一的ID，在存储时以该ID 为数组的下标。这样，在访问时通过其ID作为下标，实现快速定位。 数据读写：在Neo4j中，存储节点时，每个节点都有指向其邻居节点的指针，可以让我们在O(1)的时间内找到邻居节点。另外，按照官方的说法，在Neo4j中边是最重要的,是”first-class entities”，所以单独存储，这有利于在图遍历的时候提高速度，也可以很方便地以任何方向进行遍历。邻近查询帮助Neo4j始终查找的是有限的局部数据，不会对整个数据库进行搜索。所以，Neo4j具有非常高效的查询性能，相比于RDBMS可以提高数倍乃至数十倍的查询速度。而且查询速度不会因数据量的增长而下降。 MySQL 操作系统：macOS Mojave 10.14.1 Beta 硬件：Intel(R) Core(TM) i5-3470 CPU @ 3.20GHz/ 4 GB 1600 MHz DDR3 软件：Docker 1.13.1/ MySQL 5.7 选择理由： MySQL是时下使用率最高的几款关系型数据库之一，且其体积相较其他关系型数据库更小巧且性能不输大型关系型数据库。关系型数据库是我们最常接触也是在对数据进行存储时会最先想到的数据库类型。我们想要借助关系型数据库以及行式存储对我们的数据进行存储，并通过对应的数据库操作对存储对数据进行分析/查询，实现我们对应的目的。 存储模型简介： 在本项目中我们选择雪花模型作为我们关系型数据库的存储模型。雪花存储模型使用规范化的数据，数据在数据库内部是组织好的，消除冗余以减少数据量。相比之下，星型模型使用的是反规范化的数据，会在存储时存储大量的冗余数据。规范化存储数据同时也带来查询时间上的消耗，其查询更新速度会慢于星型存储模型。但是考虑到我们项目到数据单表最大12万左右，对于这个数量级到数据星型模型的查询速度相比雪花模型没有非常明显的差距，而雪花模型能够帮助我们减少了很多不必要的冗余数据的存储，所以我们选用了雪花模型。我们的数据库设计了实体表与关系表，各个实体表有自己的唯一的主键，实体表之间的联系使用关系表进行关联，减少了很多实体数据的存储，符合第三范式。 性能分析： 在一开始，我们并没有对每个表建立相应的索引，在这种情况下我们单表的query速度在一个可接受的范围内，但是一旦涉及多表联合查询，如查询每个导演执导的电影数量时，需要关联三张表，在这种情况下查询速度非常的慢，因为其中涉及来表的结合与数据的聚合查询。针对联合查询过慢的速度下，我们为每张实体表以及关系表建立主码索引，并且在常用的搜索字段，如电影的上映日期上建立对应的索引，并且在这种大量数据的情况需要先对表建立索引再将数据导入，因为导入数据之后再建立索引会消耗大量的时间。索引建立之后再进行同样的多表联合查询操作，可以发现速度得到了明显的提升，在当前十万级别的数据量下查询耗时基本在五十毫秒之内。所以在MySQL中建立适当的索引能够在很大程度上提升查询的速度，同时也会牺牲一定的查询/更新效率。 Influx 操作系统：windows10 硬件：Intel(R) Core(TM) i5-6300HQ CPU @2.30GHz &amp; 8GB RAM 软件：influx1.7.1 选择理由：首先，查询场景中有用到对世界特性比较敏感的数据，例如，根据时间查询等，所以使用influxDB。influxDB继承了LSM Tree的顺序写入的特点，所以写入性能很好（先把大量的数据顺序写，然后持久化到磁盘。）时序数据库每次读取数据都是读取固定series的指定时间范围的连续数据，因为是顺序写入，所以这种读取比较快速。 存储模型简介：influxdb中我们主要存 电影id，电影类别，电影语言，电影观看人数，电影上映时间。其中，将电影类别与电影语言当做tag存储，电影id以及电影观看人数当做field存储，其中上映时间就是时间戳存储。 存储模型： 性能对比分析： InfluxDB用于存储大量的时间序列数据，并对这些数据进行快速的实时分析。SQL数据库也可以提供时序的功能，但时序并不是其目的。在InfluxDB中，timestamp标识了在任何给定数据series中的单个点。就像关系型数据库中的主键。InfluxDB考虑到schema可能随时间而改变，因此赋予了其便利的动态能力。但是由于在项目中，时间相关的数据较为固定，因此其性能的体现并不是特别好。 Hive 操作系统：macOS Mojave 10.14.1 硬件：Core i7 &amp; 16GB RAM 软件：Hive 3.1.1 &amp; Hadoop 3.1.1 &amp; MySQL 5.7 选择理由： Hive首先有很多以上数据库所不具有的优点，如扩展性和容错性，本项目我们选择hive来处理一部分数据主要是作为MySQL数据库的对照。针对我们项目的百万级的数据量来对比分析关系型数据库和分布式数据库在数据量较大时的性能优劣性，以此窥见数据仓库对比于数据库的所展现出来的优点。同时对于项目中的部分功能需求组合采用hive与其他数据库分治的方式，来实现复杂的功能需求，以此来学习工程中数据仓库与普通数据库结合的实现方法。而由于数据量及需求的限制，我们只可窥见数据仓库其作用的冰山一角，希望藉此加深我们对数据仓库的理解。 存储模型简介： 在hive中我们存储的数据与MySQL中一样。因此建立了与MySQL完全相同的存储结构。另外针对hive本身自带的不同的存储模型，我们还创建了textfile和ORCfile两种表存储结构。 分布式架构： 性能对比分析： \b\b从我们对于MySQL和Hive这两种比较有可比性的数据库之间的对比来说，MySQL的执行时间基本上是远远快于Hive的执行时间的。首先，考虑我们在这两种数据库中执行的操作，如果对于一开始数据从文件进入数据库中这一过程忽略的话，我们整个项目执行的都是OLAP即联机分析处理操作。Hive作为一个经典的数据仓库工具，本身应该是擅长执行OLAP操作的，因此暂且认为”操作”不是造成二者执行时间差异的原因；其次，Hive官网有句话\b”Hive在大型数据集上会表现出优越的性能”，考虑到我们的项目数据集\b中，最多的数据集是700多万条的用户评论数据，而基本功能的实现都是操作在数据量仅有10万余条的电影数据，我们猜测是数据量限制了Hive体现其\b优越性。因此我们作了如下实验：在等量的数据量变化上，我们比较二者变化前后的执行的时间，得到下表：就时间来说，很显然MySQL更胜一筹，但从增长比例来说，MySQL从9ms增长至271ms增长约为30倍，而\bHive增长约为5倍，由此我们可窥见Hive在大量数据集时性能会更加优越。然而在这过程中，\b我们所使用的Hive所采用的为textfile存储结构，意即内容即文件，表数据完全按照表结构存储成为文本文件，我们创建了t_comment表存储用户评论信息，表数据文件如下：\b从Hive官方文档我们得知Hive有其他更加优越的存储格式，它包含SequenceFile、RCFile、ORCFile，我们采取了所谓最优的ORCFile来Duplicate了用户评论表，想以此对比ORCFile之于TextFile的优点，我们创建了\bt_comment_orc表，并从t_comment中把数据原封不动的导入进来，可见表数据文件如下：不难看到\bORC表文件(260MB)明显比TextFile表文件(705MB)小多了，至于性能，同样对于上述实验，我们添加了ORC表的结果：结果显而易见，当数据达到\b数百万量级时，Hive较优的使用方法下已经要比MySQL要稍显胜势了。通过以上两点以及常识我们不难看出： 限制Hive的效率的因素： 数据量 \b计算框架 Hive在我们项目中使用的是MapReduce框架来执行分布式计算，然而比\b现在已经有很多比MapReduce快得多的计算框架例如Spark等，因此若使用这些框架必定会使 网络通信 由于我们的集群搭建在Docker容器中，其间数据通过程序写定的程序通道传输而非真实的网络通信，因此暂且看不出网络对执行的影响，而真实场景中，这必是一项重要的考虑因素 百万级数据的OLAP场景或者OLTP操作需求较多的场景下，MySQL(关系型数据库)是优选 千万乃至亿万级数据的批处理、分析场景，Hive(数据仓库/分布式数据库)在存储、读取、分析效率上都要更优 其三，上述操作均是在单表查询的前提下，但是在多表查询情况下Hive的效率如何呢？先看测试结果，我们仅在”导演-执导-电影”三表上做了多表查询，执行”某导演执导电影的数量”的操作，执行时间记录如下：此现象引出了数据仓库在实际应用中的一种常见处理方式：为了提高速度而产生数据冗余。Hive中的表是很特殊的，其没有主键、外键同时库中各个表之间的冗余会很明显，这使得\b管理人员方便针对各种功能设计所需的信息表，这也是数据仓库作为大量数据集的OLAP最佳选择的原因之一。 \b性能对比 走势变化： 由图可见，四种数据库中执行时间都是先较多然后减少最后趋于稳定，我们对其分析可能是jdbc在首次连接时需要较多时间进行网络通信，当一次连接建立后，我们并没有关闭该连接，在此基础上程序执行后续的事务才应当是其真实的操作时间。 功能对比： 不同的数据库，在不同的功能需求下各有优劣。举个例子，在查询实体间的关系时，对于完全符合3NF的关系型数据库来说，可能需要多表连接查询，这明显会消耗大量时间，而对于基于relation的数据库例如Neo4j来说，类似查询正是其强项。 总结 本项目使用了JavaWeb框架，并基于sementicUI进行前端开发。 在数据库选择上，我们使用了Mysql，hive，Neo4j以及influxDB四个不同的数据库进行横向纵向比对，通过实现一定的基本功能搜索以及多表联查，统计他们的性能，查询时间等数据并进行相应的分析，对于不同数据库的优劣势有了更为清晰的了解。 在项目过程中，我们将上课学到的知识应用到实践中，尝试了雪花，星型等不同的存储结构，并根据自己的项目实情选择了最适合我们的项目存储结构。针对不同的实验现象，我们也通过网络等资源来进行辅助学习，帮助我们更好的了解不同数据库以及其不同的存储，读取等方式。 项目过程中，特别感谢老师和助教们的帮助，让我们更为深入了解了数据仓库技术，为我们今后的项目实践打下了扎实的基础。","categories":[{"name":"数据库","slug":"数据库","permalink":"http://blog.guitoubing.top/categories/数据库/"}],"tags":[{"name":"Neo4j","slug":"Neo4j","permalink":"http://blog.guitoubing.top/tags/Neo4j/"},{"name":"MySQL","slug":"MySQL","permalink":"http://blog.guitoubing.top/tags/MySQL/"},{"name":"Influx","slug":"Influx","permalink":"http://blog.guitoubing.top/tags/Influx/"},{"name":"Hive","slug":"Hive","permalink":"http://blog.guitoubing.top/tags/Hive/"}]},{"title":"Hadoop - 云计算期末项目","slug":"云计算期末项目文档","date":"2018-12-31T09:31:32.000Z","updated":"2021-05-16T11:00:29.634Z","comments":true,"path":"2018/12/31/云计算期末项目文档/","link":"","permalink":"http://blog.guitoubing.top/2018/12/31/云计算期末项目文档/","excerpt":"","text":"系统架构集群架构图 集群机器 主机名 内存 IP 软件 运行进程 node0 512MB 192.168.137.200 ZooKeeper QuorumPeerMain node1 512MB 192.168.137.201 ZooKeeper QuorumPeerMain node2 512MB 192.168.137.202 ZooKeeper QuorumPeerMain master 2GB 192.168.137.100 Hadoop,Hive,MySql JournalNode,NameNode,ResourceManager,DFSZKFailoverController,HiveServer2,MySql master1 2GB 192.168.137.10 Hadoop,Hive JournalNode,NameNode,ResourceManager,DFSZKFailoverController,HiveServer2 slave1 1GB 192.168.137.101 Hadoop JournalNode,DataNode,NodeManager slave2 1GB 192.168.137.102 Hadoop DataNode,NodeManager slave3 1GB 192.168.137.103 Hadoop DataNode,NodeManager host 8GB 192.168.137.1 应用服务器 集群搭建简介集群使用VirtualBox创建了8台虚拟机模拟真实环境中的分布式集群(因机器内存不够，特地为此买了内存条及SSD)，虚拟机全部使用CentOS7-x86_64系统，其中3台ZooKeeper集群，5台Hadoop集群（2台Master，3台Slave），Windows本机作为应用程序服务器用于连接此集群。 虚拟机创建此集群中机器的系统基本配置几乎是一样的，只是在后期所担任的角色不同，因此这里我创建了一台虚拟机，而后将环境配好后复制了7台，而后针对其所担任角色进行针对性修改。 首先创建了一台裸机，要解决的第一个问题是虚拟机与主机的网络通信，这里我采用VirtualBox中的Host-Only连接方式，以保证虚拟机与主机之间正常的网络通信，同时需要在主机上共享网络，以保证虚拟机同时还能访问互联网。 在主机网络设置中共享网络： 在VirtualBox中执行以下操作设置主机连接方式： 在虚拟机终端执行以下操作： # 修改虚拟机的IP、子网掩码vim /etc/sysconfig/network-scripts/ifcfg-enp0s3# 修改为以下内容TYPE=EthernetIPADDR=192.168.137.100NETMASK=255.255.255.0# 保存退出# 修改网关地址vim /etc/sysconfig/network# 修改为以下内容NETWORKING=yesGATEWAY=192.168.137.1# 保存退出# 修改主机名为master，后续过程中访问本机只需要主机名而不用敲IPhostnamectl set-hostname master# 关闭并停用防火墙，由于这里使用的是局域网，因此无需太多考虑网络安全systemctl stop firewalldsystemctl disable firewalld# 重启网络服务systemctl restart network# 尝试从虚拟机ping网关以及从主机ping虚拟机hostname或者ip，若都能ping通说明网络配置成功ping 192.168.137.1# 从虚拟机ping外网查看是否可以连接互联网，这里测试百度IP：61.135.169.105ping 61.135.169.105# 修改hosts文件，添加局域网中其他主机的主机名与ip的映射vim /etc/hosts# 修改为以下内容192.168.137.100 master192.168.137.10 master1192.168.137.101 slave1192.168.137.102 slave2192.168.137.103 slave3192.168.137.200 node0192.168.137.201 node1192.168.137.202 node20.0.0.0 localhost# 保存退出 至此，虚拟机网络配置已完成，下面安装Hadoop 2.9.2及Hive 2.3.4（下载、解压步骤省略），并执行一些准备工作： # 首先添加Hadoop和Hive相关环境变量vim /etc/profile# 添加下列内容export HADOOP_MAPRED_HOME=/usr/local/hadoopexport HADOOP_HOME=/usr/local/hadoopexport HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoopexport HIVE_HOME=/usr/local/hiveexport PATH=$HIVE_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH# 保存退出，并使环境变量生效source /etc/profile Hadoop和Hive的配置需放到各台虚拟机上分别执行，因为不同虚拟机所需要的配置不同。 虚拟机复制上述步骤已经创建好了一个虚拟机，下面需要复制出7个，并对每台机器针对性的进行一些修改。 网络配置对于每台虚拟机需要执行以下几个步骤以保证9台机器之间形成一个网络： 修改IP vim /etc/sysconfig/network-scripts/ifcfg-enp0s3 针对集群机器中定义的IP将IPADDR项修改为对应的IP 修改主机名 hostnamectl set-hostname XXX 针对集群机器中定义的主机名执行以上命令修改为特定的主机名 重启网络服务 systemctl restart network ping各个节点测试是否成功 ping masterping master1ping slave1ping slave2ping slave3ping node0ping node1ping node2ping 192.168.137.1ping 61.135.169.105 Hadoop配置修改core-site.xmlvim $HADOOP_HOME/etc/hadoop/core-site.xml 作用：Hadoop集群的核心配置文件 需要修改的机器：master、master1、slave1、slave2、slave3 内容： &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://ns&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/var/hadoop&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zookeeper地址 --&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;node0:2181,node1:2182,node2:2181&lt;/value&gt; &lt;/property&gt; &lt;!-- 允许访问此hdfs的主机和群组，此处设置为任意 --&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改hdfs-site.xmlvim $HADOOP_HOME/etc/hadoop/hdfs-site.xml 作用：hdfs集群配置文件 需要修改的机器：master、master1、slave1、slave2、slave3 内容： &lt;configuration&gt; &lt;!-- 指定dfs文件存储位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.data.dir&lt;/name&gt; &lt;value&gt;/var/hadoop-data&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定文件备份份数 --&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定机器运行情况检查时间间隔 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.heartbead.recheck-interval&lt;/name&gt; &lt;value&gt;3000000ms&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hdfs的nameservice为ns，和core-site.xml保持一致 --&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;ns&lt;/value&gt; &lt;/property&gt; &lt;!-- NS下面的NameNode --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.ns&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;!-- nn1的RPC通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ns.nn1&lt;/name&gt; &lt;value&gt;master:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- nn1的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.ns.nn1&lt;/name&gt; &lt;value&gt;master:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- nn2的RPC通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ns.nn2&lt;/name&gt; &lt;value&gt;master1:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- nn2的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.ns.nn2&lt;/name&gt; &lt;value&gt;master1:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定NameNode的元数据在JournalNode上的存放位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://master:8485;master1:8485;slave1:8485/ns&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定JournalNode在本地磁盘存放数据的位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/journaldata&lt;/value&gt; &lt;/property&gt; &lt;!-- 开启机器故障自动切换主从机器 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定failover切换的方法(java类的名称) --&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.ns&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定failover切换的方法，这里使用ssh通信方式交换 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt; sshfence shell(/bin/true) &lt;/value&gt; &lt;/property&gt; &lt;!-- ssh切换方法需要指定私钥文件位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 注意 假设当备份份数为2时，现在有三台DataNode机器，文件被分为2个block，block1位于1和2上，block2位于1和3上，这是若机器3宕机了，hdfs会在设定的dfs.namenode.heartbead.recheck-interval时间间隔内检查出机器3(在此时间间隔内可能会出现文件数量紊乱的现象)，此时block2数量变为1，hdfs会自动将1中的block2复制一份到另外一台可用机器上（此处为2）。当机器3恢复运行时，3中备份的block2会自动删除。 当使用jdbc访问hdfs时，不会使用hdfs-site.xml中的dfs.replication，而会默认使用3，可在java的configuration中配置为指定值 修改slaves文件vim $HADOOP_HOME/etc/hadoop/slaves 作用：为各个master指定为其工作的slave 需要修改的机器：master、master1 内容 slave1slave2slave3 修改yarn-site.xmlvim $HADOOP_HOME/etc/hadoop/yarn-site.xml 作用：yarn集群的核心配置文件 需要修改的机器：master、master1、slave1、slave2、slave3 内容： &lt;configuration&gt; &lt;!-- 启用yarn集群的高可用机制 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定ResourceManager集群id，可为任意字串 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;yrc&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定两台ResourceManager的名称 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定两台ResourceManager的主机名 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;master&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;master1&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定两台ResourceManager的web端口，正常情况为8088 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt; &lt;value&gt;master:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt; &lt;value&gt;master1:8088&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定管理集群的Zookeeper集群的地址及对应端口 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;node0:2181,node1:2181,node2:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.auxservices.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定jar包路径 --&gt; &lt;property&gt; &lt;name&gt;yarn.application.classpath&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/usr/local/hadoop/contrib/capacity-scheduler/*.jar&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改mapred-site.xml 作用：指定MapReduce操作的基本属性 需要修改的机器：master、master1 内容 &lt;configuration&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.application.classpath&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/usr/local/hadoop/contrib/capacity-scheduler/*.jar&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 注意： MapReduce是不一定依赖yarn的，但一般使用yarn框架来实现MapReduce 此项若是不配，一些job只会在本机跑，而不会分发给其他机器 修改hive-site.xmlvim $HIVE_HOME/conf/hive-site.xml 作用：hive的基本配置 需要修改的机器：master、master1 内容： 修改hive.server2.webui.host &lt;property&gt; &lt;name&gt;hive.server2.webui.host&lt;/name&gt; &lt;value&gt;$&#123;hostname&#125;&lt;/value&gt; &lt;description&gt;The host address the HiveServer2 WebUI will listen on&lt;/description&gt;&lt;/property&gt; 其中${hostname}需要改成对应的主机名称(master与master1)，或者都改为0.0.0.0 修改hive.server2.bind.host &lt;property&gt; &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt; &lt;value&gt;$&#123;hostname&#125;&lt;/value&gt; &lt;description&gt;Bind host on which to run the HiveServer2 Thrift service.&lt;/description&gt;&lt;/property&gt; 其中${hostname}需要改成对应的主机名称(master与master1)，或者都改为0.0.0.0 修改hive.server2.zookeeper.namespace &lt;property&gt; &lt;name&gt;hive.server2.zookeeper.namespace&lt;/name&gt; &lt;value&gt;hiveserver2&lt;/value&gt; &lt;description&gt;The parent node in ZooKeeper used by HiveServer2 when supporting dynamic service discovery.&lt;/description&gt;&lt;/property&gt; 注意两个hiveserver节点的该值应设置为一样，指定了改值后，每当一个hiveserver节点启动时，在Zookeeper集群中，目录树下的hiveserver2文件夹下就可以看到该节点注册到Zookeeper中。 修改javax.jdo.option.ConnectionURL、javax.jdo.option.ConnectionPassword和javax.jdo.option.ConnectionDriverName &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://master:3306/hive?createDatabaseIfNotExist=true&amp;amp;useSSL=false&amp;amp;allowPublicKeyRetrieval=true&lt;/value&gt; &lt;description&gt; JDBC connect string for a JDBC metastore. To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL. For example, jdbc:postgresql://myhost/db?ssl=true for postgres database. &lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;&lt;/property&gt; 这里两个节点的数据库连接字符串也应该是一样的，需要知道的是这里只有master节点安装并运行了mysql服务，存储hive元数据的均在此mysql数据库中，意即存储元数据信息是与master1无关的，实际上mysql服务器可以在网络上的任意位置(此处我一开始也误解了，以为master和master1节点都需要存储hive的元数据)。 另外，mysql的连接jar包需要下载并复制到hive的lib目录下。 至此整个Hadoop集群已经搭建完毕，却未完，需要使用Zookeeper集群来实现集群的高可用性（HA）。 Zookeeper配置对于剩下的node0、node1、node2三台机器是用于搭建Zookeeper集群的，因此需要安装并配置Zookeeper-3.4.10： vim /usr/local/zookeeper/conf/zoo.cfg# 添加一下内容server.1=192.168.137.200:2888:3888server.2=192.168.137.201:2888:3888server.3=192.168.137.202:2888:3888# 保存并关闭 ssh免密登录配置 作用：保证任何一台机器都可通过ssh免密访问其他机器，这对于使用sshfence策略的failover机制是很必要的 需要修改的机器：所有机器 内容： # 进入用户目录下的`.ssh`目录cdcd .ssh/# 创建公钥私钥对ssh-keygen -t rsa# 将公钥发送给其他所有节点，hostname需对应每一台机器更改为其主机名执行一遍ssh-copy-id $&#123;hostname&#125; 至此，Hadoop集群与Zookeeper集群已搭建完毕，接下来需要启动之。 集群启动初始化数据库在master机器上执行以下操作以初始化数据库： cd $HIVE_HOMEschematool -initSchema -dbType mysql 此命令执行完之后将会在mysql中创建hive的元数据表，以存储hive的表结构及其他属性。 启动集群在master机器上执行以下操作以启动集群： 项目功能需求项目主题超市销售管理系统 功能简介商品进货功能点说明：超市管理员查询供应商，并根据结果输入从该供应商进货的商品信息 数据需求：查询供应商，新增进货记录，新增商品 查询商品功能点说明：超市管理员通过商品名搜索库存中的所有相关商品 数据需求：查询商品 生成订单功能点说明：超市管理员查询相应商品并将其添加至订单中，添加完毕后生成订单 数据需求：查询商品，新增购买记录，新增订单 概念设计基于面向对象的思想，我们在分析数据需求的时候简单地将我们的系统分为两个模块：进货和购买，这两个模块都以商品为核心。因此在构建实体-联系模型时，我们也根据这个思想出发，将E-R图分为了两个模块。 总体E-R图 进货模块E-R图 进货模块主要包含两个实体集，公司（Corporation）和商品（Commodity）。商品中包含着超市库存商品的信息如商品名称、数量、价格，公司类包含供应商的信息如公司名称、地址、国家等。两个实体之间有着关系集供应（Supply），表示某商品从某公司进货。由于一种商品可能从多个公司进货，一个公司也可能供应多种商品，因此他们之间的关系应该是多对多。供应关系集同时还具有属性，表示进货的信息包括数量、成本、时间。 购买模块E-R图 购买模块包含的实体集有商品（Commodity）和订单（Bill）。订单表示一次完整购买的总订单（包含购买的所有商品情况），包含的信息有总价、折扣、实际价格等。商品和订单之间有关系集购买（Purchase），由于一个订单包含多个商品，一个商品也会被多次购买，因此这个关系是多对多的。同时，购买关系还具有属性，表示订单中的每一种商品购买的数量和小计价格。 逻辑设计表设计根据E-R模型的转化，我们生成了5张表，分别是：Corporation（公司），Supply（供应），Commodity（商品），Purchase（购买），Bill（订单）。表的详细设计如下： Commodity表 字段名 数据类型 长度 说明 备注 ID number 10 商品的ID PK name varchar 20 商品名称 quantity decimal 10,2 商品库存量 price decimal 10,2 商品单价 Corporation表 字段名 数据类型 长度 说明 备注 ID number 10 公司的ID PK name varchar 20 公司名称 address varchar 20 公司地址 country varchar 10 公司所在国家 business varchar 10 公司业务 Bill表 字段名 数据类型 长度 说明 备注 ID number 10 订单的ID PK totalprice decimal 10,2 订单总价 discount decimal 10,2 订单折扣 finalprice decimal 10,2 订单实际总价 realpay decimal 10,2 顾客付款 charge decimal 10,2 找钱 Supply表 字段名 数据类型 长度 说明 备注 corID number 10 供应公司的ID PK; FK，参照于Corporation的ID productID number 10 商品的ID PK；FK，参照于Commodity的ID amount decimal 10,2 商品供应数量 totalcost number 6 商品供应总成本 supplydate datetime 0 供应日期 Purchase表 字段名 数据类型 长度 说明 备注 productID number 10 购买商品的ID PK；参照于Commodity的ID billID number 10 订单ID PK；参照于Bill的ID amount decimal 10,2 购买该商品数量 sumprice decimal 10,2 购买该商品小计价格 数据库关系图根据表的设计和之间的外码约束，绘制出数据库的关系图： Hive中的实际存储经过多次尝试与实验，发现Hive中实际上是不支持表间外键联系的，因此我们在实际存储上述的表结构时将所有的外键均去除了。 这引出了数据仓库在实际应用中的一种常见处理方式：为了提高速度而产生数据冗余。Hive中的表结构的特殊性，使得\b使用人员方便针对各种功能设计所需的信息表，而非使用传统的符合3NF或者其他范式的表结构，这也是数据仓库作为大量数据集的OLAP最佳选择的原因之一(其消除了由于大量表间连接时而产生的冗余操作，这是很典型的以空间换取时间的策略)。 项目实现方法整个项目我们使用传统的JavaWeb框架。后端使用Servlet处理数据交互，使用JDBC连接Zookeeper管理的Hiveserver2集群；前端使用Bootstrap框架完成基本的项目展示功能： ​ 图1：查看库存 ​ 图2：查看历史账单 ​ 图3：创建新账单 我们深知本项目重点在分布式集群而非前端展示上，因此我们组将95%的精力放在项目的理论理解、环境搭建、性能提升以及实践使用上。 项目亮点HA的原理理解及实现HA的实现是我们组在搭建集群时遇到的最大的难题，如何协调其主次关系、如何保证网络通信、如何确定哪些进程应该运行在哪些合适的机器上，举个例子：在我们完成集群搭建到了最后启动Hive时，启动了两个Hive客户端进程和两个Hiveserver2服务端进程，而两个Hive客户端又共用了同一个MySQL服务器，导致jdbc在连接Zookeeper时会随机访问到两个Hive客户端，这导致了数据库数据时而一致、时而不一致，后来我们在Hive的配置文件将其连接Hive客户端改为同一个后将其解决了。诸如此类的小问题我们遇到了很多很多，最后都一一得到了解决。 Hive与MySQL的横向对比我们发现Hadoop集群启动后，前端与后端进行数据交互的速度很慢，于是我们使用MySQL与Hive做了简单的对比，结果很让我们困惑：MySQL的执行时间基本上是远远快于Hive的执行时间的。 我们从以下角度进行了思考与实验： 首先，考虑我们在这两种数据库中执行的操作，如果对于一开始数据从文件进入数据库中这一过程忽略的话，我们整个项目执行的都是OLAP即联机分析处理操作。Hive作为一个经典的数据仓库工具，本身应该是擅长执行OLAP操作的，因此暂且认为”操作”不是造成二者执行时间差异的原因； 其次，Hive官网有句话\b”Hive在大型数据集上会表现出优越的性能”，考虑到我们的项目数据集\b中，最多的数据集是数百万条的商品库存数据，我们猜测是数据量限制了Hive体现其\b优越性。因此我们作了如下实验：将数据量从10W变化到1000W，然后观察在等量的数据量变化上，二者执行时间的变化，得到下表： 数据库类型 10W数据 1000W数据 MySQL 9ms 271ms Hive(textfile) 1428ms 5100ms 就时间来说，很显然MySQL更胜一筹，但从增长比例来说，MySQL从9ms增长至271ms增长约为30倍，而\bHive增长约为5倍，由此我们可窥见Hive在大量数据集时性能会更加优越。 然而在这过程中，\b我们所使用的Hive所采用的为textfile存储结构，意即内容即文件，表数据完全按照表结构存储成为文本文件。\b从Hive官方文档我们得知Hive有其他更加优越的存储格式，它包含SequenceFile、RCFile、ORCFile，因此我们采取了所谓最优的ORCFile来Duplicate了用户评论表，想以此对比ORCFile之于TextFile的优点。存储中源数据文件大小为872MB，当使用textfile格式存储时，Hive会将我们导入的文件原封不动的移动到hdfs的Hive数据文件目录下，而使用ORCFile格式存储时文件大小只有260MB大小，这是其优点之一：文件压缩。至于性能，我们执行了上述同样的实验： 数据库类型 10W数据 1000W数据 MySQL 9ms 271ms Hive(textfile) 1428ms 5100ms Hive(ORC) 110ms 126ms 结果显而易见，当数据达到\b数百万量级时，Hive在存储模式较优的使用方法下已经要比MySQL要稍显胜势了。 通过以上两点我们不难总结出以下几点： 限制Hive的效率的因素： 数据量 在百万数据量以下时，Hadoop是很难发挥出其优点的 \b计算框架 Hive在我们项目中使用的是MapReduce框架来执行分布式计算，然而比\b现在已经有很多比MapReduce快得多的计算框架例如Spark等，因此若使用这些框架必定会使Hive执行效率更上一层 网络通信 由于我们的集群搭建在虚拟机中，其间数据通过真实的网络通信传输，虽然少了中间光纤传递的过程，但是在建立连接到发送数据到取消连接这一过程所耗费的时间都是很难被忽略的，因此生产环境下的Hadoop集群对网络带宽的要求是很高的 百万级数据的OLAP场景或者OLTP操作需求较多的场景下，MySQL(关系型数据库)是优选 千万乃至亿万级数据的批处理、分析场景，Hive(数据仓库/分布式数据库)在存储、读取、分析效率上都要更优","categories":[{"name":"分布式与云计算","slug":"分布式与云计算","permalink":"http://blog.guitoubing.top/categories/分布式与云计算/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://blog.guitoubing.top/tags/Hadoop/"},{"name":"云计算","slug":"云计算","permalink":"http://blog.guitoubing.top/tags/云计算/"}]},{"title":"Hadoop-Tags","slug":"Hadoop-Tags","date":"2018-12-11T16:33:02.000Z","updated":"2021-05-16T10:55:03.721Z","comments":true,"path":"2018/12/12/Hadoop-Tags/","link":"","permalink":"http://blog.guitoubing.top/2018/12/12/Hadoop-Tags/","excerpt":"Hadoop TagsHDFS","text":"Hadoop TagsHDFS NameNode内存要求较高，存储文件系统元结构（文件目录结构、分块情况、每块位置、权限等） 文件分块默认最小块128M jps命令查看NameNode/DataNode是否启动 jps在jdk8u191中好像不适用，暂未找到解决方法 ip:9870利用web界面查看Hadoop节点信息（Mac上端口号为50070） 进入用户目录下的.ssh目录，执行ssh-keygen -t rsa创建公钥私钥，使用ssh-copy-id ${hostname}将公钥传给每个节点（NameNode和DataNode都需要） 使用hadoop fs -ls /查看Hadoop上所有文件，使用hadoop fs -put ${filename} /上传文件… hdfs-site.xml中修改dfs.replication配置可修改文件备份份数（默认为3），修改dfs.namenode.heartbead.recheck-interval指定Hadoop检查机器运行情况的时间间隔（默认3000000ms） 注意： 例如当备份份数为2时，现在有三台DataNode机器，文件被分为2个block，block1位于1和2上，block2位于1和3上，这是若机器3宕机了，hdfs会在设定的dfs.namenode.heartbead.recheck-interval时间间隔内检查出机器3，此时block2数量变为1，hdfs会自动将1中的block2复制一份到另外一台可用机器上（此处为2）。当机器3恢复运行时，3中备份的block2会自动删除。 当使用java访问hdfs时，不会使用hdfs-site.xml中的dfs.replication，而会默认使用3，可在java的configuration中配置为指定值 分鱼展:分块、冗余、可扩展 Yarn ResourceManage NodeManage一般与DataNode放一起 Yarn逻辑上与HDFS完全分离，但一般绑定HDFS一起使用 yarn-site.xml的配置 注意：master与slaves都需要进行配置。 &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;master&lt;/value&gt;&lt;/property&gt; mapred-site.xml的配置 注意： 仅NameNode需要配置 MapReduce不一定需要Yarn 若不配MapReduce，其会仅在单机跑 &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; Hive 创建Hive、Hadoop环境变量，方便敲命令 修改hive-site.xml hive的conf目录下刚初始化时没有\bhive-site.xml，需要将hive-default.xml.template复制一份更名为hive-site.xml 将hive-site.xml中所有的(4个)${system:java.io.tmpdir}替换为/usr/local/hive/tmp，将所有的(4个)${system:user.name}替换为root 进入hive根目录，执行 schematool -initSchema -dbType derby 上述命令执行完毕后会在对应目录下新建metastore_db目录，用于存储数据目录 derby是hive自带的小数据库，后续需要将derby更换成mysql(TODO) \b在该目录下启动\b执行hive 注意： hive命令执行时，必须与metastore_db在同一目录下 hive启动前需要将hdfs也启动，不然会报错 hive连接mysql 关于\b虚拟机安装了mysql数据库，主机无法连接的问题如下： mysql&gt; use mysql;Database changedmysql&gt; select &apos;host&apos; from user where user=&apos;root&apos; -&gt; ;+------+| host |+------+| localhost |+------+1 row in set (0.00 sec)mysql&gt; update user set host = &apos;%&apos; where user =&apos;root&apos;;Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; flush privileges;Query OK, 0 rows affected (0.01 sec)mysql&gt; select &apos;host&apos; from user where user=&apos;root&apos;;+------+| host |+------+| % |+------+1 row in set (0.00 sec) 解决jdbc连接hive时出现Open Session Error &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt; hive表存储格式 TextFile SequenceFile RCFile ORCFile HA的实现集群环境 Hadoop 2.9.2 \bHive 2.3.4 MySQL 5.7 Zookeeper 3.4.10 JDK 8u191 集群结构图 共9台机器，其中3台ZooKeeper集群，5台Hadoop集群（2台Master，3台Slave），1台应用服务器 主机名 IP 软件 运行进程 node0 192.168.137.200 ZooKeeper QuorumPeerMain node1 192.168.137.201 ZooKeeper QuorumPeerMain node2 192.168.137.202 ZooKeeper QuorumPeerMain master 192.168.137.100 Hadoop,Hive,MySql JournalNode,NameNode,ResourceManager,DFSZKFailoverController,HiveServer2,MySql master1 192.168.137.10 Hadoop,Hive JournalNode,NameNode,ResourceManager,DFSZKFailoverController,HiveServer2 slave1 192.168.137.101 Hadoop JournalNode,DataNode,NodeManager slave2 192.168.137.102 Hadoop DataNode,NodeManager slave3 192.168.137.103 Hadoop DataNode,NodeManager host 192.168.137.1 应用服务器 集群启动步骤# 在三台zookeeper上启动zkServerzkServer.sh start# master上执行hdfs和yarn集群的启动start-dfs.shstart-yarn.sh# master1上的ResourceManager不知道为何不会自动启动，因此手动yarn-daemon.sh start resourcemanager# master和master1上\b都要启动hiveserver2hiveserver2 关于二级缓存的若干事宜在集群启动完毕之后，我在master的hive中使用create table test(...)语句创建了一个表并导入了一些数据，而后通过应用执行该表的相关查询后，发现时不时提示表test不存在，通过hive查看master和master1的表结构，发现master1中竟然没有表test，查阅资料后看到一篇博文《hive datanucleus cache 不一致问题》，是关于hive中的DataNucleus二级缓存的设置，datanucleus.cache.level2.type的设置(none,soft,weak)直接影响二级缓存是否启用，关于二级缓存的具体机制我还没弄清楚，之后我又在Hortonworks的文档中看到下面的话： Important:Hortonworks recommends that deployments disable the DataNucleus cache by setting the value of the datanucleus.cache.level2.type configuration parameter to none. The datanucleus.cache.level2 configuration parameter is ignored, and assigning a value of none to this parameter does not have the desired effect. 可能是\bHiveserver2本身在HA的层面就不建议修改库、表结构，因此若要更改表结构或者创建新表同时实现数据同步，我试了以下两种方式均可实现： 第一种关闭并重新初始化集群，启动master和所有的DataNode，在master上执行建库建表导入数据，而后启动master1将其作为standby初始化，这时master1会同步master的数据，最后启动master和master1的hiveserver2 第二种是同时在master和master1的hive客户端上执行同样的建表语句，而后在master(或者master1)上执行load命令加载数据，即可同步数据 \b第二种方法又出现了个有趣的问题：导入数据完成后，在两台机器上分别执行count操作会发现由于加载数据的机器count正常，另一台机器count结果为0，但是执行select *又确实能发现数据存在，就很玄学。 TODO List 安全与权限（kerberos） Secondary NameNode（check point NameNode） HA（High Ability）实现 Federation，超大规模数据中心","categories":[{"name":"分布式与云计算","slug":"分布式与云计算","permalink":"http://blog.guitoubing.top/categories/分布式与云计算/"}],"tags":[{"name":"Hadoop配置记录","slug":"Hadoop配置记录","permalink":"http://blog.guitoubing.top/tags/Hadoop配置记录/"}]},{"title":"内存数据库 - Best Practice","slug":"Practice","date":"2018-11-24T06:18:44.000Z","updated":"2021-05-16T11:00:19.557Z","comments":true,"path":"2018/11/24/Practice/","link":"","permalink":"http://blog.guitoubing.top/2018/11/24/Practice/","excerpt":"Oracle专家的三次授课笔记及Best Practice记录。","text":"Oracle专家的三次授课笔记及Best Practice记录。 Lesson 1创建用户并分配权限创建测试schema，命名为testcreate user test identified by test; 分配连接资源grant connect,resource to test;grant execute on dbms_lock to test;grant execute on UTL_FILE to test; 为test用户创建external_data目录以及分配权限create directory external_data as &apos;/home/oracle/data&apos;;grant read,write on directory external_data to test; 要注意oracle用户必须拥有对这里的external_data路径读写的权限。 分配表空间权限我们知道oracle中没有库的概念，取而代之的是表空间（Tablespace），在oracle初次被安装时，数据库中只有系统本身内置的表空间： SYSTEM - 存储数据字典 SYSAUX - 存储辅助应用程序的数据 TEMP - 存储数据库临时对象 USERS - 存储各个用户创建的对象 UNDOTBS - 存储不一致数据，用于事物回滚、数据库恢复、读一致性、闪回查询 …… 而当第一次通过管理员创建一个用户且未为其创建并指定表空间时，数据库系统会为其指定默认的表空间为SYSTEM，而他并没有使用SYSTEM表空间的权限，因此该用户无法完成建表等操作，可通过执行以下操作： -- DBA下执行：-- 查看数据库中的所有表空间select * from v$tablespace;-- 查看当前用户所在的表空间(注意oracle系统表中存储的用户名字段都是大写，要注意这与“oracle中不区分大小写”这一概念区分开来)select username,default_tablespace from dba_users where username=&apos;TEST&apos;;-- 为用户赋予当前表空间下的权限alter user test quota unlimited on users;-- 或者制定用户可用大小：alter user test quota 50M on users; 连接用户，建表，跑存储过程和函数连接test用户-- 在系统命令下连接cd $ORACLE_HOME/bin./sqlplus test/test -- 在进入sqlplus后的连接conn test/test 创建表create table t_mobiles(f_id number(6),f_mobile_head varchar2(50),f_province varchar2(50),f_city varchar2(50),f_platform varchar2(50),f_tel_head varchar2(50),f_zipcode varchar2(50),primary key(f_id));COMMENT ON COLUMN T_MOBILES.F_ID IS &apos;主键&apos;;COMMENT ON COLUMN T_MOBILES.F_MOBILE_HEAD IS &apos;手机号段&apos;;COMMENT ON COLUMN T_MOBILES.F_PROVINCE IS &apos;省份地区&apos;;COMMENT ON COLUMN T_MOBILES.F_CITY IS &apos;城市&apos;;COMMENT ON COLUMN T_MOBILES.F_PLATFORM IS &apos;运营商&apos;;COMMENT ON COLUMN T_MOBILES.F_TEL_HEAD IS &apos;固话区号&apos;;COMMENT ON COLUMN T_MOBILES.F_ZIPCODE IS &apos;邮政编码&apos;;COMMENT ON TABLE T_MOBILES IS &apos;号段表&apos;;create table t_records(f_id number(6),f_no varchar2(50),f_begin_time date,f_end_time date,f_duration number(10,0),f_province VARCHAR2(50), f_platform varchar2(50), f_mobile NUMBER(1) DEFAULT -1);--*注：因f_id导入时缺少数据，所有先不设置为PK.COMMENT ON COLUMN T_RECORDS.F_ID IS &apos;主键&apos;;COMMENT ON COLUMN T_RECORDS.F_NO IS &apos;通话号码&apos;;COMMENT ON COLUMN T_RECORDS.F_BEGIN_TIME IS &apos;开始时间&apos;;COMMENT ON COLUMN T_RECORDS.F_END_TIME IS &apos;结束时间&apos;;COMMENT ON COLUMN T_RECORDS.F_DURATION IS &apos;通话时长&apos;;COMMENT ON COLUMN T_RECORDS.F_PROVINCE IS &apos;省份地区&apos;;COMMENT ON COLUMN T_RECORDS.F_PLATFORM IS &apos;运营商&apos;;COMMENT ON COLUMN T_RECORDS.F_MOBILE IS &apos;手机号码标志&apos;;COMMENT ON TABLE T_RECORDS IS &apos;通话清单表&apos;; 创建ctl文件导入csv数据进入external_data路径下并创建以下文件： $ cd /home/oracle/data$ vi control_mobiles.ctl$ vi control_records.ctl control_mobiles.ctl: LOAD DATACHARACTERSET UTF8INFILE &apos;/home/oracle/data/mobiles.csv&apos;TRUNCATE INTO TABLE t_mobilesFIELDS TERMINATED BY &apos;,&apos; OPTIONALLY ENCLOSED BY &apos;&quot;&apos;TRAILING NULLCOLS( F_ID, F_MOBILE_HEAD, F_PROVINCE, F_CITY, F_PLATFORM, F_TEL_HEAD, F_ZIPCODE) control_records.ctl: LOAD DATACHARACTERSET UTF8INFILE &apos;/home/oracle/data/records.csv&apos;TRUNCATE INTO TABLE t_recordsFIELDS TERMINATED BY &apos;,&apos; OPTIONALLY ENCLOSED BY &apos;&quot;&apos;TRAILING NULLCOLS( F_NO, F_BEGIN_TIME DATE &quot;YYYY-MM-DD HH24:MI:SS&quot;, F_END_TIME DATE &quot;YYYY-MM-DD HH24:MI:SS&quot;, F_DURATION INTEGER EXTERNAL) 在该路径下执行导入操作： $ $ORACLE_HOME/bin/sqlldr userid=test/test control=control_mobiles.ctl$ $ORACLE_HOME/bin/sqlldr userid=test/test control=control_records.ctl 教程中命令为： &gt; $ sqlldr userid=test/test@orcl control=control_mobiles.ctl&gt; 即在导入时指定连接字符串（这里的orcl实际上是连接字符串的别名），其在$ORACLE_HOME/network/admin/tnsname.ora中被声明，但是默认状态下oracle中并没有配置该连接字符串，意味着我们在连接时不需要为其指定值。 既然如此，应用程序该如何在未进行上述配置的情况下连接到该字符串呢？这里就是连接字符串和服务名的区别，oracle有个默认服务名XE，实际上oracle中还有多个备用服务，当XE服务崩掉的时候会自动切换到备用服务。连接字符串如下： &gt; jdbc:oracle:thin:@localhost:1521:XE&gt; 那么没有配置连接字符串别名时，sqlplus如何通过此方法连接呢？如下直接将连接字符串全部写全： &gt; # 命令格式：sqlplus username/password@host:port/service_name&gt; $ sqlplus tanrui/tanrui@127.0.0.1:1521/xe&gt; 数据预处理-- 1、创建序列seq_records_pk用于生成通话记录表t_records的主键create sequence seq_records increment by 1 start with 1 ;-- 2、修补通话记录表t_records的主键数据，并把f_id改为主键update t_records set f_id=seq_records.nextval;alter table t_records add constraint t_records_pk primary key (f_id);-- 3、创建并初始化同步锁表，用于多线程同步控制CREATE TABLE T_LOCK(F_NAME VARCHAR2(30),F_INDEX NUMBER(20,0),PRIMARY KEY(F_NAME));COMMENT ON COLUMN T_LOCK.F_NAME IS &apos;锁名&apos;;COMMENT ON COLUMN T_LOCK.F_INDEX IS &apos;锁的当前值&apos;;COMMENT ON TABLE T_LOCK IS &apos;同步锁表&apos;;insert into T_LOCK values(&apos;_RECORD_INDEX&apos;,0);-- 4、在电话号段表中创建唯一性索引，提高号段检索速度create unique index uniq_mobile_head on t_mobiles(f_mobile_head);update t_mobiles set f_province = &apos;内蒙古&apos; where f_province = &apos;内蒙&apos;;-- 5、创建日志表，用于记录程序执行过程中的日志信息。create table t_log(f_time date, f_head varchar2(20), f_content varchar2(500));COMMENT ON COLUMN T_LOG.F_TIME IS &apos;日志时间&apos;;COMMENT ON COLUMN T_LOG.F_HEAD IS &apos;日志类型标志&apos;;COMMENT ON COLUMN T_LOG.F_CONTENT IS &apos;日志内容&apos;;COMMENT ON TABLE T_LOG IS &apos;日志表&apos;; 创建函数和存储过程声明函数和存储过程 函数is_mobile，判断通话号码是否为手机号码 --函数：判断通话号码是否为手机号码CREATE OR REPLACE FUNCTION is_mobile(phone VARCHAR2) RETURN BOOLEAN IS v_phone VARCHAR2(20); v_head VARCHAR2(2);BEGIN --检查参数func IF phone IS NULL THEN RETURN FALSE; END IF; --去除前后空格 v_phone := TRIM(phone); --去除号码前面的0 IF substr(v_phone,0,1) = &apos;0&apos; THEN v_phone := substr(v_phone, 2); END IF; --检查手机号码长度 IF substr(v_phone,0,1) &lt;&gt; &apos;1&apos; OR LENGTH(v_phone) &lt;&gt; 11 THEN RETURN FALSE; END IF; --截取号码前两位 v_head := substr(v_phone,1,2); IF v_head = &apos;13&apos; OR v_head = &apos;14&apos; OR v_head =&apos;15&apos; OR v_head =&apos;17&apos; OR v_head = &apos;18&apos; THEN RETURN TRUE; ELSE RETURN FALSE; END IF;END;/ 存储过程init，清空t_log，同时t_lock置零 --存储过程：初始化测试数据CREATE OR REPLACE PROCEDURE init IS CURSOR job_cursor IS SELECT JOB FROM user_jobs;BEGIN --重置处理位置为0 EXECUTE IMMEDIATE &apos;update t_lock set f_index=0&apos;; --清除日志表中的记录 EXECUTE IMMEDIATE &apos;truncate table t_log&apos;; --重置话单表中的记录 EXECUTE IMMEDIATE &apos;update t_records set f_province = NULL,f_platform=NULL, f_mobile=-1&apos;; COMMIT; FOR tmp_job IN job_cursor LOOP dbms_job.broken(tmp_job.JOB,TRUE,sysdate); dbms_job.REMOVE(tmp_job.JOB); END LOOP;END;/ 存储过程print，打印日志，存到T_LOG表中 --存储过程：打印日志CREATE OR REPLACE PROCEDURE print(prefix VARCHAR2, content VARCHAR2) ISBEGIN --dbms_output.put_line(to_char(&apos;yyyy-mm-dd hh24:mi:ss&apos;)||&apos;,&apos;||prefix||&apos;,&apos;||content); INSERT INTO t_log VALUES(sysdate,prefix, content); COMMIT;EXCEPTION WHEN OTHERS THEN dbms_output.put_line(&apos;Error code: &apos;||SQLCODE); dbms_output.put_line(&apos;Error mesg: &apos;||sqlerrm);END;/ 存储过程show，显示当前处理情况 --存储过程：显示当前处理情况CREATE OR REPLACE PROCEDURE show IS --待处理记录总数 v_record_count NUMBER; --当前日志表记录总数 v_log_count NUMBER; --当前数据处理位置 v_current_index NUMBER; --用户Job表游标 CURSOR job_cursor IS SELECT * FROM user_jobs;BEGIN SELECT COUNT(1) INTO v_log_count FROM t_log; SELECT f_index INTO v_current_index FROM t_lock; SELECT COUNT(1) INTO v_record_count FROM t_records; dbms_output.put_line(&apos;log count: &apos;||v_log_count); dbms_output.put_line(&apos;record count: &apos;||v_record_count); dbms_output.put_line(&apos;current index: &apos;||v_current_index); --清除用户job记录 FOR tmp_job IN job_cursor LOOP dbms_output.put_line(&apos;job:&apos;||tmp_job.JOB||&apos;,broken:&apos;||tmp_job.broken||&apos;,total_time:&apos;||tmp_job.total_time||&apos;,failures:&apos;||tmp_job.failures||&apos;,interval:&apos;||tmp_job.INTERVAL||&apos;,last_sec:&apos;||tmp_job.last_sec||&apos;,next_sec:&apos;||tmp_job.next_sec); END LOOP;END;/ 存储过程process_data，提交一个job处理数据 共享锁和排它锁: 当某事务对数据添加共享锁时，此时该事务只能读不能写，其他事务只能对该数据添加共享锁，而不能添加排它锁 当某事务对数据添加排它锁时，此时该事务既能读又能写，其他事务不能对该数据添加任何锁 autocommit需要关掉: 假设现在有三个job对T_LOCK表进行并发读写，如下： 步骤如下： 阻塞情况： --存储过程：处理数据CREATE OR REPLACE PROCEDURE process_data(process_no IN NUMBER, batch_size IN NUMBER) IS --定义常量 c_record_index CONSTANT VARCHAR2(20) :=&apos;_RECORD_INDEX&apos;; c_process_prefix CONSTANT VARCHAR2(20) := &apos;[ PROCESS ]&apos;; c_select_record_sql VARCHAR2(100) := &apos;select * from t_records where f_id &gt;= :x and f_id &lt;= :y&apos;; c_select_mobile_sql VARCHAR2(100) := &apos;select * from t_mobiles where f_mobile_head = :x&apos;; c_update_mobile_sql VARCHAR2(100) := &apos;update t_records set f_province = :x, f_platform = :y, f_mobile = 1 where f_id = :z&apos;; c_update_record_sql VARCHAR2(100) := &apos;update t_records set f_mobile = 0 where f_id = :n&apos;; v_record_count NUMBER; v_current_index NUMBER; v_begin_index NUMBER; v_end_index NUMBER; v_id NUMBER; v_phone VARCHAR2(20); v_province VARCHAR2(20); v_platform VARCHAR2(20); --定义动态游标 TYPE ty_record_cursor IS REF CURSOR; record_cursor ty_record_cursor; mobile_cursor ty_record_cursor; v_record_row t_records%rowtype; v_mobile_row t_mobiles%rowtype;BEGIN PRINT(c_process_prefix, &apos;process[&apos;||process_no||&apos;], running...&apos;); --获取待处理的记录总数 SELECT COUNT(1) INTO v_record_count FROM t_records; PRINT(c_process_prefix, &apos;process[&apos;||process_no||&apos;], records count: &apos;||v_record_count); LOOP --获取记录锁 SELECT f_index INTO v_current_index FROM t_lock WHERE f_name = c_record_index FOR UPDATE; PRINT(c_process_prefix, &apos;process[&apos;||process_no||&apos;], current index: &apos;||v_current_index); IF v_current_index = v_record_count THEN PRINT(c_process_prefix, &apos;process[&apos;||process_no||&apos;], finished.&apos;); EXIT; END IF; --记录本次处理的开始和结束记录位置 v_end_index := v_current_index + batch_size; IF v_end_index &gt; v_record_count THEN v_end_index := v_record_count; END IF; --提交事务，释放锁 UPDATE t_lock SET f_index = v_end_index WHERE f_name =c_record_index; COMMIT; --计算开始位置 v_begin_index := v_current_index +1; PRINT(c_process_prefix, &apos;process[&apos;||process_no||&apos;], begin index:&apos;||v_begin_index||&apos;, end index:&apos;||v_end_index); --test：dbms_lock.sleep(5); --查询一批记录进行逐个处理 OPEN record_cursor FOR c_select_record_sql USING v_begin_index, v_end_index; LOOP FETCH record_cursor INTO v_record_row; EXIT WHEN record_cursor%notfound; v_id := v_record_row.f_id; v_phone := v_record_row.f_no; IF is_mobile(v_phone) THEN v_phone := TRIM(v_phone); IF substr(v_phone,0,1) = &apos;0&apos; THEN v_phone := substr(v_phone, 2); END IF; --PRINT(c_process_prefix, &apos;process[&apos;||process_no||&apos;], id:&apos;||v_id||&apos;, phone:&apos;||v_phone); --更新话单记录中的省份、运营商以及手机类型标志 OPEN mobile_cursor FOR c_select_mobile_sql USING substr(v_phone,1,7); FETCH mobile_cursor INTO v_mobile_row; v_province := v_mobile_row.f_province; v_platform := v_mobile_row.f_platform; --FETCH mobile_cursor INTO v_province, v_platform; CLOSE mobile_cursor; --更新话单记录的运营商、省份地区信息 EXECUTE IMMEDIATE c_update_mobile_sql USING v_province,v_platform,v_id; ELSE --更新话单记录为非移动号码类型 EXECUTE IMMEDIATE c_update_record_sql USING v_id; END IF; --提交事务 COMMIT; END LOOP; CLOSE record_cursor; PRINT(c_process_prefix, &apos;process[&apos;||process_no||&apos;], processed index: &apos;||v_end_index); END LOOP;EXCEPTION WHEN OTHERS THEN dbms_output.put_line(&apos;Error code: &apos;||SQLCODE); dbms_output.put_line(&apos;Error mesg: &apos;||sqlerrm);END;/ 存储过程generate_csv_report，生成报表 --存储过程：生成报表CREATE OR REPLACE PROCEDURE generate_csv_report IS c_report_prefix CONSTANT VARCHAR2(20) := &apos;[ REPORT ]&apos;; v_report_1 UTL_FILE.FILE_TYPE; v_report_2 UTL_FILE.FILE_TYPE; CURSOR report_1_cursor IS SELECT f_platform,f_province,SUM(f_duration) total FROM t_records WHERE f_mobile=1 GROUP BY f_platform,f_province ORDER BY f_platform ASC,SUM(f_duration) DESC; cursor report_2_cursor is select f_province,f_platform,sum(f_duration) total from t_records where f_mobile=1 group by f_province,f_platform order by f_province asc,sum(f_duration) desc;BEGIN --生成报表1，根据运营商分类汇总各省份地区的通话量 v_report_1 := UTL_FILE.FOPEN( LOCATION =&gt; &apos;EXTERNAL_DATA&apos;, filename =&gt; &apos;report1.csv&apos;, open_mode =&gt; &apos;w&apos;, max_linesize =&gt; 32767); FOR cur_tmp IN report_1_cursor LOOP UTL_FILE.PUT_LINE(v_report_1, cur_tmp.f_platform || &apos;,&apos; || cur_tmp.f_province || &apos;,&apos; || cur_tmp.total); END LOOP; UTL_FILE.FCLOSE(v_report_1); --生成报表2，根据各省份地区汇总各运营商的通话量 v_report_2 := UTL_FILE.FOPEN( LOCATION =&gt; &apos;EXTERNAL_DATA&apos;, filename =&gt; &apos;report2.csv&apos;, open_mode =&gt; &apos;w&apos;, max_linesize =&gt; 32767); FOR cur_tmp IN report_2_cursor LOOP UTL_FILE.PUT_LINE(v_report_2, cur_tmp.f_province || &apos;,&apos; || cur_tmp.f_platform || &apos;,&apos; || cur_tmp.total); END LOOP; UTL_FILE.FCLOSE(v_report_2); PRINT(c_report_prefix, &apos;generated reports.&apos;); EXCEPTION WHEN OTHERS THEN dbms_output.put_line(&apos;Error code: &apos;||SQLCODE); dbms_output.put_line(&apos;Error mesg: &apos;||sqlerrm);END;/ 存储过程analysis，调用上述函数，完成任务逻辑，支持指定任务个数和一批数量 dbms_job: 用于管理job的package oracle限定的job_queue_processes: oracle中有一个对任务可启动进程的数量进行限制的参数： &gt; SQL&gt; show parameter job_queue_processes;&gt; NAME TYPE VALUE&gt; ----------------------------------------------------------&gt; job_queue_processeses integer 10&gt;&gt; SQL&gt; alter system set job_queue_processes=0...1000;&gt; 使用ctrl+c是无法停止job的: 可使用top命令查看当前进程详情，如果需要结束特定job可kill对应job的进程号 CREATE OR REPLACE PROCEDURE analysis (job_count IN NUMBER, batch_size IN NUMBER)IS --定义常量 c_record_index CONSTANT VARCHAR2(20) :=&apos;_RECORD_INDEX&apos;; c_analysis_prefix CONSTANT VARCHAR2(20) := &apos;[ ANALYSIS ]&apos;; --当前处理位置 v_record_index NUMBER; --待处理的记录总数 v_record_count NUMBER; --保存临时创建的job no v_tmp_jobno NUMBER; --开始结束时间 v_begin_time NUMBER; v_process_end_time NUMBER; v_analysis_end_time NUMBER; --异常变量 e_invalid_input EXCEPTION;BEGIN PRINT(c_analysis_prefix, &apos; start analysis...&apos;); --输入参数检查 IF job_count &lt; 1 OR batch_size&lt;1 THEN RAISE e_invalid_input; END IF; PRINT(c_analysis_prefix, &apos; checked input parameters.&apos;); --记录开始时间 v_begin_time := dbms_utility.get_time; --获取待处理的记录总数 SELECT COUNT(1) INTO v_record_count FROM t_records; PRINT(c_analysis_prefix, &apos; records count: &apos;||v_record_count); --开始计算重置为0 UPDATE t_lock SET f_index=0 WHERE f_name=c_record_index; COMMIT; PRINT(c_analysis_prefix, &apos; reset index to zero.&apos;); --提交多个job FOR I IN 1.. job_count LOOP dbms_job.submit(v_tmp_jobno,&apos;begin process_data(&apos;||I||&apos;,&apos;||batch_size||&apos;); end;&apos;); PRINT(c_analysis_prefix, &apos; submitted new job, no: &apos;||v_tmp_jobno); END LOOP; PRINT(c_analysis_prefix, &apos; created &apos;||job_count||&apos; jobs.&apos;); --定时检查处理进度 LOOP SELECT f_index INTO v_record_index FROM t_lock WHERE f_name = c_record_index; PRINT(c_analysis_prefix, &apos; current index: &apos;||v_record_index); IF v_record_index = v_record_count THEN PRINT(c_analysis_prefix, &apos; processed all records, exiting...&apos;); EXIT; ELSE dbms_lock.sleep(5);--暂停等待5秒 END IF; END LOOP; v_process_end_time := dbms_utility.get_time; PRINT(c_analysis_prefix, &apos;process, elapsed time: &apos;||(v_process_end_time-v_begin_time)/100||&apos; seconds.&apos;); dbms_output.put_line(&apos;process, elapsed time: &apos;||(v_process_end_time-v_begin_time)/100||&apos; seconds.&apos;); --分类汇总产生报表 generate_csv_report; --结束时间 v_analysis_end_time := dbms_utility.get_time; PRINT(c_analysis_prefix, &apos;report, elapsed time: &apos;||(v_analysis_end_time-v_process_end_time)/100||&apos; seconds.&apos;); dbms_output.put_line(&apos;report, elapsed time: &apos;||(v_analysis_end_time-v_process_end_time)/100||&apos; seconds.&apos;);--异常捕获部分EXCEPTION WHEN e_invalid_input THEN dbms_output.put_line(&apos;Invalid input values, job_count:&apos;||job_count||&apos;, batch_size:&apos;||batch_size); WHEN OTHERS THEN dbms_output.put_line(&apos;Error code: &apos;||SQLCODE); dbms_output.put_line(&apos;Error mesg: &apos;||sqlerrm);END;/ 存储过程mul_analysis，循环调用analysis，指定不同的任务个数和批数量，并将运行时间存入T_RESULT中 -- 调用多次analysis，指定不同的job数和批数create or replace procedure mul_analysis is -- 最小job数 v_begin_job_no NUMBER := 3; -- 最大job数 v_end_job_no NUMBER := 8; -- 每次增长的batch数量 v_range NUMBER := 2000; -- 最小batch数量 v_begin_range NUMBER := 1000; -- 最大batch数量 v_end_range NUMBER := 10000; -- 当前range range NUMBER; begin for I in v_begin_job_no..v_end_job_no LOOP range := v_begin_range; LOOP -- 清洗表 init(); -- 分析 analysis(I, range); range := range+v_range; -- range增长到10000则停止 if range &gt; v_end_range then exit; end if; end loop; end loop; end;/ 执行函数和存储过程 在sqlplus中执行函数和存储过程之前需先打开serveroutput，即： &gt; SQL&gt; set serveroutput on;&gt; 这是因为存储过程中用到了dbms_output.put_line，上述语句是相当于告诉pl/sql引擎将dbms_output.put_line传递到缓冲区的内容输出到主控制台上。 call init();call analysis(4,1000); 结果分析通过执行mul_analysis()对一系列job和batch组合值进行测试，结果如下： Lesson 2创建用户并分配权限创建用户create user audittest identified by audittest; 分配权限grant connect,resource to audittest;grant execute on dbms_lock to audittest;alter user audittest quota unlimited on users;conn audittest/audittest; 创建表及其他对象创建表 注意： 这里在创建表时添加了ENABLE限制条件，oracle中对表和列的约束有Enable/Disable(启用/禁用)和Validate/NoValidate(验证/不验证) 举两个例子： 需更改的错误： &gt; -- 创建表，对name字段添加唯一性约束&gt; drop table T_TEST;&gt; create table T_TEST(&gt; id int primary key ,&gt; name varchar2(10) constraint unique_name unique disable&gt; );&gt; -- 由于某些错误，添加的记录违反了唯一性约束，但添加不会报错&gt; begin&gt; insert into T_TEST values (1, &apos;tan&apos;);&gt; insert into T_TEST values (2, &apos;rui&apos;);&gt; insert into T_TEST values (3, &apos;tan&apos;);&gt; end;&gt; select * from T_TEST;&gt; -- 修改掉违反唯一性约束的值&gt; update T_TEST set name=&apos;chen&apos; where id=3;&gt; -- 使得唯一性约束生效&gt; alter table T_TEST modify constraint unique_name enable;&gt; select * from T_TEST;&gt;&gt; 需保留的错误： &gt; -- 创建表，无约束&gt; drop table T_TEST;&gt; create table T_TEST(&gt; id int primary key ,&gt; name varchar2(10)&gt; );&gt; -- 一些old的记录本身可能存在重复数据&gt; begin&gt; insert into T_TEST values (1, &apos;tan&apos;);&gt; insert into T_TEST values (2, &apos;rui&apos;);&gt; insert into T_TEST values (3, &apos;tan&apos;);&gt; end;&gt; select * from T_TEST;&gt; -- 对name列创建非唯一性索引&gt; create index i_name on T_TEST(name);&gt; -- 新要求需要对name添加唯一性约束unique_name，但保留旧值，注意这里一定要使用非唯一性索引&gt; alter table T_TEST add constraint unique_name unique(name) using index i_name ENABLE NOVALIDATE ;&gt; -- 此时无法插入name相同的数据了&gt; insert into T_TEST values (4, &apos;tan&apos;);&gt; --部门表CREATE TABLE &quot;AUDITTEST&quot;.&quot;T_DEPARTMENT&quot;( &quot;F_ID&quot; NUMBER(6,0) NOT NULL ENABLE, &quot;F_NAME&quot; VARCHAR2(50 BYTE) NOT NULL ENABLE, &quot;F_CODE&quot; VARCHAR2(50 BYTE) NOT NULL ENABLE, &quot;F_PARENT_ID&quot; NUMBER(6,0), &quot;F_MANAGER&quot; VARCHAR2(50 BYTE) NOT NULL ENABLE, &quot;F_REMARK&quot; VARCHAR2(200 BYTE), CONSTRAINT &quot;T_DEPARTMENT_PK&quot; PRIMARY KEY (&quot;F_ID&quot;)) ;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_DEPARTMENT&quot;.&quot;F_ID&quot; IS &apos;PK&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_DEPARTMENT&quot;.&quot;F_NAME&quot; IS &apos;部门名称&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_DEPARTMENT&quot;.&quot;F_CODE&quot; IS &apos;部门编号&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_DEPARTMENT&quot;.&quot;F_PARENT_ID&quot; IS &apos;上级部门ID&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_DEPARTMENT&quot;.&quot;F_MANAGER&quot; IS &apos;部门经理&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_DEPARTMENT&quot;.&quot;F_REMARK&quot; IS &apos;备注信息&apos;;COMMENT ON TABLE &quot;AUDITTEST&quot;.&quot;T_DEPARTMENT&quot; IS &apos;部门表&apos;;--用户表CREATE TABLE &quot;AUDITTEST&quot;.&quot;T_USER&quot;( &quot;F_ID&quot; NUMBER(6,0) NOT NULL ENABLE, &quot;F_DEPT_ID&quot; NUMBER(6,0) NOT NULL ENABLE, &quot;F_NAME&quot; VARCHAR2(50 BYTE) NOT NULL ENABLE, &quot;F_CODE&quot; VARCHAR2(20 BYTE), &quot;F_SEX&quot; VARCHAR2(5 BYTE) DEFAULT NULL, &quot;F_MOBILE&quot; VARCHAR2(20 BYTE), &quot;F_TELEPHONE&quot; VARCHAR2(20 BYTE), &quot;F_EMAIL&quot; VARCHAR2(50 BYTE), &quot;F_REMARK&quot; VARCHAR2(200 BYTE), CONSTRAINT &quot;T_USER_PK&quot; PRIMARY KEY (&quot;F_ID&quot;));COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_USER&quot;.&quot;F_ID&quot; IS &apos;PK&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_USER&quot;.&quot;F_DEPT_ID&quot; IS &apos;部门ID&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_USER&quot;.&quot;F_NAME&quot; IS &apos;用户名&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_USER&quot;.&quot;F_CODE&quot; IS &apos;员工编号&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_USER&quot;.&quot;F_SEX&quot; IS &apos;性别&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_USER&quot;.&quot;F_MOBILE&quot; IS &apos;手机&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_USER&quot;.&quot;F_TELEPHONE&quot; IS &apos;固话&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_USER&quot;.&quot;F_EMAIL&quot; IS &apos;邮箱&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_USER&quot;.&quot;F_REMARK&quot; IS &apos;备注信息&apos;;COMMENT ON TABLE &quot;AUDITTEST&quot;.&quot;T_USER&quot; IS &apos;用户表&apos;;--客户信息表CREATE TABLE &quot;AUDITTEST&quot;.&quot;T_CUSTOMER&quot;( &quot;F_ID&quot; NUMBER(6,0) NOT NULL ENABLE, &quot;F_CODE&quot; VARCHAR2(45 BYTE) NOT NULL ENABLE, &quot;F_FULL_NAME&quot; VARCHAR2(145 BYTE) NOT NULL ENABLE, &quot;F_LINKMAN&quot; VARCHAR2(45 BYTE) NOT NULL ENABLE, &quot;F_MOBILE&quot; VARCHAR2(11 BYTE) NOT NULL ENABLE, &quot;F_TELEPHONE&quot; VARCHAR2(20 BYTE), &quot;F_EMAIL&quot; VARCHAR2(60 BYTE), &quot;F_ADDRESS&quot; VARCHAR2(200 BYTE), &quot;F_CITY&quot; VARCHAR2(45 BYTE), &quot;F_BALANCE&quot; NUMBER(10,2) NOT NULL ENABLE, &quot;F_PARTNER&quot; VARCHAR2(45 BYTE), &quot;F_REMARK&quot; VARCHAR2(200 BYTE), &quot;F_SALESMAN&quot; VARCHAR2(45 BYTE) NOT NULL ENABLE, &quot;F_DELETED_TAG&quot; NUMBER(1,0) DEFAULT 0 NOT NULL ENABLE, &quot;F_CREATED_ID&quot; NUMBER(6,0) NOT NULL ENABLE, &quot;F_CREATED_TIME&quot; TIMESTAMP (6) NOT NULL ENABLE, &quot;F_MODIFIED_ID&quot; NUMBER(6,0) NOT NULL ENABLE, &quot;F_MODIFIED_TIME&quot; TIMESTAMP (6) NOT NULL ENABLE, &quot;F_VERSION&quot; NUMBER(6,0) DEFAULT 1 NOT NULL ENABLE, PRIMARY KEY (&quot;F_ID&quot;));COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_CUSTOMER&quot;.&quot;F_ID&quot; IS &apos;主键&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_CUSTOMER&quot;.&quot;F_CODE&quot; IS &apos;客户编码&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_CUSTOMER&quot;.&quot;F_FULL_NAME&quot; IS &apos;客户全名&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_CUSTOMER&quot;.&quot;F_LINKMAN&quot; IS &apos;联系人&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_CUSTOMER&quot;.&quot;F_MOBILE&quot; IS &apos;联系手机&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_CUSTOMER&quot;.&quot;F_TELEPHONE&quot; IS &apos;联系固话&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_CUSTOMER&quot;.&quot;F_EMAIL&quot; IS &apos;联系邮箱&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_CUSTOMER&quot;.&quot;F_ADDRESS&quot; IS &apos;地址&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_CUSTOMER&quot;.&quot;F_CITY&quot; IS &apos;城市&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_CUSTOMER&quot;.&quot;F_BALANCE&quot; IS &apos;余额&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_CUSTOMER&quot;.&quot;F_PARTNER&quot; IS &apos;所属合作伙伴&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_CUSTOMER&quot;.&quot;F_REMARK&quot; IS &apos;备注信息&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_CUSTOMER&quot;.&quot;F_SALESMAN&quot; IS &apos;业务员&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_CUSTOMER&quot;.&quot;F_DELETED_TAG&quot; IS &apos;删除标志，0：可用，1：已删除&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_CUSTOMER&quot;.&quot;F_CREATED_ID&quot; IS &apos;创建人&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_CUSTOMER&quot;.&quot;F_CREATED_TIME&quot; IS &apos;创建时间&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_CUSTOMER&quot;.&quot;F_MODIFIED_ID&quot; IS &apos;最后修改人&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_CUSTOMER&quot;.&quot;F_MODIFIED_TIME&quot; IS &apos;最后修改时间&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_CUSTOMER&quot;.&quot;F_VERSION&quot; IS &apos;版本号&apos;;COMMENT ON TABLE &quot;AUDITTEST&quot;.&quot;T_CUSTOMER&quot; IS &apos;客户信息表&apos;;--CREATE TABLE &quot;AUDITTEST&quot;.&quot;T_CUSTOMER_HISTORY&quot;( &quot;F_ID&quot; NUMBER(6,0) NOT NULL ENABLE, &quot;F_CODE&quot; VARCHAR2(45 BYTE) NOT NULL ENABLE, &quot;F_FULL_NAME&quot; VARCHAR2(145 BYTE) NOT NULL ENABLE, &quot;F_LINKMAN&quot; VARCHAR2(45 BYTE) NOT NULL ENABLE, &quot;F_MOBILE&quot; VARCHAR2(11 BYTE) NOT NULL ENABLE, &quot;F_TELEPHONE&quot; VARCHAR2(20 BYTE), &quot;F_EMAIL&quot; VARCHAR2(60 BYTE), &quot;F_ADDRESS&quot; VARCHAR2(200 BYTE), &quot;F_CITY&quot; VARCHAR2(45 BYTE), &quot;F_BALANCE&quot; NUMBER(10,2) NOT NULL ENABLE, &quot;F_PARTNER&quot; VARCHAR2(45 BYTE), &quot;F_REMARK&quot; VARCHAR2(200 BYTE), &quot;F_SALESMAN&quot; VARCHAR2(45 BYTE) NOT NULL ENABLE, &quot;F_DELETED_TAG&quot; NUMBER(1,0) DEFAULT 0 NOT NULL ENABLE, &quot;F_CREATED_ID&quot; NUMBER(6,0) NOT NULL ENABLE, &quot;F_CREATED_TIME&quot; TIMESTAMP (6) NOT NULL ENABLE, &quot;F_MODIFIED_ID&quot; NUMBER(6,0) NOT NULL ENABLE, &quot;F_MODIFIED_TIME&quot; TIMESTAMP (6) NOT NULL ENABLE, &quot;F_VERSION&quot; NUMBER(6,0) DEFAULT 1 NOT NULL ENABLE, CONSTRAINT &quot;T_CUSTOMER_HISTORY_PK&quot; PRIMARY KEY (&quot;F_ID&quot;, &quot;F_VERSION&quot;));--客户信息历史表COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_CUSTOMER_HISTORY&quot;.&quot;F_ID&quot; IS &apos;主键&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_CUSTOMER_HISTORY&quot;.&quot;F_CODE&quot; IS &apos;客户编码&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_CUSTOMER_HISTORY&quot;.&quot;F_FULL_NAME&quot; IS &apos;客户全名&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_CUSTOMER_HISTORY&quot;.&quot;F_LINKMAN&quot; IS &apos;联系人&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_CUSTOMER_HISTORY&quot;.&quot;F_MOBILE&quot; IS &apos;联系手机&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_CUSTOMER_HISTORY&quot;.&quot;F_TELEPHONE&quot; IS &apos;联系固话&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_CUSTOMER_HISTORY&quot;.&quot;F_EMAIL&quot; IS &apos;联系邮箱&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_CUSTOMER_HISTORY&quot;.&quot;F_ADDRESS&quot; IS &apos;地址&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_CUSTOMER_HISTORY&quot;.&quot;F_CITY&quot; IS &apos;城市&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_CUSTOMER_HISTORY&quot;.&quot;F_BALANCE&quot; IS &apos;余额&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_CUSTOMER_HISTORY&quot;.&quot;F_PARTNER&quot; IS &apos;所属合作伙伴&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_CUSTOMER_HISTORY&quot;.&quot;F_REMARK&quot; IS &apos;备注信息&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_CUSTOMER_HISTORY&quot;.&quot;F_SALESMAN&quot; IS &apos;业务员&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_CUSTOMER_HISTORY&quot;.&quot;F_DELETED_TAG&quot; IS &apos;删除标志，0：可用，1：已删除&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_CUSTOMER_HISTORY&quot;.&quot;F_CREATED_ID&quot; IS &apos;创建人&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_CUSTOMER_HISTORY&quot;.&quot;F_CREATED_TIME&quot; IS &apos;创建时间&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_CUSTOMER_HISTORY&quot;.&quot;F_MODIFIED_ID&quot; IS &apos;最后修改人&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_CUSTOMER_HISTORY&quot;.&quot;F_MODIFIED_TIME&quot; IS &apos;最后修改时间&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_CUSTOMER_HISTORY&quot;.&quot;F_VERSION&quot; IS &apos;版本号&apos;;COMMENT ON TABLE &quot;AUDITTEST&quot;.&quot;T_CUSTOMER_HISTORY&quot; IS &apos;客户信息历史表&apos;;--审计表CREATE TABLE &quot;AUDITTEST&quot;.&quot;T_AUDIT&quot;( &quot;F_ID&quot; NUMBER(6,0) NOT NULL ENABLE, &quot;F_TABLE_NAME&quot; VARCHAR2(50 BYTE) NOT NULL ENABLE, &quot;F_ROW_ID&quot; NUMBER(6,0) NOT NULL ENABLE, &quot;F_NEW_VERSION&quot; NUMBER(6,0) NOT NULL ENABLE, &quot;F_COLUMN_NAME&quot; VARCHAR2(50 BYTE) NOT NULL ENABLE, &quot;F_OLD_VALUE&quot; VARCHAR2(200 BYTE), &quot;F_NEW_VALUE&quot; VARCHAR2(200 BYTE), &quot;F_OPERATOR_ID&quot; NUMBER(6,0) NOT NULL ENABLE, &quot;F_OPERATION_TIME&quot; TIMESTAMP (6) NOT NULL ENABLE, CONSTRAINT &quot;T_AUDIT_PK&quot; PRIMARY KEY (&quot;F_ID&quot;));COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_AUDIT&quot;.&quot;F_ID&quot; IS &apos;主键&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_AUDIT&quot;.&quot;F_TABLE_NAME&quot; IS &apos;表名&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_AUDIT&quot;.&quot;F_ROW_ID&quot; IS &apos;业务数据主键&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_AUDIT&quot;.&quot;F_NEW_VERSION&quot; IS &apos;新的版本号&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_AUDIT&quot;.&quot;F_COLUMN_NAME&quot; IS &apos;字段&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_AUDIT&quot;.&quot;F_OLD_VALUE&quot; IS &apos;原值&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_AUDIT&quot;.&quot;F_NEW_VALUE&quot; IS &apos;新值&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_AUDIT&quot;.&quot;F_OPERATOR_ID&quot; IS &apos;操作用户&apos;;COMMENT ON COLUMN &quot;AUDITTEST&quot;.&quot;T_AUDIT&quot;.&quot;F_OPERATION_TIME&quot; IS &apos;操作时间&apos;;COMMENT ON TABLE &quot;AUDITTEST&quot;.&quot;T_AUDIT&quot; IS &apos;审计表&apos;; 创建索引、序列-- 创建复合索引CREATE INDEX &quot;AUDITTEST&quot;.&quot;IDX_TABLE_ROWID&quot; ON &quot;AUDITTEST&quot;.&quot;T_AUDIT&quot; (&quot;F_TABLE_NAME&quot;, &quot;F_ROW_ID&quot;) ;-- 创建序列CREATE SEQUENCE SEQ_AUDIT_PK INCREMENT BY 1 START WITH 1; 创建触发器--创建触发器create or replace trigger trg_customer_auditbefore update on t_customerfor each rowdeclare c_insert_sql constant varchar2(100) := &apos;insert into t_audit values(:1,:2,:3,:4,:5,:6,:7,:8,systimestamp)&apos;; c_table_name constant varchar2(20) := &apos;T_CUSTOMER&apos;; v_column_name varchar2(20);begin --记录当前数据到历史表 insert into t_customer_history values(:old.f_id,:old.f_code,:old.f_full_name,:old.f_linkman,:old.f_mobile,:old.f_telephone,:old.f_email,:old.f_address,:old.f_city,:old.f_balance,:old.f_partner,:old.f_remark,:old.f_salesman,:old.f_deleted_tag,:old.f_created_id,:old.f_created_time,:old.f_modified_id,:old.f_modified_time,:old.f_version); --递增记录的版本号 :new.f_version := :old.f_version+1; --判断字段变化 if updating(&apos;F_LINKMAN&apos;) then v_column_name := &apos;联系人&apos;; execute immediate c_insert_sql using seq_audit_pk.nextval,c_table_name,:new.f_id,:old.f_version,v_column_name,:old.f_linkman,:new.f_linkman,:new.f_modified_id; end if; if updating(&apos;F_MOBILE&apos;) then v_column_name := &apos;手机号码&apos;; execute immediate c_insert_sql using seq_audit_pk.nextval,c_table_name,:new.f_id,:old.f_version,v_column_name,:old.f_mobile,:new.f_mobile,:new.f_modified_id; end if; if updating(&apos;F_TELEPHONE&apos;) then v_column_name := &apos;固定电话&apos;; execute immediate c_insert_sql using seq_audit_pk.nextval,c_table_name,:new.f_id,:old.f_version,v_column_name,:old.f_telephone,:new.f_telephone,:new.f_modified_id; end if; if updating(&apos;F_EMAIL&apos;) then v_column_name := &apos;电子邮箱&apos;; execute immediate c_insert_sql using seq_audit_pk.nextval,c_table_name,:new.f_id,:old.f_version,v_column_name,:old.f_email,:new.f_email,:new.f_modified_id; end if; if updating(&apos;F_ADDRESS&apos;) then v_column_name := &apos;联系地址&apos;; execute immediate c_insert_sql using seq_audit_pk.nextval,c_table_name,:new.f_id,:old.f_version,v_column_name,:old.f_address,:new.f_address,:new.f_modified_id; end if; if updating(&apos;F_BALANCE&apos;) then v_column_name := &apos;余额&apos;; execute immediate c_insert_sql using seq_audit_pk.nextval,c_table_name,:new.f_id,:old.f_version,v_column_name,:old.f_balance,:new.f_balance,:new.f_modified_id; end if;end;/--创建过程--过程：重设序列值create or replace PROCEDURE reset_seq( seq_name IN VARCHAR2 )IS v_val NUMBER;BEGIN EXECUTE IMMEDIATE &apos;SELECT &apos; || seq_name || &apos;.NEXTVAL FROM dual&apos; INTO v_val; EXECUTE IMMEDIATE &apos;ALTER SEQUENCE &apos; || seq_name || &apos; INCREMENT BY -&apos; || v_val ||&apos; MINVALUE 0&apos;; EXECUTE IMMEDIATE &apos;SELECT &apos; || seq_name || &apos;.NEXTVAL FROM dual&apos; INTO v_val; EXECUTE IMMEDIATE &apos;ALTER SEQUENCE &apos; || seq_name || &apos; INCREMENT BY 1 MINVALUE 0&apos;;end;/ 创建过程过程reset_seq 将序列为输入参数seq_name的值重置 --过程：重设序列值create or replace PROCEDURE reset_seq( seq_name IN VARCHAR2 )IS v_val NUMBER;BEGIN EXECUTE IMMEDIATE &apos;SELECT &apos; || seq_name || &apos;.NEXTVAL FROM dual&apos; INTO v_val; EXECUTE IMMEDIATE &apos;ALTER SEQUENCE &apos; || seq_name || &apos; INCREMENT BY -&apos; || v_val ||&apos; MINVALUE 0&apos;; EXECUTE IMMEDIATE &apos;SELECT &apos; || seq_name || &apos;.NEXTVAL FROM dual&apos; INTO v_val; EXECUTE IMMEDIATE &apos;ALTER SEQUENCE &apos; || seq_name || &apos; INCREMENT BY 1 MINVALUE 0&apos;;end;/ 过程init truncate(截断)所有表，重设序列，并添加初始值 注意： truncate与delete的区别：delete通常用于删除表中的某些行或者所有行，且delete支持回滚，删除掉的记录的物理空间在commit前并不会被回收。truncate只能删除表的所有行且不支持回滚，删除掉的记录的物理空间也会被立刻回收。 truncate的好处在于当需要删除所有行它比delete要快，尤其在包含大量触发器、索引和其他依赖项的情况下；且它不会改变表结构、表依赖等关系，这一特性又使得它比重建表更有效，删除和重建表会使得表的依赖关系断开，因此需要重新创建依赖、创建约束、赋予权限等等操作。 但是truncate也有不好的地方，比如说当被truncate的表被依赖时，举例： &gt; -- 创建表，f_id字段引用T_TEST的主码id&gt; drop table T_TEST2;&gt; create table T_TEST2(&gt; id1 int primary key ,&gt; f_id int,&gt; constraint fk&gt; foreign key (f_id)&gt; references T_TEST(id) on delete cascade&gt; );&gt; select *&gt; from T_TEST2;&gt; insert into T_TEST2 values(1, 1);&gt; -- 可级联删除&gt; delete from T_TEST;&gt; -- 将外码置为禁用&gt; alter table T_TEST2 modify constraint fk disable validate ;&gt; -- 可截断（当不禁用外码时无法截断）&gt; truncate table T_TEST;&gt; 可见，可通过禁用约束来完成truncate，但是这些主外键约束应是创建数据库时的我们定义的强制关系，上述方法可能会使得这种强制关系紊乱，因此需做好取舍决策。 --过程：数据初始化create or replace procedure init isbegin --清除数据 execute immediate &apos;truncate table t_audit&apos;; execute immediate &apos;truncate table t_customer_history&apos;; execute immediate &apos;truncate table t_customer&apos;; execute immediate &apos;truncate table t_user&apos;; execute immediate &apos;truncate table t_department&apos;; --重调序列 reset_seq(&apos;seq_audit_pk&apos;); --插入部门 insert into t_department values(1,&apos;销售部&apos;,&apos;D01&apos;,NULL,&apos;李明&apos;,&apos;备注1...&apos;); insert into t_department values(2,&apos;销售部-北京分部&apos;,&apos;D0101&apos;,1,&apos;赵军&apos;,&apos;备注2...&apos;); insert into t_department values(3,&apos;销售部-上海分部&apos;,&apos;D0102&apos;,1,&apos;张华&apos;,&apos;备注3...&apos;); insert into t_department values(4,&apos;销售部-深圳分部&apos;,&apos;D0103&apos;,1,&apos;王兵&apos;,&apos;备注4...&apos;); --插入用户 insert into t_user values(1,1,&apos;仲芳芳&apos;,&apos;U8201&apos;,&apos;女&apos;,&apos;13771234101&apos;,&apos;02131231011&apos;,&apos;use1@samtech.com&apos;,&apos;备注1...&apos;); insert into t_user values(2,1,&apos;李明申&apos;,&apos;U8202&apos;,&apos;男&apos;,&apos;13771234102&apos;,&apos;02131231012&apos;,&apos;use2@samtech.com&apos;,&apos;备注2...&apos;); insert into t_user values(3,2,&apos;张雪&apos;, &apos;U8203&apos;,&apos;女&apos;,&apos;13771234103&apos;,&apos;02131231013&apos;,&apos;use3@samtech.com&apos;,&apos;备注3...&apos;); insert into t_user values(4,2,&apos;王刚&apos;, &apos;U8204&apos;,&apos;男&apos;,&apos;13771234104&apos;,&apos;02131231014&apos;,&apos;use4@samtech.com&apos;,&apos;备注4...&apos;); insert into t_user values(5,3,&apos;赵昌日&apos;,&apos;U8205&apos;,&apos;男&apos;,&apos;13771234105&apos;,&apos;02131231015&apos;,&apos;use5@samtech.com&apos;,&apos;备注5...&apos;); insert into t_user values(6,3,&apos;孙晓华&apos;,&apos;U8206&apos;,&apos;男&apos;,&apos;13771234106&apos;,&apos;02131231016&apos;,&apos;use6@samtech.com&apos;,&apos;备注6...&apos;); insert into t_user values(7,4,&apos;陈亚男&apos;,&apos;U8207&apos;,&apos;女&apos;,&apos;13771234107&apos;,&apos;02131231017&apos;,&apos;use7@samtech.com&apos;,&apos;备注7...&apos;); insert into t_user values(8,4,&apos;刘兵超&apos;,&apos;U8208&apos;,&apos;男&apos;,&apos;13771234108&apos;,&apos;02131231018&apos;,&apos;use8@samtech.com&apos;,&apos;备注8...&apos;); --插入客户 insert into t_customer values(1,&apos;C1808001&apos;,&apos;上海市永辉电子股份有限公司&apos;,&apos;张明升&apos;,&apos;15352678121&apos;,&apos;02135681589&apos;,&apos;ming@google.com&apos;,&apos;上海市静安区城区安泰路1108号&apos;,&apos;上海市&apos;,12082,&apos;上海中远&apos;,&apos;备注...&apos;,&apos;张娜&apos;,0,1,sysdate,1,sysdate,1); commit;end;/ 修改客户信息过程--过程：修改客户地址create or replace procedure modify_address(p_row_id in number,p_address in varchar2, p_operator in number)asbegin --校验参数省略 --... execute immediate &apos;update t_customer set f_address=:1,f_modified_id=:2, f_modified_time=sysdate where f_id=:3&apos; using p_address,p_operator,p_row_id; commit; dbms_output.put_line(&apos;Updated address successfully.&apos;); --异常捕获省略 --...end;/--过程：修改客户余额create or replace procedure modify_balance(p_row_id in number,p_balance in number, p_operator in number)asbegin --校验参数省略 --... execute immediate &apos;update t_customer set f_balance=:1,f_modified_id=:2, f_modified_time=sysdate where f_id=:3&apos; using p_balance,p_operator,p_row_id; commit; dbms_output.put_line(&apos;Updated balance successfully.&apos;); --异常捕获省略 --...end;/--过程：修改客户电子邮箱create or replace procedure modify_email(p_row_id in number,p_email in varchar2, p_operator in number)asbegin --校验参数省略 --... execute immediate &apos;update t_customer set f_email=:1,f_modified_id=:2, f_modified_time=sysdate where f_id=:3&apos; using p_email,p_operator,p_row_id; commit; dbms_output.put_line(&apos;Updated email successfully.&apos;); --异常捕获省略 --...end;/--过程：修改客户联系人create or replace procedure modify_linkman(p_row_id in number,p_linkman_name in varchar2, p_operator in number)asbegin --校验参数省略 --... execute immediate &apos;update t_customer set f_linkman=:1,f_modified_id=:2, f_modified_time=sysdate where f_id=:3&apos; using p_linkman_name,p_operator,p_row_id; commit; dbms_output.put_line(&apos;Updated linkman name successfully.&apos;); --异常捕获省略 --...end;/--过程：修改客户联系人信息create or replace procedure modify_linkman_info(p_row_id in number,p_linkman_name in varchar2,p_mobile in varchar2, p_telephone in varchar2,p_email in varchar2,p_operator in number)asbegin --校验参数省略 --... execute immediate &apos;update t_customer set f_linkman=:1, f_mobile=:2, f_telephone=:3, f_email=:4, f_modified_id=:5, f_modified_time=sysdate where f_id=:6&apos; using p_linkman_name,p_mobile,p_telephone,p_email,p_operator,p_row_id; commit; dbms_output.put_line(&apos;Updated linkman info successfully.&apos;); --异常捕获省略 --...end;/--过程：修改联系手机create or replace procedure modify_mobile(p_row_id in number,p_mobile in varchar2,p_operator in number)asbegin --校验参数省略 --... execute immediate &apos;update t_customer set f_mobile=:1,f_modified_id=:2, f_modified_time=sysdate where f_id=:3&apos; using p_mobile,p_operator,p_row_id; commit; dbms_output.put_line(&apos;Updated mobile successfully.&apos;); --异常捕获省略 --...end;/--过程：修改联系固话create or replace procedure modify_telephone(p_row_id in number,p_telephone in varchar2,p_operator in number)asbegin --校验参数省略 --... execute immediate &apos;update t_customer set f_telephone=:1,f_modified_id=:2, f_modified_time=sysdate where f_id=:3&apos; using p_telephone,p_operator,p_row_id; commit; dbms_output.put_line(&apos;Updated mobile successfully.&apos;); --异常捕获省略 --...end;/ 执行-- 初始化call init();-- 更改客户信息begin modify_linkman(1,&apos;李明顺&apos;,1); dbms_lock.sleep(1); modify_mobile(1,&apos;13771083211&apos;,2); dbms_lock.sleep(1); modify_balance(1,20020,3); dbms_lock.sleep(1); modify_address(1,&apos;中国上海市嘉定区xxx路&apos;,4); dbms_lock.sleep(1); modify_email(1,&apos;test1@163.com&apos;,5); dbms_lock.sleep(1); modify_telephone(1,&apos;02183652145&apos;,6); dbms_lock.sleep(1); modify_linkman_info(1,&apos;张雨轩&apos;,&apos;15332892301&apos;,&apos;02188881111&apos;,&apos;zhangyx@gmail.com&apos;,7);end;/-- 审计select * from T_AUDIT;-- 回滚客户信息-- 方法1：update t_customer aset(a.f_full_name,a.f_linkman,a.f_mobile,a.f_telephone,a.f_email,a.f_address,a.f_city,a.f_balance,a.f_partner,a.f_remark,a.f_salesman,a.f_deleted_tag,a.f_modified_id,a.f_modified_time)=(select b.f_full_name,b.f_linkman,b.f_mobile,b.f_telephone,b.f_email,b.f_address,b.f_city,b.f_balance,b.f_partner,b.f_remark,b.f_salesman,b.f_deleted_tag,5,sysdatefrom t_customer_history b where b.f_id=a.f_id and b.f_version=3)where a.f_id=1;-- 方法2：merge into t_customer a using t_customer_history b on (a.f_id=1 and a.f_id=b.f_id and b.f_version=3)when matched thenupdate set a.f_full_name=b.f_full_name,a.f_linkman=b.f_linkman,a.f_mobile=b.f_mobile,a.f_telephone=b.f_telephone,a.f_email=b.f_email,a.f_address=b.f_address,a.f_city=b.f_city,a.f_balance=b.f_balance,a.f_partner=b.f_partner,a.f_remark=b.f_remark,a.f_salesman=b.f_salesman,a.f_deleted_tag=b.f_deleted_tag,a.f_modified_id=5,a.f_modified_time=sysdate;-- 查看验证数据select * from t_customer where f_id=1unionselect * from t_customer_history where f_id=1 and f_version=3; Lesson 3创建用户并分配权限创建用户create user permission identified by permission; 分配权限grant connect,resource to permission;alter user permisson quota unlimited on users;conn permission/permission; 创建表及其他对象方案一 方案一在T_CUSTOMER表中存放创建人员ID，以查询该客户的直接负责人，在T_USER表中存放直属领导的ID，用于查询某领导所有下属的客户。 创建表--方案一--部门表CREATE TABLE T_DEPARTMENT( &quot;F_ID&quot; NUMBER(6,0) NOT NULL ENABLE, &quot;F_NAME&quot; VARCHAR2(50 BYTE) NOT NULL ENABLE, &quot;F_PARENT_ID&quot; NUMBER(6,0), &quot;F_MANAGER_ID&quot; NUMBER(6,0) NOT NULL ENABLE, &quot;F_REMARK&quot; VARCHAR2(200 BYTE), CONSTRAINT &quot;T_DEPARTMENT_PK&quot; PRIMARY KEY (&quot;F_ID&quot;)) ;COMMENT ON COLUMN &quot;T_DEPARTMENT&quot;.&quot;F_ID&quot; IS &apos;PK&apos;;COMMENT ON COLUMN &quot;T_DEPARTMENT&quot;.&quot;F_NAME&quot; IS &apos;部门名称&apos;;COMMENT ON COLUMN &quot;T_DEPARTMENT&quot;.&quot;F_PARENT_ID&quot; IS &apos;上级部门ID&apos;;COMMENT ON COLUMN &quot;T_DEPARTMENT&quot;.&quot;F_MANAGER_ID&quot; IS &apos;部门经理&apos;;COMMENT ON COLUMN &quot;T_DEPARTMENT&quot;.&quot;F_REMARK&quot; IS &apos;备注信息&apos;;COMMENT ON TABLE &quot;T_DEPARTMENT&quot; IS &apos;部门表&apos;;--用户表CREATE TABLE &quot;T_USER&quot;( &quot;F_ID&quot; NUMBER(6,0) NOT NULL ENABLE, &quot;F_DEPT_ID&quot; NUMBER(6,0) NOT NULL ENABLE, &quot;F_NAME&quot; VARCHAR2(50 BYTE) NOT NULL ENABLE, &quot;F_SEX&quot; VARCHAR2(5 BYTE) DEFAULT NULL, &quot;F_MOBILE&quot; VARCHAR2(20 BYTE), &quot;F_EMAIL&quot; VARCHAR2(50 BYTE), &quot;F_REMARK&quot; VARCHAR2(200 BYTE), CONSTRAINT &quot;T_USER_PK&quot; PRIMARY KEY (&quot;F_ID&quot;));COMMENT ON COLUMN &quot;T_USER&quot;.&quot;F_ID&quot; IS &apos;PK&apos;;COMMENT ON COLUMN &quot;T_USER&quot;.&quot;F_DEPT_ID&quot; IS &apos;部门ID&apos;;COMMENT ON COLUMN &quot;T_USER&quot;.&quot;F_NAME&quot; IS &apos;用户名&apos;;COMMENT ON COLUMN &quot;T_USER&quot;.&quot;F_SEX&quot; IS &apos;性别&apos;;COMMENT ON COLUMN &quot;T_USER&quot;.&quot;F_MOBILE&quot; IS &apos;手机&apos;;COMMENT ON COLUMN &quot;T_USER&quot;.&quot;F_EMAIL&quot; IS &apos;邮箱&apos;;COMMENT ON COLUMN &quot;T_USER&quot;.&quot;F_REMARK&quot; IS &apos;备注信息&apos;;COMMENT ON TABLE &quot;T_USER&quot; IS &apos;用户表&apos;;--客户信息表CREATE TABLE &quot;T_CUSTOMER&quot;( &quot;F_ID&quot; NUMBER(6,0) NOT NULL ENABLE, &quot;F_NAME&quot; VARCHAR2(145 BYTE) NOT NULL ENABLE, &quot;F_LINKMAN&quot; VARCHAR2(45 BYTE) NOT NULL ENABLE, &quot;F_MOBILE&quot; VARCHAR2(11 BYTE) NOT NULL ENABLE, &quot;F_EMAIL&quot; VARCHAR2(60 BYTE), &quot;F_ADDRESS&quot; VARCHAR2(200 BYTE), &quot;F_REMARK&quot; VARCHAR2(200 BYTE), &quot;F_CREATED_ID&quot; NUMBER(6,0) NOT NULL ENABLE, &quot;F_CREATED_TIME&quot; TIMESTAMP (6) NOT NULL ENABLE, PRIMARY KEY (&quot;F_ID&quot;));COMMENT ON COLUMN &quot;T_CUSTOMER&quot;.&quot;F_ID&quot; IS &apos;PK&apos;;COMMENT ON COLUMN &quot;T_CUSTOMER&quot;.&quot;F_NAME&quot; IS &apos;客户全名&apos;;COMMENT ON COLUMN &quot;T_CUSTOMER&quot;.&quot;F_LINKMAN&quot; IS &apos;联系人&apos;;COMMENT ON COLUMN &quot;T_CUSTOMER&quot;.&quot;F_MOBILE&quot; IS &apos;联系手机&apos;;COMMENT ON COLUMN &quot;T_CUSTOMER&quot;.&quot;F_EMAIL&quot; IS &apos;联系邮箱&apos;;COMMENT ON COLUMN &quot;T_CUSTOMER&quot;.&quot;F_ADDRESS&quot; IS &apos;地址&apos;;COMMENT ON COLUMN &quot;T_CUSTOMER&quot;.&quot;F_REMARK&quot; IS &apos;备注信息&apos;;COMMENT ON COLUMN &quot;T_CUSTOMER&quot;.&quot;F_CREATED_ID&quot; IS &apos;创建人&apos;;COMMENT ON COLUMN &quot;T_CUSTOMER&quot;.&quot;F_CREATED_TIME&quot; IS &apos;创建时间&apos;;COMMENT ON TABLE &quot;T_CUSTOMER&quot; IS &apos;客户信息表&apos;; 创建过程初始化--过程：数据初始化CREATE OR REPLACE PROCEDURE INIT ISBEGIN --清除数据 EXECUTE IMMEDIATE &apos;TRUNCATE TABLE T_CUSTOMER&apos;; EXECUTE IMMEDIATE &apos;TRUNCATE TABLE T_USER&apos;; EXECUTE IMMEDIATE &apos;TRUNCATE TABLE T_DEPARTMENT&apos;; --插入部门 INSERT INTO T_DEPARTMENT VALUES(1,&apos;公司&apos;,NULL,1,&apos;REMARK...&apos;); INSERT INTO T_DEPARTMENT VALUES(2,&apos;行政部&apos;,1,1,&apos;REMARK...&apos;); INSERT INTO T_DEPARTMENT VALUES(3,&apos;销售部&apos;,1,2,&apos;REMARK...&apos;); INSERT INTO T_DEPARTMENT VALUES(4,&apos;电销组&apos;,3,3,&apos;销售部电销组&apos;); INSERT INTO T_DEPARTMENT VALUES(5,&apos;推销组&apos;,3,6,&apos;销售部推销组&apos;); --插入用户 INSERT INTO T_USER VALUES(1,1,&apos;管理员&apos;,&apos;男&apos;,&apos;13771234101&apos;,&apos;USE1@SAMTECH.COM&apos;,&apos;系统管理员&apos;); INSERT INTO T_USER VALUES(2,3,&apos;李明申&apos;,&apos;男&apos;,&apos;13771234102&apos;,&apos;USE2@SAMTECH.COM&apos;,&apos;销售部经理&apos;); INSERT INTO T_USER VALUES(3,4,&apos;张雪&apos;, &apos;女&apos;,&apos;13771234103&apos;,&apos;USE3@SAMTECH.COM&apos;,&apos;销售部电销组主管&apos;); INSERT INTO T_USER VALUES(4,4,&apos;王刚&apos;, &apos;男&apos;,&apos;13771234104&apos;,&apos;USE4@SAMTECH.COM&apos;,&apos;销售部电销组业务员1&apos;); INSERT INTO T_USER VALUES(5,4,&apos;赵昌日&apos;,&apos;男&apos;,&apos;13771234105&apos;,&apos;USE5@SAMTECH.COM&apos;,&apos;销售部电销组业务员2&apos;); INSERT INTO T_USER VALUES(6,5,&apos;孙晓华&apos;,&apos;男&apos;,&apos;13771234106&apos;,&apos;USE6@SAMTECH.COM&apos;,&apos;销售部推销组主管&apos;); INSERT INTO T_USER VALUES(7,5,&apos;陈亚男&apos;,&apos;女&apos;,&apos;13771234107&apos;,&apos;USE7@SAMTECH.COM&apos;,&apos;销售部推销组业务员3&apos;); INSERT INTO T_USER VALUES(8,5,&apos;刘兵超&apos;,&apos;男&apos;,&apos;13771234108&apos;,&apos;USE8@SAMTECH.COM&apos;,&apos;销售部推销组业务员4&apos;); INSERT INTO T_USER VALUES(9,3,&apos;陈彬&apos;,&apos;女&apos;,&apos;13771234109&apos;,&apos;USE9@SAMTECH.COM&apos;,&apos;销售部业务员X1&apos;); INSERT INTO T_USER VALUES(10,3,&apos;王军&apos;,&apos;男&apos;,&apos;13771234110&apos;,&apos;USE10@SAMTECH.COM&apos;,&apos;销售部业务员X2&apos;); --插入客户 INSERT INTO T_CUSTOMER VALUES(1,&apos;上海市永辉电子股份有限公司&apos; ,&apos;张明升&apos;,&apos;15352678121&apos;,&apos;MING1@GOOGLE.COM&apos;,&apos;上海市静安区城区安泰路1108号&apos;,&apos;电销组主管创建&apos;,3,SYSDATE); INSERT INTO T_CUSTOMER VALUES(2,&apos;上海博运汽车销售有限公司&apos; ,&apos;朱荣荣&apos; ,&apos;13231289212&apos;,&apos;MING2@GOOGLE.COM&apos;,&apos;上海市徐汇区钦江路256号&apos;,&apos;电销组业务员1创建&apos;,4,SYSDATE); INSERT INTO T_CUSTOMER VALUES(3,&apos;安徽广宏顶管装备制造有限公司&apos; ,&apos;邱阳阳&apos; ,&apos;15328921231&apos;,&apos;MING3@GOOGLE.COM&apos;,&apos;安徽省广德县经济开发区东纬路5号&apos;,&apos;电销组业务员2创建&apos;,5,SYSDATE); INSERT INTO T_CUSTOMER VALUES(4,&apos;上海定丰霖贸易有限公司&apos; ,&apos;赵兰&apos; ,&apos;15532212322&apos;,&apos;MING4@GOOGLE.COM&apos;,&apos;上海市浦东新区东延路112号408室&apos;,&apos;推销组主管创建&apos;,6,SYSDATE); INSERT INTO T_CUSTOMER VALUES(5,&apos;上海东俊科技有限公司&apos; ,&apos;张军&apos; ,&apos;15367823660&apos;,&apos;MING5@GOOGLE.COM&apos;,&apos;上海市长宁区王安路135号&apos;,&apos;推销组业务员1创建&apos;,7,SYSDATE); INSERT INTO T_CUSTOMER VALUES(6,&apos;中科创客（深圳）智能工业设备公司&apos;,&apos;李明&apos; ,&apos;17723180234&apos;,&apos;MING6@GOOGLE.COM&apos;,&apos;深圳市龙岗区富民工业园致康路301号&apos;,&apos;推销组业务员2创建&apos;,8,SYSDATE); INSERT INTO T_CUSTOMER VALUES(7,&apos;南宁云讯科技有限公司&apos; ,&apos;王永成&apos;,&apos;13568932166&apos;,&apos;MING7@GOOGLE.COM&apos;,&apos;广东省深圳市福田区长川路102号&apos;,&apos;销售部业务员X1创建&apos;,9,SYSDATE); INSERT INTO T_CUSTOMER VALUES(8,&apos;沈阳优速家政服务有限公司&apos; ,&apos;李东升&apos;,&apos;13392312343&apos;,&apos;MING8@GOOGLE.COM&apos;,&apos;辽宁省沈阳市铁西区北二路青年易居东门32号&apos;,&apos;销售部业务员X2创建&apos;,10,SYSDATE); COMMIT;END;/ 执行初始化set serveroutput on;exec init; 查询自己的客户SELECT * FROM t_customer A WHERE A.f_created_id=&amp;id; &amp;id是所查询人员的ID 查询某领导下属人员的所有客户select * from t_user a where exists(SELECT 1 FROM t_department bWHERE a.f_dept_id=b.f_id and b.f_manager_id=&amp;idCONNECT BY b.F_PARENT_ID = PRIOR b.F_IDstart with b.F_ID = (select c.f_dept_id from t_user c where c.f_id=&amp;id)); &amp;id是该领导的ID 当部门结构或员工归属调整时，权限编码如何处理？对于方案一，只需要更改员工直属领导ID即可 方案二 方案二取消在T_USER中添加直属领导ID，改为在员工、部门、客户表中添加权限码，查看时直接搜索对应权限码即可 创建表--方案二--部门表CREATE TABLE T_DEPARTMENT_2( &quot;F_ID&quot; NUMBER(6,0) NOT NULL ENABLE, &quot;F_NAME&quot; VARCHAR2(50 BYTE) NOT NULL ENABLE, &quot;F_CODE&quot; VARCHAR2(50 BYTE) NOT NULL ENABLE, &quot;F_PARENT_ID&quot; NUMBER(6,0), &quot;F_MANAGER_ID&quot; NUMBER(6,0) NOT NULL ENABLE, &quot;F_REMARK&quot; VARCHAR2(200 BYTE), CONSTRAINT &quot;T_DEPARTMENT_PK2&quot; PRIMARY KEY (&quot;F_ID&quot;)) ;COMMENT ON COLUMN &quot;T_DEPARTMENT_2&quot;.&quot;F_ID&quot; IS &apos;PK&apos;;COMMENT ON COLUMN &quot;T_DEPARTMENT_2&quot;.&quot;F_NAME&quot; IS &apos;部门名称&apos;;COMMENT ON COLUMN &quot;T_DEPARTMENT_2&quot;.&quot;F_CODE&quot; IS &apos;部门编码&apos;;COMMENT ON COLUMN &quot;T_DEPARTMENT_2&quot;.&quot;F_PARENT_ID&quot; IS &apos;上级部门ID&apos;;COMMENT ON COLUMN &quot;T_DEPARTMENT_2&quot;.&quot;F_MANAGER_ID&quot; IS &apos;部门经理&apos;;COMMENT ON COLUMN &quot;T_DEPARTMENT_2&quot;.&quot;F_REMARK&quot; IS &apos;备注信息&apos;;COMMENT ON TABLE &quot;T_DEPARTMENT_2&quot; IS &apos;部门表2&apos;;--用户表CREATE TABLE &quot;T_USER_2&quot;( &quot;F_ID&quot; NUMBER(6,0) NOT NULL ENABLE, &quot;F_DEPT_ID&quot; NUMBER(6,0) NOT NULL ENABLE, &quot;F_NAME&quot; VARCHAR2(50 BYTE) NOT NULL ENABLE, &quot;F_CODE&quot; VARCHAR2(50 BYTE) NOT NULL ENABLE, &quot;F_SEX&quot; VARCHAR2(5 BYTE) DEFAULT NULL, &quot;F_MOBILE&quot; VARCHAR2(20 BYTE), &quot;F_EMAIL&quot; VARCHAR2(50 BYTE), &quot;F_REMARK&quot; VARCHAR2(200 BYTE), CONSTRAINT &quot;T_USER_PK2&quot; PRIMARY KEY (&quot;F_ID&quot;));COMMENT ON COLUMN &quot;T_USER_2&quot;.&quot;F_ID&quot; IS &apos;PK&apos;;COMMENT ON COLUMN &quot;T_USER_2&quot;.&quot;F_DEPT_ID&quot; IS &apos;部门ID&apos;;COMMENT ON COLUMN &quot;T_USER_2&quot;.&quot;F_NAME&quot; IS &apos;用户名&apos;;COMMENT ON COLUMN &quot;T_USER_2&quot;.&quot;F_CODE&quot; IS &apos;用户编码&apos;;COMMENT ON COLUMN &quot;T_USER_2&quot;.&quot;F_SEX&quot; IS &apos;性别&apos;;COMMENT ON COLUMN &quot;T_USER_2&quot;.&quot;F_MOBILE&quot; IS &apos;手机&apos;;COMMENT ON COLUMN &quot;T_USER_2&quot;.&quot;F_EMAIL&quot; IS &apos;邮箱&apos;;COMMENT ON COLUMN &quot;T_USER_2&quot;.&quot;F_REMARK&quot; IS &apos;备注信息&apos;;COMMENT ON TABLE &quot;T_USER_2&quot; IS &apos;用户表2&apos;;--客户信息表CREATE TABLE &quot;T_CUSTOMER_2&quot;( &quot;F_ID&quot; NUMBER(6,0) NOT NULL ENABLE, &quot;F_NAME&quot; VARCHAR2(145 BYTE) NOT NULL ENABLE, &quot;F_LINKMAN&quot; VARCHAR2(45 BYTE) NOT NULL ENABLE, &quot;F_MOBILE&quot; VARCHAR2(11 BYTE) NOT NULL ENABLE, &quot;F_EMAIL&quot; VARCHAR2(60 BYTE), &quot;F_ADDRESS&quot; VARCHAR2(200 BYTE), &quot;F_REMARK&quot; VARCHAR2(200 BYTE), &quot;F_ACCESS_CODE&quot; VARCHAR2(50 BYTE) NOT NULL ENABLE, &quot;F_CREATED_ID&quot; NUMBER(6,0) NOT NULL ENABLE, &quot;F_CREATED_TIME&quot; TIMESTAMP (6) NOT NULL ENABLE, PRIMARY KEY (&quot;F_ID&quot;));COMMENT ON COLUMN &quot;T_CUSTOMER_2&quot;.&quot;F_ID&quot; IS &apos;PK&apos;;COMMENT ON COLUMN &quot;T_CUSTOMER_2&quot;.&quot;F_NAME&quot; IS &apos;客户全名&apos;;COMMENT ON COLUMN &quot;T_CUSTOMER_2&quot;.&quot;F_LINKMAN&quot; IS &apos;联系人&apos;;COMMENT ON COLUMN &quot;T_CUSTOMER_2&quot;.&quot;F_MOBILE&quot; IS &apos;联系手机&apos;;COMMENT ON COLUMN &quot;T_CUSTOMER_2&quot;.&quot;F_EMAIL&quot; IS &apos;联系邮箱&apos;;COMMENT ON COLUMN &quot;T_CUSTOMER_2&quot;.&quot;F_ADDRESS&quot; IS &apos;地址&apos;;COMMENT ON COLUMN &quot;T_CUSTOMER_2&quot;.&quot;F_REMARK&quot; IS &apos;备注信息&apos;;COMMENT ON COLUMN &quot;T_CUSTOMER_2&quot;.&quot;F_ACCESS_CODE&quot; IS &apos;权限编码&apos;;COMMENT ON COLUMN &quot;T_CUSTOMER_2&quot;.&quot;F_CREATED_ID&quot; IS &apos;创建人&apos;;COMMENT ON COLUMN &quot;T_CUSTOMER_2&quot;.&quot;F_CREATED_TIME&quot; IS &apos;创建时间&apos;;COMMENT ON TABLE &quot;T_CUSTOMER_2&quot; IS &apos;客户信息表2&apos;;-- 创建人员更改历史表CREATE TABLE T_USER_HISTORY( ID NUMBER(6,0) PRIMARY KEY , F_ID NUMBER(6,0) , O_DEP_ID NUMBER(6,0) , O_ACCESS_CODE VARCHAR2(50 BYTE) , N_DEP_ID NUMBER(6,0), N_ACCESS_CODE VARCHAR2(50 BYTE), TIME DATE);COMMENT ON COLUMN T_USER_HISTORY.ID IS &apos;PK&apos;;COMMENT ON COLUMN T_USER_HISTORY.F_ID IS &apos;修改的人员ID&apos;;COMMENT ON COLUMN T_USER_HISTORY.O_DEP_ID IS &apos;旧的部门&apos;;COMMENT ON COLUMN T_USER_HISTORY.O_ACCESS_CODE IS &apos;旧的权限&apos;;COMMENT ON COLUMN T_USER_HISTORY.N_DEP_ID IS &apos;新的部门&apos;;COMMENT ON COLUMN T_USER_HISTORY.N_ACCESS_CODE IS &apos;新的权限&apos;;COMMENT ON TABLE T_USER_HISTORY IS &apos;用户权限更改历史&apos;; 创建过程初始化--过程：数据初始化CREATE OR REPLACE PROCEDURE INIT2 ISBEGIN --清除数据 EXECUTE IMMEDIATE &apos;TRUNCATE TABLE T_CUSTOMER_2&apos;; EXECUTE IMMEDIATE &apos;TRUNCATE TABLE T_USER_2&apos;; EXECUTE IMMEDIATE &apos;TRUNCATE TABLE T_DEPARTMENT_2&apos;; EXECUTE IMMEDIATE &apos;TRUNCATE TABLE T_USER_HISTORY&apos;; --插入部门 INSERT INTO T_DEPARTMENT_2 VALUES(1,&apos;公司&apos;,&apos;1&apos;,NULL,1,&apos;REMARK...&apos;); INSERT INTO T_DEPARTMENT_2 VALUES(2,&apos;行政部&apos;,&apos;101&apos;,1,1,&apos;REMARK...&apos;); INSERT INTO T_DEPARTMENT_2 VALUES(3,&apos;销售部&apos;,&apos;102&apos;,1,2,&apos;REMARK...&apos;); INSERT INTO T_DEPARTMENT_2 VALUES(4,&apos;电销组&apos;,&apos;10201&apos;,3,3,&apos;销售部电销组&apos;); INSERT INTO T_DEPARTMENT_2 VALUES(5,&apos;推销组&apos;,&apos;10202&apos;,3,6,&apos;销售部推销组&apos;); --插入用户 INSERT INTO T_USER_2 VALUES(1,1,&apos;管理员&apos;,&apos;1&apos;,&apos;男&apos;,&apos;13771234101&apos;,&apos;USE1@SAMTECH.COM&apos;,&apos;系统管理员&apos;); INSERT INTO T_USER_2 VALUES(2,3,&apos;李明申&apos;,&apos;102&apos;,&apos;男&apos;,&apos;13771234102&apos;,&apos;USE2@SAMTECH.COM&apos;,&apos;销售部经理&apos;); INSERT INTO T_USER_2 VALUES(3,4,&apos;张雪&apos;, &apos;10201&apos;, &apos;女&apos;,&apos;13771234103&apos;,&apos;USE3@SAMTECH.COM&apos;,&apos;销售部电销组主管&apos;); INSERT INTO T_USER_2 VALUES(4,4,&apos;王刚&apos;, &apos;1020101&apos;, &apos;男&apos;,&apos;13771234104&apos;,&apos;USE4@SAMTECH.COM&apos;,&apos;销售部电销组业务员1&apos;); INSERT INTO T_USER_2 VALUES(5,4,&apos;赵昌日&apos;,&apos;1020102&apos;,&apos;男&apos;,&apos;13771234105&apos;,&apos;USE5@SAMTECH.COM&apos;,&apos;销售部电销组业务员2&apos;); INSERT INTO T_USER_2 VALUES(6,5,&apos;孙晓华&apos;,&apos;10202&apos;,&apos;男&apos;,&apos;13771234106&apos;,&apos;USE6@SAMTECH.COM&apos;,&apos;销售部推销组主管&apos;); INSERT INTO T_USER_2 VALUES(7,5,&apos;陈亚男&apos;,&apos;1020201&apos;,&apos;女&apos;,&apos;13771234107&apos;,&apos;USE7@SAMTECH.COM&apos;,&apos;销售部推销组业务员3&apos;); INSERT INTO T_USER_2 VALUES(8,5,&apos;刘兵超&apos;,&apos;1020202&apos;,&apos;男&apos;,&apos;13771234108&apos;,&apos;USE8@SAMTECH.COM&apos;,&apos;销售部推销组业务员4&apos;); INSERT INTO T_USER_2 VALUES(9,3,&apos;陈彬&apos;, &apos;10203&apos;,&apos;女&apos;,&apos;13771234109&apos;,&apos;USE9@SAMTECH.COM&apos;,&apos;销售部业务员X1&apos;); INSERT INTO T_USER_2 VALUES(10,3,&apos;王军&apos;, &apos;10204&apos;,&apos;男&apos;,&apos;13771234110&apos;,&apos;USE10@SAMTECH.COM&apos;,&apos;销售部业务员X2&apos;); --插入客户 INSERT INTO T_CUSTOMER_2 VALUES(1,&apos;上海市永辉电子股份有限公司&apos; ,&apos;张明升&apos;,&apos;15352678121&apos;,&apos;MING1@GOOGLE.COM&apos;,&apos;上海市静安区城区安泰路1108号&apos;,&apos;电销组主管创建&apos;,&apos;10201&apos;,3,SYSDATE); INSERT INTO T_CUSTOMER_2 VALUES(2,&apos;上海博运汽车销售有限公司&apos; ,&apos;朱荣荣&apos; ,&apos;13231289212&apos;,&apos;MING2@GOOGLE.COM&apos;,&apos;上海市徐汇区钦江路256号&apos;,&apos;电销组业务员1创建&apos;,&apos;1020101&apos;,4,SYSDATE); INSERT INTO T_CUSTOMER_2 VALUES(3,&apos;安徽广宏顶管装备制造有限公司&apos; ,&apos;邱阳阳&apos; ,&apos;15328921231&apos;,&apos;MING3@GOOGLE.COM&apos;,&apos;安徽省广德县经济开发区东纬路5号&apos;,&apos;电销组业务员2创建&apos;,&apos;1020102&apos;,5,SYSDATE); INSERT INTO T_CUSTOMER_2 VALUES(4,&apos;上海定丰霖贸易有限公司&apos; ,&apos;赵兰&apos; ,&apos;15532212322&apos;,&apos;MING4@GOOGLE.COM&apos;,&apos;上海市浦东新区东延路112号408室&apos;,&apos;推销组主管创建&apos;,&apos;10202&apos;,6,SYSDATE); INSERT INTO T_CUSTOMER_2 VALUES(5,&apos;上海东俊科技有限公司&apos; ,&apos;张军&apos; ,&apos;15367823660&apos;,&apos;MING5@GOOGLE.COM&apos;,&apos;上海市长宁区王安路135号&apos;,&apos;推销组业务员1创建&apos;,&apos;1020201&apos;,7,SYSDATE); INSERT INTO T_CUSTOMER_2 VALUES(6,&apos;中科创客（深圳）智能工业设备公司&apos;,&apos;李明&apos; ,&apos;17723180234&apos;,&apos;MING6@GOOGLE.COM&apos;,&apos;深圳市龙岗区富民工业园致康路301号&apos;,&apos;推销组业务员2创建&apos;,&apos;1020202&apos;,8,SYSDATE); INSERT INTO T_CUSTOMER_2 VALUES(7,&apos;南宁云讯科技有限公司&apos; ,&apos;王永成&apos;,&apos;13568932166&apos;,&apos;MING7@GOOGLE.COM&apos;,&apos;广东省深圳市福田区长川路102号&apos;,&apos;销售部业务员X1创建&apos;,&apos;10203&apos;,9,SYSDATE); INSERT INTO T_CUSTOMER_2 VALUES(8,&apos;沈阳优速家政服务有限公司&apos; ,&apos;李东升&apos;,&apos;13392312343&apos;,&apos;MING8@GOOGLE.COM&apos;,&apos;辽宁省沈阳市铁西区北二路青年易居东门32号&apos;,&apos;销售部业务员X2创建&apos;,&apos;10204&apos;,10,SYSDATE); COMMIT;END;/ 将某员工调换到某部门 -- 更改用户到特定部门CREATE OR REPLACE PROCEDURE CHANGE_TO_DEPARTMENT(C_F_ID IN T_USER_2.F_ID%TYPE, N_DEP_ID IN T_DEPARTMENT_2.F_ID%TYPE) IS -- 旧部门 O_DEP_ID T_DEPARTMENT_2.F_ID%TYPE; -- 旧权限 O_ACCESS_CODE T_USER_2.F_CODE%TYPE; -- 部门权限前缀 DEP_ACCESS_CODE_PREFIX T_DEPARTMENT_2.F_CODE%TYPE; -- 部门当前人数 DEP_USER_COUNT T_USER_2.F_CODE%TYPE; -- 本部门下的部门数 DEP_DEP_COUNT T_DEPARTMENT_2.F_CODE%TYPE; -- 新权限 N_ACCESS_CODE T_USER_2.F_CODE%TYPE; -- 更新该员工权限 C_UPDATE_USER VARCHAR2(100) := &apos;UPDATE T_USER_2 SET F_CODE = :1, F_DEPT_ID = :2 WHERE F_ID = :3&apos;; -- 更新所有该员工的客户的ACCESS权限 C_UPDATE_CUSTOMER VARCHAR2(100) := &apos;UPDATE T_CUSTOMER_2 SET F_ACCESS_CODE = :1 WHERE F_CREATED_ID = :2&apos;; -- 插入一条修改记录 C_INSERT_HISTORY VARCHAR2(100) := &apos;INSERT INTO T_USER_HISTORY VALUES (:1, :2, :3, :4, :5, :6, :7)&apos;; BEGIN -- 旧部门 SELECT F_DEPT_ID INTO O_DEP_ID FROM T_USER_2 WHERE T_USER_2.F_ID=C_F_ID; -- 旧权限 SELECT F_CODE INTO O_ACCESS_CODE FROM T_USER_2 WHERE T_USER_2.F_ID=C_F_ID; -- 新部门权限作为前缀 SELECT F_CODE INTO DEP_ACCESS_CODE_PREFIX FROM T_DEPARTMENT_2 WHERE T_DEPARTMENT_2.F_ID = N_DEP_ID; -- 计算该部门人员数量 SELECT MAX(T_USER_2.F_CODE) INTO DEP_USER_COUNT FROM T_USER_2 WHERE T_USER_2.F_DEPT_ID = N_DEP_ID; -- 计算子部门数量 SELECT MAX(T_DEPARTMENT_2.F_CODE) INTO DEP_DEP_COUNT FROM T_DEPARTMENT_2 WHERE SUBSTR(T_DEPARTMENT_2.F_CODE, 0, LENGTH(DEP_ACCESS_CODE_PREFIX))=DEP_ACCESS_CODE_PREFIX AND LENGTH(T_DEPARTMENT_2.F_CODE)=LENGTH(DEP_ACCESS_CODE_PREFIX)+2; -- 若新部门与旧部门相同，无需更改 IF N_DEP_ID=O_DEP_ID THEN RETURN; END IF; -- 新权限CODE IF DEP_DEP_COUNT &gt; DEP_USER_COUNT THEN N_ACCESS_CODE := TO_CHAR(TO_NUMBER(DEP_DEP_COUNT) + 1); ELSE N_ACCESS_CODE := TO_CHAR(TO_NUMBER(DEP_USER_COUNT) + 1); end if; -- 输出相关变量 dbms_output.put_line(&apos;DEP_USER_COUNT : &apos; || DEP_USER_COUNT); dbms_output.put_line(&apos;DEP_DEP_COUNT : &apos; || DEP_DEP_COUNT); -- 输出相关变量 dbms_output.put_line(&apos;DEP_ACCESS_CODE_PREFIX : &apos; || DEP_ACCESS_CODE_PREFIX); dbms_output.put_line(&apos;C_F_ID : &apos; || C_F_ID); dbms_output.put_line(&apos;O_ACCESS_CODE : &apos; || O_ACCESS_CODE); dbms_output.put_line(&apos;N_DEP_ID : &apos; || N_DEP_ID); dbms_output.put_line(&apos;N_ACCESS_CODE : &apos; || N_ACCESS_CODE); -- 更新该员工权限 EXECUTE IMMEDIATE C_UPDATE_USER USING N_ACCESS_CODE, N_DEP_ID, C_F_ID; -- 更新所有该员工的客户的ACCESS权限 EXECUTE IMMEDIATE C_UPDATE_CUSTOMER USING N_ACCESS_CODE, C_F_ID; -- 插入一条修改记录 EXECUTE IMMEDIATE C_INSERT_HISTORY USING USER_HISTORY.NEXTVAL, C_F_ID, O_DEP_ID, O_ACCESS_CODE, N_DEP_ID, N_ACCESS_CODE, SYSDATE; -- 提交 COMMIT; END; / 创建序列-- 员工部门历史记录主码序列CREATE SEQUENCE USER_HISTORY INCREMENT BY 1 START WITH 1; 执行select * from t_customer_2 where f_access_code like &apos;xxx%&apos;; xxx%指匹配所有以xxx开头的权限码 当部门结构或员工归属调整时，权限编码如何处理？方案二中，调用新增的过程CHANGE_TO_DEPARTMENT即可级联更改权限码。","categories":[{"name":"数据库","slug":"数据库","permalink":"http://blog.guitoubing.top/categories/数据库/"}],"tags":[{"name":"TimesTen","slug":"TimesTen","permalink":"http://blog.guitoubing.top/tags/TimesTen/"},{"name":"内存数据库","slug":"内存数据库","permalink":"http://blog.guitoubing.top/tags/内存数据库/"}]},{"title":"感谢Docker,让我远离环境配置","slug":"使用Docker安装Oracle-12c","date":"2018-11-12T15:26:45.000Z","updated":"2021-05-16T10:42:54.170Z","comments":true,"path":"2018/11/12/使用Docker安装Oracle-12c/","link":"","permalink":"http://blog.guitoubing.top/2018/11/12/使用Docker安装Oracle-12c/","excerpt":"Why Docker自开始用Oracle以来，环境配置一直是让我掉头发的事。而如今也只是在Windows上的安装界面点点点成功安装了Oracle，Linux上就从来没成功过，Mac的话Oracle 11g后好像就没Mac版的了，就很头疼。 这学期上了门内存数据库，老师给了个镜像，RedHat+Oracle+TimesTen究极体镜像，扔到VirtualBox上打开直接登录用户名密码，无需安装组件，无需配置环境，即开即用。自由的气息。 偶然间在网上看到了有关于Docker安装oracle的说法，于是便尝试了一下。真的，简洁，优雅，自由，甚至比虚拟机好用多了。","text":"Why Docker自开始用Oracle以来，环境配置一直是让我掉头发的事。而如今也只是在Windows上的安装界面点点点成功安装了Oracle，Linux上就从来没成功过，Mac的话Oracle 11g后好像就没Mac版的了，就很头疼。 这学期上了门内存数据库，老师给了个镜像，RedHat+Oracle+TimesTen究极体镜像，扔到VirtualBox上打开直接登录用户名密码，无需安装组件，无需配置环境，即开即用。自由的气息。 偶然间在网上看到了有关于Docker安装oracle的说法，于是便尝试了一下。真的，简洁，优雅，自由，甚至比虚拟机好用多了。 正题Docker安装并启动Oracle 12c安装# 在docker中寻找oracle镜像，可看到一条sath89/oracle-12c的镜像，便是我们需要安装的docker search oracledocker pull sath89/oracle-12c# 查看已安装的镜像docker images 由于docker使用的是国外源，在拉取时的速度可能很慢，可参见博客切换国内源以加快拉取速度：Docker切换国内镜像下载源 初始化# 使用log记录oracle启动的容器号log=$(sudo docker run -d -p 8080:8080 -p 1521:1521 -v /Users/tanrui/Oracle/oradata:/u01/app/oracle sath89/oracle-12c)# 显示当前容器初始化进程docker logs -f $log# 显示docker中当前运行的容器(可查看到容器ID)sudo docker ps 正常情况下，第一次创建容器应称之为初始化，而以后创建的容器都应基于上次的历史数据，称作容器的数据持久化，在上述命令中-v后的:之前是当前系统想要存储持久性数据的路径，用户想要共享到容器中的文件也可放入其中，:后面是在容器中想要访问当前系统的共享文件的路径。 因此在初始化完成后，若仍然使用上述命令，会提示数据库未初始化，从而会重新创建持久性数据文件；因此以后的容器创建应该使用以下命令^1^： sudo docker run -it -p 8080:8080 -p 1521:1521 -v /Users/tanrui/Oracle/oradata:/u01/app/oracle sath89/oracle-12c 至于上述的重复初始化是会造成文件覆盖还是文件并存我没有尝试过，猜测应该会是覆盖。 同时，重复执行命令^1^时，还会产生端口冲突。因此如果想创建两个Oracle容器应该执行初始化命令，执行时将持久化数据路径更改到其他地方且需将端口号修改掉。 进入容器# 进入特定的容器，$&#123;ContainerID&#125;为上述查看到的容器ID# env LANG=C.UTF-8 表示当前容器使用支持中文的UTF-8格式(默认为POSIX，不支持中文)sudo docker exec -it $&#123;ContainerID&#125; env LANG=C.UTF-8 /bin/bash 连接oracle数据库root@1386ef844664:/# su oracleoracle@1386ef844664:/$ cd $ORACLE_HOMEoracle@1386ef844664:/u01/app/oracle/product/12.1.0/xe$ bin/sqlplus / as sysdba Oracle数据库设置字符集## 查看数据库编码，结果最下面一行则是目前编码SQL&gt; select * from nls_database_parameters where parameter ='NLS_CHARACTERSET'; ## 关闭数据库SQL&gt; shutdown immediate; ## 启动到 mount状态，oracle分为4个状态，详情请百度SQL&gt; startup mount; ## 设置session ，下同SQL&gt; ALTER SYSTEM ENABLE RESTRICTED SESSION; SQL&gt; ALTER SYSTEM SET JOB_QUEUE_PROCESSES=0;SQL&gt; ALTER SYSTEM SET AQ_TM_PROCESSES=0;## 打开oracle到 open状态SQL&gt; alter database open; ## 修改编码为 ZHS16GBKSQL&gt; ALTER DATABASE character set INTERNAL_USE ZHS16GBK; ## 重启oracle ，先关闭，再启动SQL&gt; shutdown immediate; SQL&gt; startup; 升华Docker真的好用！（俗","categories":[{"name":"数据库","slug":"数据库","permalink":"http://blog.guitoubing.top/categories/数据库/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://blog.guitoubing.top/tags/Docker/"},{"name":"Oracle","slug":"Oracle","permalink":"http://blog.guitoubing.top/tags/Oracle/"}]},{"title":"记一次Win10+Fedora双系统的小折腾","slug":"记一次Win10-Fedora双系统的小折腾","date":"2018-11-06T13:21:10.000Z","updated":"2021-05-16T10:40:31.564Z","comments":true,"path":"2018/11/06/记一次Win10-Fedora双系统的小折腾/","link":"","permalink":"http://blog.guitoubing.top/2018/11/06/记一次Win10-Fedora双系统的小折腾/","excerpt":"问题描述因课程需要，我在Win10上安装了Fedora双系统，结果出现了奇怪的问题，现Fedora系统可正常进入，Win10也有引导项，但无法进入Win10系统，报错信息见图。我在Google上搜了类似的问题，大多是诸如以下的原因：","text":"问题描述因课程需要，我在Win10上安装了Fedora双系统，结果出现了奇怪的问题，现Fedora系统可正常进入，Win10也有引导项，但无法进入Win10系统，报错信息见图。我在Google上搜了类似的问题，大多是诸如以下的原因： 主板供电不足（我使用的是台式机应该不会有这个问题） BIOS中系统时间不正确（我也未曾修改过该时间） 这些原因可能会造成与我类似的状况，但很显然这些都不是此处的问题所在 问题出现的环境背景及自己尝试过哪些方法系统相关信息：主系统Windows10专业版（安装在100G的SSD中），Fedora29（安装在由1T的HHD分出的50G硬盘中） 尝试过得方法： 曾使用PE系统中的引导修复工具修复Win10引导，无果 在Fedora中安装了grub工具尝试修复Win10引导，grub是用来配置启动时引导的系统，而我这里启动后切换到grub界面是有Win10引导的，因此问题应该不是出在这儿，而是出在Win10的引导文件\\Windows\\System32\\winload.efi上，感觉此方法应该是行不通的（到此处我排除了grub引导出错的可能性） 至此，我想既然问题出在引导文件上，我从我室友电脑上拷贝了一份该文件替换了我的引导文件，然后再使用PE中的引导修复工具修复了一遍，仍然无果 问题截图 如上所示，错误信息提示文件\\Windows\\System32\\winload.efi出错，导致我一直陷入找winload.efi文件错误的怪圈。 问题解决方法鼓捣大半天，我仍然无法解决此问题，便在SegmentFault上提问，希望藉此找到解答。在此要非常感谢解决了我的问题的答主冯恒智，一言点睛。 具体解决方法如下（划重点）：在PE中使用bootice的bcd编辑功能，打开了Win10所在磁盘中的BCD文件（C:\\EFI\\Microsoft\\Boot\\BCD），发现其中的【启动设备】项下的启动磁盘和启动分区项被置空了，我将其填写完毕后（如下图所示）发现Win10就可以正常启动了，我想这应该是我在安装Fedora时的一些不当操作使得BCD文件被修改的缘故而让Win10无法正常启动（Bootice使用方法可参见此博客）。 疑问：我在安装Fedora时应该说，和Win10所在盘是完全分隔开来的，为何Fedora安装好后会影响到Win10的Boot文件呢？更疑惑的是它只影响了配置中的启动磁盘和启动分区两项，而其他都未曾影响？待解…… 就很玄学（挠头11月7日更新SegmentFault上的答主冯恒智又回复了我的问题，如下： 并不是因为你编辑过bcd文件而导致启动磁盘和启动分区项被置空了，而是在win10安完后编辑过磁盘（比如分区啊，改盘符啊，调整容量什么的）导致找不到启动磁盘和启动分区，重新指定一下就行了","categories":[{"name":"小折腾","slug":"小折腾","permalink":"http://blog.guitoubing.top/categories/小折腾/"}],"tags":[{"name":"Win10","slug":"Win10","permalink":"http://blog.guitoubing.top/tags/Win10/"},{"name":"Linux","slug":"Linux","permalink":"http://blog.guitoubing.top/tags/Linux/"},{"name":"双系统","slug":"双系统","permalink":"http://blog.guitoubing.top/tags/双系统/"},{"name":"引导修复","slug":"引导修复","permalink":"http://blog.guitoubing.top/tags/引导修复/"}]},{"title":"Java - JavaFX学习小记","slug":"JavaFX-学习小记","date":"2018-10-27T16:08:25.000Z","updated":"2021-05-16T10:55:33.577Z","comments":true,"path":"2018/10/28/JavaFX-学习小记/","link":"","permalink":"http://blog.guitoubing.top/2018/10/28/JavaFX-学习小记/","excerpt":"JavaFX小记简介 JavaFX JavaFX是由甲骨文(Oracle)公司推出的一系列的产品和技术，主要应用于创建Rich Internet application(RIAs)，它是一个跨平台的桌面应用程序开发框架。","text":"JavaFX小记简介 JavaFX JavaFX是由甲骨文(Oracle)公司推出的一系列的产品和技术，主要应用于创建Rich Internet application(RIAs)，它是一个跨平台的桌面应用程序开发框架。 典型的MVC架构 定义Model，使用javafx.beans封装类型定义属性类型 使用fxml文件创建View，利用SceneBuilder工具进行布局 创建Controller实现动作操作以及Model和View的联系 View 创建FXML文件，利用SceneBuilder工具进行布局 Model 定义Model中的Person类，使用Property和Bind java.beans包中的对象类型不是标准的Java原语，而是新的封装起来的类，它封装了Java原语并添加了一些额外的功能，Property和Bind方便我们实现以下功能：当某个属性如First Name被改变时，会自动收到通知而修改视图，从而保证视图与数据的同步。当然仅仅声明这种类型是不够的，声明只是为后续操作提供类型前提，还需要进一步操作，可参考JavaFX文档。 Person.java package com.tanrui.model;import java.time.LocalDate;import javafx.beans.property.IntegerProperty;import javafx.beans.property.ObjectProperty;import javafx.beans.property.SimpleIntegerProperty;import javafx.beans.property.SimpleObjectProperty;import javafx.beans.property.SimpleStringProperty;import javafx.beans.property.StringProperty;/** * Model class for a Person. */public class Person &#123; private final StringProperty firstName; private final StringProperty lastName; private final StringProperty street; private final IntegerProperty postalCode; private final StringProperty city; private final ObjectProperty&lt;LocalDate&gt; birthday; /** * Default constructor. */ public Person() &#123; this(null, null); &#125; /** * Constructor with some initial data. * * @param firstName * @param lastName */ public Person(String firstName, String lastName) &#123; this.firstName = new SimpleStringProperty(firstName); this.lastName = new SimpleStringProperty(lastName); // Some initial dummy data, just for convenient testing. this.street = new SimpleStringProperty(\"some street\"); this.postalCode = new SimpleIntegerProperty(1234); this.city = new SimpleStringProperty(\"some city\"); this.birthday = new SimpleObjectProperty&lt;LocalDate&gt;(LocalDate.of(1999, 2, 21)); &#125; public String getFirstName() &#123; return firstName.get(); &#125; public void setFirstName(String firstName) &#123; this.firstName.set(firstName); &#125; public StringProperty firstNameProperty() &#123; return firstName; &#125; public String getLastName() &#123; return lastName.get(); &#125; public void setLastName(String lastName) &#123; this.lastName.set(lastName); &#125; public StringProperty lastNameProperty() &#123; return lastName; &#125; public String getStreet() &#123; return street.get(); &#125; public void setStreet(String street) &#123; this.street.set(street); &#125; public StringProperty streetProperty() &#123; return street; &#125; public int getPostalCode() &#123; return postalCode.get(); &#125; public void setPostalCode(int postalCode) &#123; this.postalCode.set(postalCode); &#125; public IntegerProperty postalCodeProperty() &#123; return postalCode; &#125; public String getCity() &#123; return city.get(); &#125; public void setCity(String city) &#123; this.city.set(city); &#125; public StringProperty cityProperty() &#123; return city; &#125; public LocalDate getBirthday() &#123; return birthday.get(); &#125; public void setBirthday(LocalDate birthday) &#123; this.birthday.set(birthday); &#125; public ObjectProperty&lt;LocalDate&gt; birthdayProperty() &#123; return birthday; &#125;&#125; 使用ObservableList管理Person 前一点所述的后续操作便是此处了，JavaFX为了实现上述目的即保持视图和数据的同步，引入了一些新的集合类，这里我们用到的是ObservableList，ObservableList继承了List类、实现了Observable接口，其实现视图和数据同步的方法是在声明ObservableList时为方法传递一个监听器，此监听器需要会通过监听personData的变化同步改变视图中对应的值，可参考ObservableList文档 Main.java: public class Main extends Application &#123; /*......Other variables......*/ /** * * The data of a observable list of Persons */ private ObservableList&lt;Person&gt; personData = FXCollections.observableArrayList(); public ObservableList&lt;Person&gt; getPersonData() &#123; return personData; &#125; public Main()&#123; personData.add(new Person(\"Tan\", \"Rui\")); personData.add(new Person(\"Chen\", \"Chao\")); personData.add(new Person(\"Liang\", \"Chengwei\")); personData.add(new Person(\"Xiao\", \"Xin\")); personData.add(new Person(\"Li\", \"Yang\")); personData.add(new Person(\"Chen\", \"Runqian\")); personData.add(new Person(\"Liang\", \"Yongchao\")); personData.add(new Person(\"Luo\", \"Jihao\")); personData.add(new Person(\"Chen\", \"Zhi\")); personData.add(new Person(\"Fan\", \"Fan\")); &#125; /* ......Other function..... */&#125; ControllerPersonOverviewController.javapackage com.tanrui.view;import javafx.fxml.FXML;import javafx.scene.control.Label;import javafx.scene.control.TableColumn;import javafx.scene.control.TableView;import com.tanrui.Main;import com.tanrui.model.Person;public class PersonOverviewController &#123; @FXML private TableView&lt;Person&gt; personTable; @FXML private TableColumn&lt;Person, String&gt; firstNameColumn; @FXML private TableColumn&lt;Person, String&gt; lastNameColumn; @FXML private Label firstNameLabel; @FXML private Label lastNameLabel; @FXML private Label streetLabel; @FXML private Label postalCodeLabel; @FXML private Label cityLabel; @FXML private Label birthdayLabel; // Reference to the main application. private Main main; /** * The constructor. * The constructor is called before the initialize() method. */ public PersonOverviewController() &#123; &#125; /** * Initializes the controller class. This method is automatically called * after the fxml file has been loaded. */ @FXML private void initialize() &#123; // Initialize the person table with the two columns. firstNameColumn.setCellValueFactory(cellData -&gt; cellData.getValue().firstNameProperty()); lastNameColumn.setCellValueFactory(cellData -&gt; cellData.getValue().lastNameProperty()); &#125; /** * Is called by the main application to give a reference back to itself. * * @param main */ public void setMain(Main main) &#123; this.main = main; // Add observable list data to the table personTable.setItems(main.getPersonData()); &#125;&#125; @FXML注解（Annotation） 使用@FXML注解可以将操作的属性、方法绑定到FXML文件的界面元素，实际上，在属性、方法是非私有的情况下可以不使用@FXML注解，但是比起非私有声明，让他们保持私有并用注解标记的方式会更好！ initialize()方法 initialize()字面意思可知其是用于初始化对应FXML文件中的属性，此方法会在加载FXML文件时被自动执行，此时，所有的FXML属性都应已被初始化 setCellValueFactory(...)方法 我们对表格列上使用setCellValueFactory(...)方法来确定为特定列使用前面Person的某个属性。-&gt;表示使用的是Lambdas特性；另外一种方法是使用PropertyValueFactory(待研究…)。 这里我们之所以可以使用cellData -&gt; cellData.getValue().firstNameProperty()，便是因为之前我们将Person的属性都定义为javafx.beans中的封装属性，firstNameProperty()等方法都会在声明成Beans封装类型时被创建，其遵循了固定的命名规则，这使得我们使用起来特别方便 连接Main和PersonOverviewController showPersonOverview() 方法 Main.java /** * Shows the person overview inside the root layout. */public void showPersonOverview() &#123; try &#123; // Load person overview. FXMLLoader loader = new FXMLLoader(); loader.setLocation(Main.class.getResource(\"view/PersonOverview.fxml\")); AnchorPane personOverview = (AnchorPane) loader.load(); // Set person overview into the center of root layout. rootLayout.setCenter(personOverview); // Give the controller access to the main app. PersonOverviewController controller = loader.getController(); controller.setMain(this); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;&#125; 将View与Controller绑定我们还需要为FXML文件指定其对应的Controller，以及FXML元素与控制器的属性的对应关系，这是因为FXML文件中的元素只能被对应Controller修改更新，若在其他方法中修改会产生运行时错误。例如：在PersonOverviewController.java中将某个Label返回到Main.java中而后在其中修改该Label的值，意即在非FX线程中执行FX线程相关的任务，则会造成当前的线程阻塞，解决方法之一是使用Platform.runLater()方法，如下所示，括号中的FX线程相关任务便不会阻塞当前进程。 Platform.runLater(() -&gt; &#123; ………相关FX线程代码………&#125;); 当然，最好的选择还是讲FX线程任务和其他任务区分开来，将特定的FXML文件与对应的Controller联系起来，当需要建立联系时可通过之前所说的使用java.beans、ObservableList等方法实现动态更新视图。 为FXML文件指定Controller 在Eclipse中好像有图形化界面直接为FXML文件选择Controller的操作，但是我使用的是IDEA，没有此功能，只能在源代码中指定，如下所示。 PersonOverview.fxml &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;AnchorPane maxHeight=\"-Infinity\" maxWidth=\"-Infinity\" minHeight=\"-Infinity\" minWidth=\"-Infinity\" prefHeight=\"300.0\" prefWidth=\"600.0\" xmlns=\"http://javafx.com/javafx/8.0.121\" xmlns:fx=\"http://javafx.com/fxml/1\" fx:controller=\"com.tanrui.view.PersonOverviewController\"&gt; &lt;children&gt; &lt;? ... 内容省略 ... ?&gt; &lt;/children&gt;&lt;/AnchorPane&gt; 如上述代码所述，在顶层节点（此处是AnchorPane）标签中添加属性如下：fx:controller=&quot;com.tanrui.view.PersonOverviewController”，以此为FXML文件指定Controller 为FXML元素指定fx:id，使其绑定对应的控制器属性 如图，选定特定元素，在右侧界面找到Code-&gt;fx:id，将其对应的控制器属性填入即可 Details界面更新 showPersonDetails(Person person)方法 showPersonDetails(Person person)方法用于使用Person实例的数据填写标签。 PersonOverviewController.java /** * Fills all text fields to show details about the person. * If the specified person is null, all text fields are cleared. * * @param person the person or null */private void showPersonDetails(Person person) &#123; if (person != null) &#123; // Fill the labels with info from the person object. firstNameLabel.setText(person.getFirstName()); lastNameLabel.setText(person.getLastName()); streetLabel.setText(person.getStreet()); postalCodeLabel.setText(Integer.toString(person.getPostalCode())); cityLabel.setText(person.getCity()); // TODO: We need a way to convert the birthday into a String! // birthdayLabel.setText(...); &#125; else &#123; // Person is null, remove all the text. firstNameLabel.setText(\"\"); lastNameLabel.setText(\"\"); streetLabel.setText(\"\"); postalCodeLabel.setText(\"\"); cityLabel.setText(\"\"); birthdayLabel.setText(\"\"); &#125;&#125; 监听用户在人员表中的选择 PersonOverviewController.java @FXMLprivate void initialize() &#123; // Initialize the person table with the two columns. firstNameColumn.setCellValueFactory( cellData -&gt; cellData.getValue().firstNameProperty()); lastNameColumn.setCellValueFactory( cellData -&gt; cellData.getValue().lastNameProperty()); // Clear person details. showPersonDetails(null); // Listen for selection changes and show the person details when changed. personTable.getSelectionModel().selectedItemProperty().addListener( (observable, oldValue, newValue) -&gt; showPersonDetails(newValue));&#125; 删除按钮事件我们的界面已经包含了一个删除的按钮 ，但是并没有为其制定实际的响应操作，因此我们定义一个响应函数，如下： PersonOverviewController.java: /** * Called when the user clicks on the delete button. */ @FXML private void handleDeletePerson() &#123; int selectedIndex = personTable.getSelectionModel().getSelectedIndex(); if (selectedIndex &gt;= 0)&#123; personTable.getItems().remove(selectedIndex); &#125; else&#123; new ShowDialog(main.getPrimaryStage(), Alert.AlertType.WARNING, \"No Person Selected\", \"Please select a person in the table.\").ShowSpecificDialog(); &#125; &#125; 错误处理从上述代码可以看到我们使用了条件判断语句来判断selectedIndex的值，当其小于0时，正常情况我们应该会让其抛出ArrayIndexOutOfBoundsException异常，但是我们想尽量简洁明了的将错误或者警告信息展示给用户，因此这里我们使用了controlsfx包，用于弹出各类提示框（可在ControlsFX官网获取）。 controlsfx有两个主要的版本，同时对于不同的版本，二者的用法也不同： 对于Java 8，需要下载ControlsFX 8.40.14包 对于Java 9及以上，需要下载ControlsFX 9.0.0包 我们这里用到的是Java 10，因此使用ControlsFX 9.0.0，使用方法如下： ShowDialog.java: package com.tanrui.util;import javafx.scene.control.Alert;import javafx.stage.Stage;/** * Util to create and show Dialog. * * @author Tan Rui */public class ShowDialog &#123; private Stage stage; private Alert.AlertType type; private String title; private String message; public ShowDialog(Stage stage, Alert.AlertType type, String title, String message)&#123; this.stage = stage; this.type = type; this.title = title; this.message = message; &#125; public void ShowSpecificDialog()&#123; Alert dlg = new Alert(type); dlg.initOwner(stage); dlg.setTitle(title); dlg.getDialogPane().setContentText(message); dlg.show(); &#125;&#125; PersonOverviewController.java /** * Called when the user clicks on the delete button. */ @FXML private void handleDeletePerson() &#123; int selectedIndex = personTable.getSelectionModel().getSelectedIndex(); if (selectedIndex &gt;= 0)&#123; personTable.getItems().remove(selectedIndex); &#125; else&#123; new ShowDialog(main.getPrimaryStage(), Alert.AlertType.WARNING, \"No Person Selected\", \"Please select a person in the table.\").ShowSpecificDialog(); &#125; &#125; 新建和编辑对话框 Tips：创建一个新的界面、新的Stage（承载新的View时），步骤一般都是： 创建FXML文件，使用SceneBuilder编辑界面； 创建对应的Controller，对FXML中的元素指定对应的属性。主要是为展示型元素指定数据、为控制型元素指定动作等； 连接FXML文件和Controller文件、连接FXML中的元素和Controller中的属性； 在Main函数中加载该控制器 为之前的New和Edit按钮添加动作，弹出对话框（新的Stage）。 设计对话框创建PersonEditDialog.fxml，完成弹出对话框的设计： 创建控制器为对话框创建控制器PersonEditDialogController.java。 PersonEditDialogController.java： package com.tanrui.view;import com.tanrui.util.ShowDialog;import javafx.fxml.FXML;import javafx.scene.control.Alert;import javafx.scene.control.TextField;import javafx.stage.Stage;import com.tanrui.model.Person;import com.tanrui.util.DateUtil;/** * Dialog to edit details of a person. * * @author Marco Jakob */public class PersonEditDialogController &#123; @FXML private TextField firstNameField; @FXML private TextField lastNameField; @FXML private TextField streetField; @FXML private TextField postalCodeField; @FXML private TextField cityField; @FXML private TextField birthdayField; private Stage dialogStage; private Person person; private boolean okClicked = false; /** * Initializes the controller class. This method is automatically called * after the fxml file has been loaded. */ @FXML private void initialize() &#123; &#125; /** * Sets the stage of this dialog. * * @param dialogStage */ public void setDialogStage(Stage dialogStage) &#123; this.dialogStage = dialogStage; &#125; /** * Sets the person to be edited in the dialog. * * @param person */ public void setPerson(Person person) &#123; this.person = person; firstNameField.setText(person.getFirstName()); lastNameField.setText(person.getLastName()); streetField.setText(person.getStreet()); postalCodeField.setText(Integer.toString(person.getPostalCode())); cityField.setText(person.getCity()); birthdayField.setText(DateUtil.format(person.getBirthday())); birthdayField.setPromptText(\"dd.mm.yyyy\"); &#125; /** * Returns true if the user clicked OK, false otherwise. * * @return */ public boolean isOkClicked() &#123; return okClicked; &#125; /** * Called when the user clicks ok. */ @FXML private void handleOk() &#123; if (isInputValid()) &#123; person.setFirstName(firstNameField.getText()); person.setLastName(lastNameField.getText()); person.setStreet(streetField.getText()); person.setPostalCode(Integer.parseInt(postalCodeField.getText())); person.setCity(cityField.getText()); person.setBirthday(DateUtil.parse(birthdayField.getText())); okClicked = true; dialogStage.close(); &#125; &#125; /** * Called when the user clicks cancel. */ @FXML private void handleCancel() &#123; dialogStage.close(); &#125; /** * Validates the user input in the text fields. * * @return true if the input is valid */ private boolean isInputValid() &#123; String errorMessage = \"\"; if (firstNameField.getText() == null || firstNameField.getText().length() == 0) &#123; errorMessage += \"No valid first name!\\n\"; &#125; if (lastNameField.getText() == null || lastNameField.getText().length() == 0) &#123; errorMessage += \"No valid last name!\\n\"; &#125; if (streetField.getText() == null || streetField.getText().length() == 0) &#123; errorMessage += \"No valid street!\\n\"; &#125; if (postalCodeField.getText() == null || postalCodeField.getText().length() == 0) &#123; errorMessage += \"No valid postal code!\\n\"; &#125; else &#123; try &#123; Integer.parseInt(postalCodeField.getText()); &#125; catch (NumberFormatException e) &#123; errorMessage += \"No valid postal code (must be an integer)!\\n\"; &#125; &#125; if (cityField.getText() == null || cityField.getText().length() == 0) &#123; errorMessage += \"No valid city!\\n\"; &#125; if (birthdayField.getText() == null || birthdayField.getText().length() == 0) &#123; errorMessage += \"No valid birthday!\\n\"; &#125; else &#123; if (!DateUtil.validDate(birthdayField.getText())) &#123; errorMessage += \"No valid birthday. Use the format dd.mm.yyyy!\\n\"; &#125; &#125; if (errorMessage.length() == 0) &#123; return true; &#125; else &#123; new ShowDialog(dialogStage, Alert.AlertType.ERROR, \"Invalid Fields\", \"Please correct invalid fields\").ShowSpecificDialog(); return false; &#125; &#125;&#125; 关于该控制器的一些事情应该注意： setPerson(…)方法可以从其它类中调用，用来设置编辑的人员。 当用户点击OK按钮时，调用handleOK()方法。首先，通过调用isInputValid()方法做一些验证。只有验证成功，Person对象使用输入的数据填充。这些修改将直接应用到Person对象上，传递给setPerson(…)。 布尔值okClicked被使用，以便调用者决定用户是否点击OK或者Cancel按钮。 连接视图和控制器使用已经创建的视图（FXML）和控制器，需要连接到一起。 使用SceneBuilder打开PersonEditDialog.fxml文件 在左边的Controller组中选择PersonEditDialogController作为控制器类 设置所有TextField的fx:id到相应的控制器字段上。 设置两个按钮的onAction到相应的处理方法上。 在Main中部署该控制器Main.java: /** * Opens a dialog to edit details for the specified person. If the user * clicks OK, the changes are saved into the provided person object and true * is returned. * * @param person the person object to be edited * @return true if the user clicked OK, false otherwise. */public boolean showPersonEditDialog(Person person) &#123; try &#123; // Load the fxml file and create a new stage for the popup dialog. FXMLLoader loader = new FXMLLoader(); loader.setLocation(Main.class.getResource(\"view/PersonEditDialog.fxml\")); AnchorPane page = (AnchorPane) loader.load(); // Create the dialog Stage. Stage dialogStage = new Stage(); dialogStage.setTitle(\"Edit Person\"); dialogStage.initModality(Modality.WINDOW_MODAL); dialogStage.initOwner(primaryStage); Scene scene = new Scene(page); dialogStage.setScene(scene); // Set the person into the controller. PersonEditDialogController controller = loader.getController(); controller.setDialogStage(dialogStage); controller.setPerson(person); // Show the dialog and wait until the user closes it dialogStage.showAndWait(); return controller.isOkClicked(); &#125; catch (IOException e) &#123; e.printStackTrace(); return false; &#125;&#125; 为主界面中New和Edit按钮创建OnAction方法，这些方法将从Main中调用showPersonEditDialog(…)方法。 PersonOverviewController.java: /** * Called when the user clicks the new button. Opens a dialog to edit * details for a new person. */@FXMLprivate void handleNewPerson() &#123; Person tempPerson = new Person(); boolean okClicked = main.showPersonEditDialog(tempPerson); if (okClicked) &#123; main.getPersonData().add(tempPerson); &#125;&#125;/** * Called when the user clicks the edit button. Opens a dialog to edit * details for the selected person. */@FXMLprivate void handleEditPerson() &#123; Person selectedPerson = personTable.getSelectionModel().getSelectedItem(); if (selectedPerson != null) &#123; boolean okClicked = main.showPersonEditDialog(selectedPerson); if (okClicked) &#123; showPersonDetails(selectedPerson); &#125; &#125; else &#123; new ShowDialog(main.getPrimaryStage(), Alert.AlertType.WARNING, \"No Person Selected\", \"Please select a person in the table.\").ShowSpecificDialog(); &#125;&#125; 而后在PersonOverview.fxml中为New和Edit两个按钮绑定对应的OnAction方法： 数据持久化我们有很多种方法来实现应用数据的持久化，例如： 使用数据库存储 使用Json文件存储 使用XML文件存储 …… 这里我们使用XML文件格式存储应用数据。之前的我们应用的数据都只是存在内存中，内存的特性使得关闭应用程序后数据便会丢失，因此我们下面要做的就是： 每次打开应用可加载上一次的用户数据 用户可选择保存当前数据到指定XML文件 用户可选择从指定XML文件加载数据 使用Preferences保存应用状态Java提供了Preferences类来帮助我们存储用户配置（本例中是XML数据文件的路径，用于下次打开从该文件中加载），Preferences类底层对各类操作系统进行了封装（实际上是Windows系统、OS X系统和类Unix文件系统三种），用户配置在Windows系统上可能保存在注册表中、在类Unix文件系统上可能保存在/tmp下的某个隐藏文件中，而对于使用者来说这些实现细节都不必考虑，只需知道Preferences类是用来保存用户配置即可。用法如下： Main.java: /** * Returns the person file preference, i.e. the file that was last opened. * The preference is read from the OS specific registry. If no such * preference can be found, null is returned. * * @return */ public File getPersonFilePath() &#123; Preferences prefs = Preferences.userNodeForPackage(Main.class); String filePath = prefs.get(\"filePath\", null); if (filePath != null) &#123; return new File(filePath); &#125; else &#123; return null; &#125; &#125; /** * Sets the file path of the currently loaded file. The path is persisted in * the OS specific registry. * * @param file the file or null to remove the path */ public void setPersonFilePath(File file) &#123; Preferences prefs = Preferences.userNodeForPackage(Main.class); if (file != null) &#123; prefs.put(\"filePath\", file.getPath()); // Update the stage title. primaryStage.setTitle(\"AddressApp - \" + file.getName()); &#125; else &#123; prefs.remove(\"filePath\"); // Update the stage title. primaryStage.setTitle(\"AddressApp\"); &#125; &#125; 使用JAXBJAXB包是Java中提供的对数据进行编列(marshall)成XML文件以及对XML文件反编列(unmarshall)为数据结构的包，Java SE中有如下支持类型：JAXB 2.0是JDK 1.6的组成部分。JAXB 2.2.3是JDK 1.7以上的组成部分，而实际上在Java 9之后就已将JAXB包移除，因此使用时需添加额外的lib包，详情可见博客真正解决方案：java.lang.ClassNotFoundException: javax.xml.bind.JAXBException。 JAXB模型类我们希望持久化的数据应该是Main中的personData，而JAXB有以下要求： 使用@XmlRootElement定义XML根元素的名称 使用@XmlElement指定一个XML元素，可选 而Main中的personData是ObservableList类型，由于ObservableList类型不支持添加注解，因此我们需要创建另外一个能保存Person列表同时又能存储为XML文件的类，如下。 PersonListWrapper.java: package com.tanrui.model;import java.util.List;import javax.xml.bind.annotation.XmlElement;import javax.xml.bind.annotation.XmlRootElement;/** * Helper class to wrap a list of persons. This is used for saving the * list of persons to XML. */@XmlRootElement(name = \"persons\")public class PersonListWrapper &#123; private List&lt;Person&gt; persons; @XmlElement(name = \"person\") public List&lt;Person&gt; getPersons() &#123; return persons; &#125; public void setPersons(List&lt;Person&gt; persons) &#123; this.persons = persons; &#125;&#125; 使用JAXB读写数据到XML文件我们将读写XML文件的逻辑放到Main类中，Controller在用到相应的逻辑时，直接调用Main中的方法即可。 Main.java: /** * Loads person data from the specified file. The current person data will * be replaced. * * @param file */public void loadPersonDataFromFile(File file) &#123; try &#123; JAXBContext context = JAXBContext .newInstance(PersonListWrapper.class); Unmarshaller um = context.createUnmarshaller(); // Reading XML from the file and unmarshalling. PersonListWrapper wrapper = (PersonListWrapper) um.unmarshal(file); personData.clear(); personData.addAll(wrapper.getPersons()); // Save the file path to the registry. setPersonFilePath(file); &#125; catch (Exception e) &#123; // catches ANY exception new ShowDialog(this.getPrimaryStage(), Alert.AlertType.ERROR, \"Error\", \"Could not save data to file:\\n\" + file.getPath()).ShowSpecificDialog(); &#125;&#125;/** * Saves the current person data to the specified file. * * @param file */public void savePersonDataToFile(File file) &#123; try &#123; JAXBContext context = JAXBContext.newInstance(PersonListWrapper.class); Marshaller m = context.createMarshaller(); m.setProperty(Marshaller.JAXB_FORMATTED_OUTPUT, true); // Wrapping our person data. PersonListWrapper wrapper = new PersonListWrapper(); wrapper.setPersons(personData); // Marshalling and saving XML to the file. m.marshal(wrapper, file); // Save the file path to the registry. setPersonFilePath(file); &#125; catch (Exception e) &#123; // catches ANY exception new ShowDialog(this.getPrimaryStage(), Alert.AlertType.ERROR, \"Error\", \"Could not save data to file:\\n\" + file.getPath()).ShowSpecificDialog(); &#125;&#125; 编组(marshall):savePersonDataToFile(…)和解组(unmarshall):loadPersonDataFromFile(…)已准备好，下面在界面中使用它。 创建打开和保存菜单为File菜单添加子项 处理菜单相应动作Controller中使用FileChooser的方法，FileChooser同样封装了不同操作系统的具体实现，使用者仅需调用接口即可。 本类中使用了FileChooser.ExtensionFilter，对文件系统中文件进行过滤，保留.xml结尾的文件。 当用户选择特定文件而后点击打开按钮时，会返回该文件，否则返回Null。 package com.tanrui.view;import com.tanrui.Main;import com.tanrui.util.ShowDialog;import javafx.fxml.FXML;import javafx.scene.control.Alert;import javafx.stage.FileChooser;import java.io.File;/** * The controller for the root layout. The root layout provides the basic * application layout containing a menu bar and space where other JavaFX * elements can be placed. */public class RootLayoutController &#123; // Reference to the main application private Main main; /** * Is called by the main application to give a reference back to itself. * * @param main */ public void setMain(Main main) &#123; this.main = main; &#125; /** * Creates an empty address book. */ @FXML private void handleNew() &#123; main.getPersonData().clear(); main.setPersonFilePath(null); &#125; /** * Opens a FileChooser to let the user select an address book to load. */ @FXML private void handleOpen() &#123; FileChooser fileChooser = new FileChooser(); // Set extension filter FileChooser.ExtensionFilter extFilter = new FileChooser.ExtensionFilter( \"XML files (*.xml)\", \"*.xml\"); fileChooser.getExtensionFilters().add(extFilter); // Show save file dialog File file = fileChooser.showOpenDialog(main.getPrimaryStage()); if (file != null) &#123; main.loadPersonDataFromFile(file); &#125; &#125; /** * Saves the file to the person file that is currently open. If there is no * open file, the \"save as\" dialog is shown. */ @FXML private void handleSave() &#123; File personFile = main.getPersonFilePath(); if (personFile != null) &#123; main.savePersonDataToFile(personFile); &#125; else &#123; handleSaveAs(); &#125; &#125; /** * Opens a FileChooser to let the user select a file to save to. */ @FXML private void handleSaveAs() &#123; FileChooser fileChooser = new FileChooser(); // Set extension filter FileChooser.ExtensionFilter extFilter = new FileChooser.ExtensionFilter( \"XML files (*.xml)\", \"*.xml\"); fileChooser.getExtensionFilters().add(extFilter); // Show save file dialog File file = fileChooser.showSaveDialog(main.getPrimaryStage()); if (file != null) &#123; // Make sure it has the correct extension if (!file.getPath().endsWith(\".xml\")) &#123; file = new File(file.getPath() + \".xml\"); &#125; main.savePersonDataToFile(file); &#125; &#125; /** * Opens an about dialog. */ @FXML private void handleAbout() &#123; new ShowDialog(main.getPrimaryStage(), Alert.AlertType.INFORMATION, \"About\", \"Author: Tan\\\\nWebsite: https://guitoubing.top\").ShowSpecificDialog(); &#125; /** * Closes the application. */ @FXML private void handleExit() &#123; System.exit(0); &#125; /** * Opens the birthday statistics. */ @FXML private void handleShowBirthdayStatistics() &#123; main.showBirthdayStatistics(); &#125;&#125; 连接FXML文件和Controller、绑定菜单和对应动作 在Main中部署该控制器Main.java: /** * Initializes the root layout. */ public void initRootLayout() &#123; try &#123; // Load root layout from fxml file. FXMLLoader loader = new FXMLLoader(); loader.setLocation(Main.class.getResource(\"view/RootLayout.fxml\")); rootLayout = (BorderPane) loader.load(); // Show the scene containing the root layout. Scene scene = new Scene(rootLayout); primaryStage.setScene(scene); RootLayoutController controller = loader.getController(); controller.setMain(this); primaryStage.show(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; File file = getPersonFilePath(); if (file != null)&#123; loadPersonDataFromFile(file); &#125; &#125; 参考资料 code.makery —— JavaFX中文教程 JavaFX Tutorial 真正解决方案：java.lang.ClassNotFoundException: javax.xml.bind.JAXBException fxexperience —— ControlFX Java SE8 —— Lambda ………… 写在后面本博主要是在学习code.makery —— JavaFX中文教程博客中对于JavaFX的教程，跟着博主的项目逻辑和代码自己过了一遍，对一些由于版本不兼容（博主使用的是JDK 8u40，我这里使用的是Java 10 2018-03-20）造成的问题进行了解决，同时对项目过程中一些功能进行了拓展学习，研究了很多用到的包源码，收获颇多。可点击JavaFX-Test中获取源码。 希望藉此次JavaFX学习开启我的Java源码学习之旅，道阻且长！","categories":[{"name":"Java","slug":"Java","permalink":"http://blog.guitoubing.top/categories/Java/"}],"tags":[{"name":"JavaFX","slug":"JavaFX","permalink":"http://blog.guitoubing.top/tags/JavaFX/"}]},{"title":"内存数据库 - 课程笔记","slug":"TimesTen内存数据库课程笔记","date":"2018-09-02T12:59:50.000Z","updated":"2021-05-16T11:00:13.403Z","comments":true,"path":"2018/09/02/TimesTen内存数据库课程笔记/","link":"","permalink":"http://blog.guitoubing.top/2018/09/02/TimesTen内存数据库课程笔记/","excerpt":"内存计算与内存数据库第零章OLTP：行存储（记录：元组），联机事务处理 OLAP：列存储（key-value），联机分析处理 Timesten操作小记","text":"内存计算与内存数据库第零章OLTP：行存储（记录：元组），联机事务处理 OLAP：列存储（key-value），联机分析处理 Timesten操作小记 平台 系统：Red Hat Enterprise Linux Server release 5.7 (Tikanga) 创建DSN（Data Source Name） 逻辑名，用于标识某一数据库连接 打开数据库配置文件(通常称为系统ODBC.INI配置文件)$ cd $TT_HOME/info$ gedit sys.odbc.ini 在数据库DSN列表中添加需要新建的数据库名称# 添加my_ttdb数据库，“=”后面是指该数据库使用某种驱动，如第3行所示[ODBC Data Sources]my_ttdb=TimesTen 11.2.2 DriverTT_1122=TimesTen 11.2.2 Driversampledb_1122=TimesTen 11.2.2 Drivercachedb1_1122=TimesTen 11.2.2 Driverrepdb1_1122=TimesTen 11.2.2 Driverrepdb2_1122=TimesTen 11.2.2 DriversampledbCS_1122=TimesTen 11.2.2 Client Drivercachedb1CS_1122=TimesTen 11.2.2 Client Driverrepdb1CS_1122=TimesTen 11.2.2 Client Driverrepdb2CS_1122=TimesTen 11.2.2 Client Driver 为2中创建的数据库添加配置，日志文件与检查点文件应存储在不同磁盘中# 配置my_ttdb[my_ttdb]# 数据库监听器驱动位置Driver=/home/oracle/TimesTen/tt1122/lib/libtten.so # DataStore为检查点文件存储位置DataStore=/u02/ttdata/datastores/my_ttdb # LogDir为日志文件存储位置LogDir=/u03/ttdata/logs# 以下两个Size是TimesTen内存数据库的内存分配PermSize=40TempSize=32# 数据库的字符集DatabaseCharacterSet=AL32UTF8 TimesTen的内存分配主要是PermSize和TempSize两块，可先参考博客如何更改TimesTen数据库的大小。 保存配置文件并关闭数据库服务器基本命令查看服务器状态[oracle@timesten-hol info]$ ttstatusTimesTen status report as of Thu Sep 27 04:08:30 2018Daemon pid 2637 port 53392 instance tt1122TimesTen server pid 2646 started on port 53393------------------------------------------------------------------------Accessible by group oracleEnd of report 启动/停止数据库[oracle@timesten-hol info]$ ttdaemonadmin -stopTimesTen Daemon stopped.[oracle@timesten-hol info]$ ttstatusttStatus: Could not connect to the TimesTen daemon.If the TimesTen daemon is not running, please start itby running \"ttDaemonAdmin -start\".[oracle@timesten-hol info]$ ttdaemonadmin -startTimesTen Daemon startup OK.[oracle@timesten-hol info]$ ttstatusTimesTen status report as of Thu Sep 27 04:10:00 2018Daemon pid 6522 port 53392 instance tt1122TimesTen server pid 6531 started on port 53393------------------------------------------------------------------------Accessible by group oracleEnd of report 创建TimesTen内存数据库 默认情况下，TimesTen内存数据库在第一次连接到数据库时创建并加载到内存中，并在关闭数据库的最后一个连接时从内存卸载。当然此行为可通过ttadmin -RAMPolicy修改，后面会说到。 也就是说，默认情况下（前提是RAM策略为inUse，下一节会讲到RAM策略的修改），每次在执行connect “dsn=ttdb_name”连接到一个特定的DSN时，都是一个创建TimesTen内存数据库、加载数据到内存中等过程，因此本节的标题是创建而不是连接到。 连接到特定DSN，创建内存数据库[oracle@timesten-hol info]$ ttisqlCopyright (c) 1996, 2014, Oracle and/or its affiliates. All rights reserved.Type ? or \"help\" for help, type \"exit\" to quit ttIsql.Command&gt; connect \"dsn=my_ttdb\";Connection successful: DSN=my_ttdb;UID=oracle;DataStore=/u02/ttdata/datastores/my_ttdb;DatabaseCharacterSet=AL32UTF8;ConnectionCharacterSet=US7ASCII;DRIVER=/home/oracle/TimesTen/tt1122/lib/libtten.so;LogDir=/u03/ttdata/logs;PermSize=40;TempSize=32;TypeMode=0;(Default setting AutoCommit=1) 或者直接在ttisql中指定DSN名称： [oracle@timesten-hol info]$ ttisql \"dsn=my_ttdb\"Copyright (c) 1996, 2014, Oracle and/or its affiliates. All rights reserved.Type ? or \"help\" for help, type \"exit\" to quit ttIsql.connect \"dsn=my_ttdb\";Connection successful: DSN=my_ttdb;UID=oracle;DataStore=/u02/ttdata/datastores/my_ttdb;DatabaseCharacterSet=AL32UTF8;ConnectionCharacterSet=US7ASCII;DRIVER=/home/oracle/TimesTen/tt1122/lib/libtten.so;LogDir=/u03/ttdata/logs;PermSize=40;TempSize=32;TypeMode=0;(Default setting AutoCommit=1)[oracle@timesten-hol ~]$ ttisql my_ttdbCopyright (c) 1996, 2014, Oracle and/or its affiliates. All rights reserved.Type ? or \"help\" for help, type \"exit\" to quit ttIsql.connect \"DSN=my_ttdb\";Connection successful: DSN=my_ttdb;UID=oracle;DataStore=/u02/ttdata/datastores/my_ttdb;DatabaseCharacterSet=AL32UTF8;ConnectionCharacterSet=US7ASCII;DRIVER=/home/oracle/TimesTen/tt1122/lib/libtten.so;LogDir=/u03/ttdata/logs;PermSize=40;TempSize=32;TypeMode=0;(Default setting AutoCommit=1) 问题：重复运行connect “dsn=ttdb_name”命令可以看到命令行中显示了多了连接，这是什么作用呢？ &gt; Command&gt; connect \"dsn=my_ttdb\";&gt; Connection successful: DSN=my_ttdb;UID=oracle;DataStore=/u02/ttdata/datastores/my_ttdb;DatabaseCharacterSet=AL32UTF8;ConnectionCharacterSet=US7ASCII;DRIVER=/home/oracle/TimesTen/tt1122/lib/libtten.so;LogDir=/u03/ttdata/logs;PermSize=40;TempSize=32;TypeMode=0;&gt; (Default setting AutoCommit=1)&gt; Command&gt; connect \"dsn=my_ttdb\";&gt; Connection successful: DSN=my_ttdb;UID=oracle;DataStore=/u02/ttdata/datastores/my_ttdb;DatabaseCharacterSet=AL32UTF8;ConnectionCharacterSet=US7ASCII;DRIVER=/home/oracle/TimesTen/tt1122/lib/libtten.so;LogDir=/u03/ttdata/logs;PermSize=40;TempSize=32;TypeMode=0;&gt; (Default setting AutoCommit=1)&gt; con1: Command&gt; connect \"dsn=my_ttdb\";&gt; Connection successful: DSN=my_ttdb;UID=oracle;DataStore=/u02/ttdata/datastores/my_ttdb;DatabaseCharacterSet=AL32UTF8;ConnectionCharacterSet=US7ASCII;DRIVER=/home/oracle/TimesTen/tt1122/lib/libtten.so;LogDir=/u03/ttdata/logs;PermSize=40;TempSize=32;TypeMode=0;&gt; (Default setting AutoCommit=1)&gt; con2: Command&gt; connect \"dsn=my_ttdb\";&gt; Connection successful: DSN=my_ttdb;UID=oracle;DataStore=/u02/ttdata/datastores/my_ttdb;DatabaseCharacterSet=AL32UTF8;ConnectionCharacterSet=US7ASCII;DRIVER=/home/oracle/TimesTen/tt1122/lib/libtten.so;LogDir=/u03/ttdata/logs;PermSize=40;TempSize=32;TypeMode=0;&gt; (Default setting AutoCommit=1)&gt; con3: Command&gt; &gt; 查看内存数据库的内存分配及容量Command&gt; dssize PERM_ALLOCATED_SIZE: 40960 PERM_IN_USE_SIZE: 9453 PERM_IN_USE_HIGH_WATER: 9453 TEMP_ALLOCATED_SIZE: 32768 TEMP_IN_USE_SIZE: 9442 TEMP_IN_USE_HIGH_WATER: 9505 使用Host命令可以调用操作系统级别的指令Command&gt; host ttstatus;TimesTen status report as of Thu Sep 27 04:37:28 2018Daemon pid 6522 port 53392 instance tt1122TimesTen server pid 6531 started on port 53393------------------------------------------------------------------------Data store /u01/ttdata/datastores/my_ttdbThere are 12 connections to the data storeShared Memory KEY 0x1200c904 ID 2785297PL/SQL Memory KEY 0x1300c904 ID 2818066 Address 0x7fa0000000Type PID Context Connection Name ConnIDProcess 6973 0x0000000000c72c00 my_ttdb 1Subdaemon 6529 0x00000000012d3360 Manager 142Subdaemon 6529 0x000000000132a1e0 Rollback 141Subdaemon 6529 0x000000000140b360 HistGC 139Subdaemon 6529 0x0000000001420070 AsyncMV 140Subdaemon 6529 0x00000000014b4e00 Log Marker 136Subdaemon 6529 0x0000000001509a30 Deadlock Detector 135Subdaemon 6529 0x000000000151e620 Flusher 134Subdaemon 6529 0x0000000001533210 Checkpoint 133Subdaemon 6529 0x00000000016286b0 Monitor 132Subdaemon 6529 0x00007f95880208e0 Aging 138Subdaemon 6529 0x00007f958808f900 IndexGC 137Replication policy : ManualCache Agent policy : ManualPL/SQL enabled.------------------------------------------------------------------------Accessible by group oracleEnd of report 修改RAM策略 上一节讲到每一次的连接到特定的DSN都是新建一个内存数据库的过程，当然这是基于默认RAM策略为inUse的情况，下面会讲到当RAM策略设置为Manual时创建内存数据库的过程。 Manual策略适用于当数据库中数据规模巨大，装载到内存中的时间可能很长，从而导致内存数据库效率低下；而inUse策略适用于大多数情况，数据规模不是很大，装载到内存中的时间很短或者说在业务需求中可以忽略不计。 查看当前RAM策略[oracle@timesten-hol info]$ ttadmin my_ttdbRAM Residence Policy : inUseReplication Agent Policy : manualReplication Manually Started : FalseCache Agent Policy : manualCache Agent Manually Started : False 修改RAM策略为手动模式（Manual） 手动模式下，创建DSN连接时并不会将数据加载到内存中，需要手动进行数据装载和卸载 [oracle@timesten-hol info]$ ttadmin -rampolicy manual my_ttdbRAM Residence Policy : manualManually Loaded In RAM : FalseReplication Agent Policy : manualReplication Manually Started : FalseCache Agent Policy : manualCache Agent Manually Started : False[oracle@timesten-hol info]$ ttisql \"dsn=my_ttdb\";Copyright (c) 1996, 2014, Oracle and/or its affiliates. All rights reserved.Type ? or \"help\" for help, type \"exit\" to quit ttIsql.connect \"dsn=my_ttdb\"; 707: Attempt to connect to a data store that has been manually unloaded from RAMThe command failed.Done.[oracle@timesten-hol info]$ 向内存中装载数据[oracle@timesten-hol info]$ ttadmin -ramload my_ttdbRAM Residence Policy : manualManually Loaded In RAM : TrueReplication Agent Policy : manualReplication Manually Started : FalseCache Agent Policy : manualCache Agent Manually Started : False[oracle@timesten-hol info]$ ttisql \"dsn=my_ttdb\";Copyright (c) 1996, 2014, Oracle and/or its affiliates. All rights reserved.Type ? or \"help\" for help, type \"exit\" to quit ttIsql.connect \"dsn=my_ttdb\";Connection successful: DSN=my_ttdb;UID=oracle;DataStore=/u02/ttdata/datastores/my_ttdb;DatabaseCharacterSet=AL32UTF8;ConnectionCharacterSet=US7ASCII;DRIVER=/home/oracle/TimesTen/tt1122/lib/libtten.so;LogDir=/u03/ttdata/logs;PermSize=40;TempSize=32;TypeMode=0;(Default setting AutoCommit=1)Command&gt; 从内存中卸载数据[oracle@timesten-hol info]$ ttadmin -ramunload my_ttdbRAM Residence Policy : manualManually Loaded In RAM : FalseReplication Agent Policy : manualReplication Manually Started : FalseCache Agent Policy : manualCache Agent Manually Started : False[oracle@timesten-hol info]$ ttisql \"dsn=my_ttdb\";Copyright (c) 1996, 2014, Oracle and/or its affiliates. All rights reserved.Type ? or \"help\" for help, type \"exit\" to quit ttIsql.connect \"dsn=my_ttdb\"; 707: Attempt to connect to a data store that has been manually unloaded from RAMThe command failed.Done.[oracle@timesten-hol info]$ 日志和检查点查看日志文件，提交之前会预写日志Command&gt; host ls -al /u03/ttdata/logs/my*-rw-rw---- 1 oracle oracle 18270208 Sep 28 23:00 /u03/ttdata/logs/my_ttdb.log4-rw-rw---- 1 oracle oracle 67108864 Sep 27 04:18 /u03/ttdata/logs/my_ttdb.res0-rw-rw---- 1 oracle oracle 67108864 Sep 27 04:18 /u03/ttdata/logs/my_ttdb.res1-rw-rw---- 1 oracle oracle 67108864 Sep 27 04:18 /u03/ttdata/logs/my_ttdb.res2 查看检查点Command&gt; host ls -al /u02/ttdata/datastores/my*-rw-rw---- 1 oracle oracle 31906840 Sep 28 23:00 /u02/ttdata/datastores/my_ttdb.ds0-rw-rw---- 1 oracle oracle 31906840 Sep 28 22:57 /u02/ttdata/datastores/my_ttdb.ds1 手动更新检查点文件 非手动状态下检查点会每间隔一段时间执行一次，会将自上次检查点后提交的事务更新到检查点中；检查点文件是非阻塞的，即更新检查点文件时也可执行事务。 如下调用检查点文件： Command&gt; call ttckpt;Command&gt; call ttckpt; ttisql基本命令——用户操作创建用户，可在表sys.all_users中查找所有的用户信息Command&gt; select * from sys.all_users;&lt; SYS, 0, 2018-09-27 04:18:18.063030 &gt;&lt; TTREP, 2, 2018-09-27 04:18:18.063030 &gt;&lt; SYSTEM, 3, 2018-09-27 04:18:18.063030 &gt;&lt; GRID, 4, 2018-09-27 04:18:18.063030 &gt;&lt; ORACLE, 10, 2018-09-27 04:18:18.063030 &gt;&lt; SCOTT, 11, 2018-09-27 05:06:39.267433 &gt;6 rows found.Command&gt; create user tthr identified by tthr;User created.Command&gt; select * from sys.all_users;&lt; SYS, 0, 2018-09-27 04:18:18.063030 &gt;&lt; TTREP, 2, 2018-09-27 04:18:18.063030 &gt;&lt; SYSTEM, 3, 2018-09-27 04:18:18.063030 &gt;&lt; GRID, 4, 2018-09-27 04:18:18.063030 &gt;&lt; ORACLE, 10, 2018-09-27 04:18:18.063030 &gt;&lt; SCOTT, 11, 2018-09-27 05:06:39.267433 &gt;&lt; TTHR, 12, 2018-09-28 23:11:57.126074 &gt;7 rows found. 给用户分配权限Command&gt; grant create session to tthr;Command&gt; grant create table to tthr;Command&gt; grant create view to tthr;Command&gt; grant create sequence to tthr; 查看当前数据库系统内用户权限Command&gt; select * from sys.dba_sys_privs;&lt; SYS, ADMIN, YES &gt;&lt; SYSTEM, ADMIN, YES &gt;&lt; ORACLE, ADMIN, YES &gt;&lt; SCOTT, CREATE SESSION, NO &gt;&lt; SCOTT, CREATE TABLE, NO &gt;&lt; TTHR, CREATE SESSION, NO &gt;&lt; TTHR, CREATE TABLE, NO &gt;&lt; TTHR, CREATE VIEW, NO &gt;&lt; TTHR, CREATE SEQUENCE, NO &gt;9 rows found. 撤回用户权限 以下示例展示了如何从用户撤回权限（赋予delete any table权限后再撤回该权限） Command&gt; grant delete any table to tthr;Command&gt; select * from sys.dba_sys_privs;&lt; SYS, ADMIN, YES &gt;&lt; SYSTEM, ADMIN, YES &gt;&lt; ORACLE, ADMIN, YES &gt;&lt; SCOTT, CREATE SESSION, NO &gt;&lt; SCOTT, CREATE TABLE, NO &gt;&lt; TTHR, CREATE SESSION, NO &gt;&lt; TTHR, DELETE ANY TABLE, NO &gt;&lt; TTHR, CREATE TABLE, NO &gt;&lt; TTHR, CREATE VIEW, NO &gt;&lt; TTHR, CREATE SEQUENCE, NO &gt;10 rows found.Command&gt; revoke delete any table from tthr;Command&gt; select * from sys.dba_sys_privs;&lt; SYS, ADMIN, YES &gt;&lt; SYSTEM, ADMIN, YES &gt;&lt; ORACLE, ADMIN, YES &gt;&lt; SCOTT, CREATE SESSION, NO &gt;&lt; SCOTT, CREATE TABLE, NO &gt;&lt; TTHR, CREATE SESSION, NO &gt;&lt; TTHR, CREATE TABLE, NO &gt;&lt; TTHR, CREATE VIEW, NO &gt;&lt; TTHR, CREATE SEQUENCE, NO &gt;9 rows found. ttisql基本命令——数据库对象操作关闭自动提交 意即每次执行事务后，均需要执行commit以提交事务。 Command&gt; autocommit off; 建表、插入数据Command&gt; create table ttemployees &gt; (employee_id NUMBER(6) NOT NULL, &gt; last_name VARCHAR2(10) NOT NULL, hire_date DATE, performance_report CLOB, &gt; PRIMARY KEY (employee_id) ) &gt; UNIQUE HASH ON (employee_id) PAGES = 1;Command&gt; insert into ttemployees values (1, &apos;Smith&apos;, &apos;2009-02-23&apos;, &apos;excellent&apos;); 1 row inserted.Command&gt; insert into ttemployees values (2, &apos;King&apos;, &apos;2005-08-05&apos;, &apos;great&apos;);1 row inserted.Command&gt; insert into ttemployees values (3, &apos;Taylor&apos;, &apos;2012-01-28&apos;, EMPTY_CLOB());1 row inserted.Command&gt; commit; 一些命令总结 tables and alltables - Lists tables indexes and allindexes - Lists indexes views and allviews - Lists views sequences and allsequences - Lists sequences synonyms and allsynonyms - Lists synonyms functions and allfunctions - Lists PL/SQL functions procedures and allprocedures - Lists PL/SQL procedures packages and allpackages - Lists PL/SQL packages PLSQL编程创建plsqldb、pls用户、运行sql脚本call ttOptUpdateStats;// 更新统计数据，用于分析生成最优执行计划 使用sql developer连接TimesTen和Oracle配置如下： plsql语法 What Is a PL/SQL Package?A package is a schema object that groups logically related PL/SQL types, items, and subprograms. Packages usually have two parts, a specification and a body, although sometimes the body is unnecessary. The specification (spec for short) is the interface to your applications; it declares the types, variables, constants, exceptions, cursors, and subprograms available for use. The body fully defines cursors and subprograms, and so implements the spec. 包是一个模式对象，它对逻辑上相关的PL/SQL类型、项和子程序进行分组。包通常有两个部分，规范和主体，主体不是必要的。规范是应用程序的接口：它声明可用的类型、变量、常量、异常、游标和子程序。主体将完全定义游标和子程序，以此实现规范。 As Figure 9-1 shows, you can think of the spec as an operational interface and of the body as a “black box.” You can debug, enhance, or replace a package body without changing the interface (package spec) to the package. ——Oracle PL/SQL Package文档 1_package.sql: CREATE OR REPLACE PACKAGE test AS -- Declare a record for the desired EMP fields TYPE empRecType IS RECORD ( r_empno EMP.EMPNO%TYPE, -- 使用EMP表中EMPNO的类型 r_ename EMP.ENAME%TYPE, r_salary EMP.SAL%TYPE ); -- Declare a Ref Cursor type TYPE EmpCurType IS REF CURSOR RETURN empRecType; -- 游标类型需要有返回值 -- A parameterized cursor，定义 -- 游标 CURSOR low_paid (num PLS_INTEGER) IS SELECT empno FROM emp WHERE rownum &lt;= num ORDER BY sal ASC; -- 过程(IN表示输入，OUT表示输出) PROCEDURE ddl_dml (myComment IN VARCHAR2, errCode OUT PLS_INTEGER, -- 整型 errText OUT VARCHAR2); PROCEDURE givePayRise (num IN PLS_INTEGER, name OUT EMP.ENAME%TYPE, -- name是plsql中的保留字，应该尽量避免使用保留字 errCode OUT PLS_INTEGER, errText OUT VARCHAR2); PROCEDURE getCommEmps (empRefCur IN OUT EmpCurType, errCode OUT PLS_INTEGER, errText OUT VARCHAR2); -- Associative array TYPE sum_multiples IS TABLE OF PLS_INTEGER -- Associative array type INDEX BY PLS_INTEGER; -- indexed by pls_integer FUNCTION get_sum_multiples ( multiple IN PLS_INTEGER, num IN PLS_INTEGER ) RETURN sum_multiples;END test;/CREATE OR REPLACE PACKAGE BODY test AS PROCEDURE ddl_dml (myComment IN VARCHAR2, errCode OUT PLS_INTEGER, errText OUT VARCHAR2) IS sql_str VARCHAR2(256); name_already_exists EXCEPTION; insufficient_privileges EXCEPTION; PRAGMA EXCEPTION_INIT(name_already_exists, -0955); PRAGMA EXCEPTION_INIT(insufficient_privileges, -1031); seq_value number; BEGIN BEGIN sql_str := &apos;create table foo (COL1 VARCHAR2 (20),COL2 NVARCHAR2 (60))&apos;; DBMS_OUTPUT.PUT_LINE(sql_str); execute immediate sql_str; EXCEPTION WHEN name_already_exists THEN DBMS_OUTPUT.PUT_LINE(&apos; Ignore existing table errors&apos;); WHEN insufficient_privileges THEN DBMS_OUTPUT.PUT_LINE(&apos; Ignore insufficient privileges errors&apos;); END; -- Cast num_col1 and char_col values insert into temp values (1, 1, myComment); commit; errCode := 0; errtext := &apos;OK&apos;; EXCEPTION WHEN name_already_exists THEN errCode := 0; errtext := &apos;OK&apos;; WHEN OTHERS THEN errCode := SQLCODE; errText := SUBSTR(SQLERRM, 1, 200); END ddl_dml; PROCEDURE givePayRise (num IN PLS_INTEGER, name OUT EMP.ENAME%TYPE, errCode OUT PLS_INTEGER, errText OUT VARCHAR2) IS -- Can use PLSQL collections within TimesTen PLSQL TYPE lowest_paid_type IS TABLE OF emp.empno%TYPE; lowest_paid lowest_paid_type; i PLS_INTEGER; numRows PLS_INTEGER; lucky_index PLS_INTEGER; lucky_emp EMP.EMPNO%TYPE; BEGIN -- Initialize the output variable name := &apos;Nobody&apos;; -- Initialize the collection lowest_paid := lowest_paid_type(0, 1, 2, 3, 4, 5, 6, 7, 8, 9); i := 1; -- Constrain the resultset size IF num &lt; 1 OR num &gt; 10 THEN -- If bad inputs, default to 5 rows numRows := 5; ELSE numRows := num; END IF; -- Create the cursor resultset with up to &apos;numRows&apos; rows OPEN low_paid( numRows ); LOOP -- Get the current empid FETCH low_paid INTO lowest_paid(i); EXIT WHEN low_paid%NOTFOUND; -- Increment the PLSQL table index i := i + 1; END LOOP; -- Close the cursor CLOSE low_paid; -- List the subset of lowest paid employees FOR j in lowest_paid.FIRST .. numRows LOOP DBMS_OUTPUT.PUT_LINE(&apos; Lowest paid empno &apos; || j || &apos; is &apos; || lowest_paid(j) ); END LOOP; -- Randomly choose one of the lowest paid employees for a 10% pay raise. lucky_index := trunc(dbms_random.value(lowest_paid.FIRST, numRows)); lucky_emp := lowest_paid(lucky_index); -- Give lucky_emp a 10% pay raise and return their name UPDATE emp SET sal = sal * 1.1 WHERE empno = lucky_emp RETURNING ename INTO name; COMMIT; errCode := 0; errtext := &apos;OK&apos;; EXCEPTION WHEN OTHERS THEN errCode := SQLCODE; errText := SUBSTR(SQLERRM, 1, 200); END givePayRise; PROCEDURE getCommEmps (empRefCur IN OUT EmpCurType, errCode OUT PLS_INTEGER, errText OUT VARCHAR2) IS salesGuy empRecType; BEGIN DBMS_OUTPUT.PUT_LINE(&apos; &apos;); DBMS_OUTPUT.PUT_LINE(&apos;Displaying the refcursor for the sales people&apos;); -- The refcursor (empRefCur) result was opened before calling this procedure LOOP FETCH empRefCur INTO salesGuy; EXIT WHEN empRefCur%NOTFOUND; DBMS_OUTPUT.PUT_LINE(salesGuy.r_ename); END LOOP; CLOSE empRefCur; errCode := 0; errtext := &apos;OK&apos;; EXCEPTION WHEN OTHERS THEN errCode := SQLCODE; errText := SUBSTR(SQLERRM, 1, 200); END getCommEmps; FUNCTION get_sum_multiples ( multiple IN PLS_INTEGER, num IN PLS_INTEGER ) RETURN sum_multiples IS s sum_multiples; BEGIN FOR i in 1..num LOOP s(i) := multiple * ((i * (i + 1)) / 2); -- sum of the multiples END LOOP; RETURN s; END get_sum_multiples;BEGIN -- package initialization goes here DBMS_OUTPUT.PUT_LINE(&apos;Initialized package test&apos;);END test;/ 2_call_package.sql: set serveroutput on;declare errCode PLS_INTEGER; errtext VARCHAR2(256); myRefCur test.EmpCurType; -- 使用test包中定义的类型 salesPerson test.empRecType; name EMP.ENAME%TYPE; n PLS_INTEGER := 5; -- number of multiples to sum for display sn PLS_INTEGER := 10; -- number of multiples to sum m PLS_INTEGER := 3; -- multiple begin dbms_output.put_line(&apos; &apos;); dbms_output.put_line(&apos; &apos;); dbms_output.put_line(&apos; &apos;); dbms_output.put_line(&apos;Find some of the lowest paid employees and give a random employee a 10% pay raise&apos;); -- Give a lowely paid random employee a 10% pay raise test.givePayRise(5, name, errCode, errText); if errCode != 0 then dbms_output.put_line(&apos;Error code = &apos; || errCode || &apos; Error Text = &apos; || errtext); else dbms_output.put_line(name || &apos; got the 10% payraise&apos;); end if; -- Open a refcursor OPEN myRefCur FOR SELECT empno, ename, sal FROM emp WHERE comm IS NOT NULL; -- display the resultset of the refcursor test.getCommEmps(myRefCur, errCode, errText); if errCode != 0 then dbms_output.put_line(&apos;Error code = &apos; || errCode || &apos; Error Text = &apos; || errtext); end if; dbms_output.put_line(&apos; &apos;); dbms_output.put_line(&apos;Do some DDL and DML in a stored procedure&apos;); test.ddl_dml(&apos;hi&apos;, errCode, errText); if errCode != 0 then dbms_output.put_line(&apos;Error code = &apos; || errCode || &apos; Error Text = &apos; || errtext); end if; -- associative arrays dbms_output.put_line(&apos; &apos;); dbms_output.put_line(&apos;Use an associative array to compute the sum of multiples&apos;); dbms_output.put_line( &apos;Sum of the first &apos; || TO_CHAR(n) || &apos; multiples of &apos; || TO_CHAR(m) || &apos; is &apos; || TO_CHAR(test.get_sum_multiples (m, sn)(n))); end;/ 3_create_package_workload.sql: CREATE OR REPLACE PACKAGE workload AS PROCEDURE oltp_read_only ( v_id IN PLS_INTEGER, v_n IN PLS_INTEGER, v_m IN PLS_INTEGER, errCode OUT PLS_INTEGER, errText OUT VARCHAR2); PROCEDURE oltp_read_write ( v_id IN PLS_INTEGER, v_n IN PLS_INTEGER, v_m IN PLS_INTEGER, v_c IN CHAR, v_p IN VARCHAR2, errCode OUT PLS_INTEGER, errText OUT VARCHAR2);END workload;/CREATE OR REPLACE PACKAGE BODY workload AS -- Private package variables used for package initialization theErrCode PLS_INTEGER := 0; theErrText VARCHAR2(256) := &apos;OK&apos;; -- Using shared package cursors for efficiency CURSOR range_query (n PLS_INTEGER, m PLS_INTEGER) IS SELECT c FROM sbtest WHERE id BETWEEN n AND m; CURSOR range_order_query (n PLS_INTEGER, m PLS_INTEGER) IS SELECT c FROM sbtest WHERE id BETWEEN n AND m ORDER BY c; CURSOR range_distinct_query (n PLS_INTEGER, m PLS_INTEGER) IS SELECT DISTINCT c FROM sbtest WHERE id BETWEEN n AND m ORDER BY c; -- The workload read only workload PROCEDURE oltp_read_only ( v_id IN PLS_INTEGER, v_n IN PLS_INTEGER, v_m IN PLS_INTEGER, errCode OUT PLS_INTEGER, errText OUT VARCHAR2) IS -- Store the result of the column &apos;c&apos; cValue char(120); -- Store the sum of the rows in (n..m) sumK number(38,0); BEGIN errCode := 0; errtext := &apos;OK&apos;; -- oltp point query FOR i in 1 .. 10 LOOP -- DBMS_OUTPUT.PUT_LINE(&apos;oltp point query&apos;); SELECT c INTO cValue FROM sbtest WHERE id = v_id; -- DBMS_OUTPUT.PUT_LINE(&apos;c = &apos; || cValue); END LOOP; -- oltp range query (using a cursor for loop)-- DBMS_OUTPUT.PUT_LINE(&apos;oltp range query&apos;); FOR range_rows IN range_query(v_n, v_m) LOOP-- DBMS_OUTPUT.PUT_LINE(range_query%ROWCOUNT || &apos; c = &apos; || range_rows.c); null; END LOOP; -- olpt range SUM() query-- DBMS_OUTPUT.PUT_LINE(&apos;oltp range SUM() query&apos;); SELECT sum(k) INTO sumK FROM sbtest WHERE id BETWEEN v_n AND v_m;-- DBMS_OUTPUT.PUT_LINE(&apos;sumK = &apos; || sumK); -- oltp range ORDER BY query (using explicit fetches)-- DBMS_OUTPUT.PUT_LINE(&apos;oltp range ORDER BY query&apos;); OPEN range_order_query(v_n, v_m); LOOP FETCH range_order_query INTO cValue; EXIT WHEN range_order_query%NOTFOUND;-- DBMS_OUTPUT.PUT_LINE(&apos;c = &apos; || cValue); END LOOP; CLOSE range_order_query; -- oltp range DISTINCT query-- DBMS_OUTPUT.PUT_LINE(&apos;oltp range DISTINCT query&apos;); OPEN range_distinct_query(v_n, v_m); LOOP FETCH range_distinct_query INTO cValue; EXIT WHEN range_distinct_query%NOTFOUND;-- DBMS_OUTPUT.PUT_LINE(&apos;c = &apos; || cValue); END LOOP; CLOSE range_distinct_query; EXCEPTION WHEN NO_DATA_FOUND THEN errCode := 0; errText := &apos;OK&apos;; WHEN OTHERS THEN errCode := SQLCODE; errText := SUBSTR(SQLERRM, 1, 200); END oltp_read_only; -- The workload read + write workload PROCEDURE oltp_read_write ( v_id IN PLS_INTEGER, v_n IN PLS_INTEGER, v_m IN PLS_INTEGER, v_c IN CHAR, v_p IN VARCHAR2, errCode OUT PLS_INTEGER, errText OUT VARCHAR2) IS -- Store the result of the column &apos;c&apos; cValue char(120); -- Store the sum of the rows in (n..m) sumK number(38,0); BEGIN errCode := 0; errtext := &apos;OK&apos;; -- oltp point query FOR i in 1 .. 10 LOOP -- DBMS_OUTPUT.PUT_LINE(&apos;oltp point query&apos;); SELECT c INTO cValue FROM sbtest WHERE id = v_id; -- DBMS_OUTPUT.PUT_LINE(&apos;c = &apos; || cValue); END LOOP; -- oltp range query (using a cursor for loop)-- DBMS_OUTPUT.PUT_LINE(&apos;oltp range query&apos;); FOR range_rows IN range_query(v_n, v_m) LOOP-- DBMS_OUTPUT.PUT_LINE(range_query%ROWCOUNT || &apos; c = &apos; || range_rows.c); null; END LOOP; -- olpt range SUM() query-- DBMS_OUTPUT.PUT_LINE(&apos;oltp range SUM() query&apos;); SELECT sum(k) INTO sumK FROM sbtest WHERE id BETWEEN v_n AND v_m;-- DBMS_OUTPUT.PUT_LINE(&apos;sumK = &apos; || sumK); -- oltp range ORDER BY query (using explict fetches)-- DBMS_OUTPUT.PUT_LINE(&apos;oltp range ORDER BY query&apos;); OPEN range_order_query(v_n, v_m); LOOP FETCH range_order_query INTO cValue; EXIT WHEN range_order_query%NOTFOUND;-- DBMS_OUTPUT.PUT_LINE(&apos;c = &apos; || cValue); END LOOP; CLOSE range_order_query; -- oltp range DISTINCT query-- DBMS_OUTPUT.PUT_LINE(&apos;oltp range DISTINCT query&apos;); OPEN range_distinct_query(v_n, v_m); LOOP FETCH range_distinct_query INTO cValue; EXIT WHEN range_distinct_query%NOTFOUND;-- DBMS_OUTPUT.PUT_LINE(&apos;c = &apos; || cValue); END LOOP; CLOSE range_distinct_query; -- oltp UPDATES on index column-- DBMS_OUTPUT.PUT_LINE(&apos;oltp UPDATES on index column&apos;); UPDATE sbtest SET k = k + 1 WHERE id = v_n; -- oltp UPDATES on non-index column-- DBMS_OUTPUT.PUT_LINE(&apos;oltp UPDATES on non-index column&apos;); UPDATE sbtest SET c = v_n WHERE id = v_m; -- oltp DELETE query-- DBMS_OUTPUT.PUT_LINE(&apos;oltp DELETE query&apos;); DELETE FROM sbtest WHERE id = v_n; -- oltp INSERT query-- DBMS_OUTPUT.PUT_LINE(&apos;oltp INSERT query&apos;); INSERT INTO sbtest (id, k, c, pad) VALUES (v_n, v_m, v_c, v_p); -- Commit the changes COMMIT; EXCEPTION WHEN NO_DATA_FOUND THEN errCode := 0; errText := &apos;OK&apos;; WHEN OTHERS THEN errCode := SQLCODE; errText := SUBSTR(SQLERRM, 1, 200); END oltp_read_write;BEGIN -- package initialization goes here -- Run the procedures once to initialize everything oltp_read_only(1, 1, 10, theErrCode, theErrText ); oltp_read_write(1, 1, 10, &apos;abc&apos;, &apos;def&apos;, theErrCode, theErrText ); DBMS_OUTPUT.PUT_LINE(&apos;Initialized the workload package&apos;);END workload;/ 4_call_workload.sql: set serveroutput on;declare counter PLS_INTEGER; errCode PLS_INTEGER; errtext VARCHAR2(256); line1 VARCHAR2(256); line2 VARCHAR2(256); someText sbtest.c%TYPE; moreText VARCHAR2(256); i PLS_INTEGER; iterations PLS_INTEGER; startTime NUMBER; endTime NUMBER; duration NUMBER;begin -- Initialize the someText string line1 := &apos;The quick brown foxy did da jumping thing over that lazy doggy. &apos;; line2 := &apos;Question three, who was scott and who or what was tiger?&apos;; someText := line1 || line2; moreText := &apos;&apos;; -- Initialize the moreText string FOR i in 1 .. 60 LOOP moreText := moreText || &apos;a&apos;; END LOOP; -- Get the start time in centi-seconds startTime := DBMS_UTILITY.GET_TIME(); iterations := 10000; for counter in 1 .. iterations LOOP workload.oltp_read_only(1, 1, 1100, errCode, errtext); if errCode != 0 then exit; end if; end loop; -- Get the end time in centi-seconds endTime := DBMS_UTILITY.GET_TIME(); if errCode !=0 then dbms_output.put_line(&apos; &apos;); dbms_output.put_line(&apos;Error code = &apos; || errCode || &apos; Error Text = &apos; || errtext); end if; duration := endTime - startTime; IF duration &gt; 0 THEN dbms_output.put_line(&apos; &apos;); dbms_output.put_line(&apos;Called workload.oltp_read_only &apos; || iterations || &apos; times. TPS = &apos; || trunc(iterations / duration * 100, 1) ); ELSE dbms_output.put_line(&apos;Could not get valid timing info&apos;); END IF; iterations := 10000; for counter in 1 .. iterations LOOP workload.oltp_read_write(1, 1, 1100, someText, moreText, errCode, errtext); if errCode != 0 then exit; end if; end loop; -- Get the end time in centi-seconds endTime := DBMS_UTILITY.GET_TIME(); if errCode !=0 then dbms_output.put_line(&apos; &apos;); dbms_output.put_line(&apos;Error code = &apos; || errCode || &apos; Error Text = &apos; || errtext); end if; duration := endTime - startTime; IF duration &gt; 0 THEN dbms_output.put_line(&apos;Called workload.oltp_read_write &apos; || iterations || &apos; times. TPS = &apos; || trunc(iterations / duration * 100, 1) ); ELSE dbms_output.put_line(&apos;Could not get valid timing info&apos;); END IF;end;/","categories":[{"name":"数据库","slug":"数据库","permalink":"http://blog.guitoubing.top/categories/数据库/"}],"tags":[{"name":"TimesTen","slug":"TimesTen","permalink":"http://blog.guitoubing.top/tags/TimesTen/"},{"name":"内存数据库","slug":"内存数据库","permalink":"http://blog.guitoubing.top/tags/内存数据库/"}]},{"title":"C# - dotnet基本配置及EFCore连接Mysql","slug":"Dotnet","date":"2018-06-26T08:57:08.000Z","updated":"2021-05-16T10:59:22.035Z","comments":true,"path":"2018/06/26/Dotnet/","link":"","permalink":"http://blog.guitoubing.top/2018/06/26/Dotnet/","excerpt":"前奏部分 下载并安装dotnet core 下载并安装vscode（需要把vscode添加到path中）","text":"前奏部分 下载并安装dotnet core 下载并安装vscode（需要把vscode添加到path中） vscode中搜索并安装C#插件、NuGet Package Manager插件 新建项目 &gt; mkdir dotnet&gt; cd dotnet&gt; dotnet new mvc&gt; code .&gt; commond + shift + p输入nuget add package安装以下依赖包，各个包的Version可在添加时选择 添加包时以下代码将自动在dotnet.csproj中添加： &gt; &lt;ItemGroup&gt;&gt; &lt;PackageReference Include=\"Microsoft.AspNetCore.All\" Version=\"2.0.6\"/&gt;&gt; &lt;PackageReference Include=\"Microsoft.EntityFrameworkCore.Sqlite\" Version=\"2.1.0\"/&gt;&gt; &lt;PackageReference Include=\"Microsoft.EntityFrameworkCore.Tools\" Version=\"2.1.0\"/&gt;&gt; &lt;PackageReference Include=\"Microsoft.EntityFrameworkCore.Sqlite.Design\" Version=\"2.0.0-preview1-final\"/&gt;&gt; &lt;PackageReference Include=\"Microsoft.EntityFrameworkCore.Tools.DotNet\" Version=\"2.1.0-preview1-final\"/&gt;&gt; &lt;PackageReference Include=\"Pomelo.EntityFrameworkCore.MySql\" Version=\"2.1.0-rc1-final\"/&gt;&gt; &lt;PackageReference Include=\"Pomelo.EntityFrameworkCore.MySql.Design\" Version=\"1.1.2\"/&gt;&gt; &lt;/ItemGroup&gt;&gt; Model部分 连接数据库创建实体： 在vscode终端中输入以下命令 dotnet ef dbcontext scaffold &quot;server=localhost;userid=user;pwd=password;port=3306;database=university;sslmode=none;&quot; Pomelo.EntityFrameworkCore.MySql -o Models dotnet ef两个问题 问题1：No executable found matching command “dotnet-ef”解决方法：dotnet.csproj中添加如下行： &gt; &lt;ItemGroup&gt;&gt; &lt;DotNetCliToolReference Include=\"Microsoft.EntityFrameworkCore.Tools.DotNet\" Version=\"2.1.0-preview1-final\"/&gt;&gt; &lt;/ItemGroup&gt;&gt; 问题2：Version for package Microsoft.EntityFrameworkCore.Tools.DotNet could not be resolved. 原因：上述配置中Version版本与包引用中的版本不一致，修改上述添加代码的Version即可 此时将会在Models文件夹下创建所有数据库表的实体，同时会创建一个universityContext.cs实体（university为我数据库名称，自行定义），用于对整个数据库的操作。至此MVC已完成Model部分。 Controller及View部分 目前项目Models文件夹下已有DBFirst模式生成的实体文件： 我们选择Student的Model创建C-V视图 这里说明一下，MVC模式中Model顾名思义是数据模型、实体，而View和Controller是相互依存的。一般步骤是先创建StudentController.cs文件，定义其中的路由(URL映射，定义了路由之后可以直接通过URL访问该函数)，如本项目中的StudentController.cs中定义的Index： &gt; public IActionResult Index()&#123;&gt; return View(_context.Student.ToList());&gt; &#125;&gt; 如此定义后，再在Views文件夹下创建对应Controller的文件夹，此处为Student，而在Controller中定义的每一个路由，都要有对应的一个cshtml文件，此处在Student下创建Index.cshtml。简而言之，View只负责处理布局，Controller只负责处理逻辑。 创建StudentController.cs &gt; using System;&gt; using System.Collections.Generic;&gt; using System.Diagnostics;&gt; using System.Linq;&gt; using System.Threading.Tasks;&gt; using Microsoft.AspNetCore.Mvc;&gt; using dotnet.Models;&gt; using dotnet;&gt; &gt; public class StudentController : Controller&#123;&gt; private universityContext _context;&gt; &gt; public StudentController(universityContext context)&#123;&gt; _context = context;&gt; &#125;&gt; &gt; public IActionResult Index()&#123;&gt; return View(_context.Student.ToList());&gt; &#125;&gt; &gt; public IActionResult Register()&#123;&gt; return View();&gt; &#125;&gt; &gt; [HttpPost]&gt; [ValidateAntiForgeryToken]&gt; public IActionResult Register(Student student)&#123;&gt; if(ModelState.IsValid)&#123;&gt; _context.Student.Add(student);&gt; _context.SaveChanges();&gt; return RedirectToAction(&quot;Index&quot;);&gt; &#125;&gt; return View(student);&gt; &#125;&gt; &#125;&gt; 创建Student文件夹，以及对应路由的cshtml Index.cshtml &gt; @&#123;&gt; ViewData[\"Title\"] = \"学生主页\";&gt; &#125;&gt; &gt; &lt;!-- 此处这个model声明不能忘记 --&gt;&gt; @model IEnumerable&lt;dotnet.Student&gt;&gt; &gt; &lt;table class=\"table\"&gt;&gt; &lt;tr&gt;&gt; &lt;th&gt;Id&lt;/th&gt;&gt; &lt;th&gt;姓名&lt;/th&gt;&gt; &lt;th&gt;系&lt;/th&gt;&gt; &lt;th&gt;学分&lt;/th&gt;&gt; &lt;/tr&gt;&gt; @foreach (var item in Model)&#123;&gt; &lt;tr&gt;&gt; &lt;td&gt;&gt; @Html.DisplayFor(modelItem =&gt; item.Id)&gt; &lt;/td&gt;&gt; &lt;td&gt;&gt; @Html.DisplayFor(modelItem =&gt; item.Name)&gt; &lt;/td&gt;&gt; &lt;td&gt;&gt; @Html.DisplayFor(modelItem =&gt; item.DeptName)&gt; &lt;/td&gt;&gt; &lt;td&gt;&gt; @Html.DisplayFor(modelItem =&gt; item.TotCred)&gt; &lt;/td&gt;&gt; &lt;/tr&gt;&gt; &#125;&gt; &lt;/table&gt;&gt; Register.cshtml &gt; @model dotnet.Student&gt; &gt; @&#123;&gt; ViewData[\"Title\"] = \"注册\";&gt; &#125;&gt; &gt; &lt;form asp-controller=\"Student\" asp-action=\"Register\" method=\"POST\"&gt;&gt; &lt;div class=\"form-group\"&gt;&gt; &lt;label asp-for=\"Id\" class=\"col-md-2 control-label\"&gt;编号：&lt;/label&gt;&gt; &lt;div class=\"col-md-10\"&gt;&gt; &lt;input class=\"form-control\" asp-for=\"Id\"/&gt;&gt; &lt;span asp-validation-for=\"Id\" class=\"text-danger\"&gt;&lt;/span&gt;&gt; &lt;/div&gt;&gt; &lt;label asp-for=\"Name\" class=\"col-md-2 control-label\"&gt;名字：&lt;/label&gt;&gt; &lt;div class=\"col-md-10\"&gt;&gt; &lt;input class=\"form-control\" asp-for=\"Name\"/&gt;&gt; &lt;span asp-validation-for=\"Name\" class=\"text-danger\"&gt;&lt;/span&gt;&gt; &lt;/div&gt;&gt; &lt;label asp-for=\"DeptName\" class=\"col-md-2 control-label\"&gt;系：&lt;/label&gt;&gt; &lt;div class=\"col-md-10\"&gt;&gt; &lt;input class=\"form-control\" asp-for=\"DeptName\"/&gt;&gt; &lt;span asp-validation-for=\"DeptName\" class=\"text-danger\"&gt;&lt;/span&gt;&gt; &lt;/div&gt;&gt; &lt;label asp-for=\"TotCred\" class=\"col-md-2 control-label\"&gt;学分：&lt;/label&gt;&gt; &lt;div class=\"col-md-10\"&gt;&gt; &lt;input class=\"form-control\" asp-for=\"TotCred\"/&gt;&gt; &lt;span asp-validation-for=\"TotCred\" class=\"text-danger\"&gt;&lt;/span&gt;&gt; &lt;/div&gt;&gt; &lt;div class=\"col-md-offset-2 col-md-10\"&gt;&gt; &lt;input type=\"submit\" value=\"保存\" class=\"btn btn-default\"/&gt;&gt; &lt;/div&gt;&gt; &lt;/div&gt;&gt; &lt;/form&gt;&gt; 关于抛出以下错误的解决方法 错误： 解决方法： 注意最下面的Tip：由于我们在Startup.cs中已经添加如下代码： &gt; public void ConfigureServices(IServiceCollection services)&gt; &#123;&gt; services.AddDbContext&lt;universityContext&gt;();&gt; services.AddMvc();&gt; &#125;&gt; 即满足条件“already configured outside of the context in Startup.cs”，因此我们需要将上述图片中的if语句注释掉，如下： &gt; //if (!optionsBuilder.IsConfigured)&#123; &gt; optionsBuilder.UseMySql(&quot;server=localhost;userid=root;pwd=tanrui;port=3306;database=university;sslmode=none;&quot;);&gt; //&#125;&gt; 运行项目 调试的方法 vscode下点按“开始调试” 浏览器将会自动跳转至localhost:5000 在URL中添加/student或student/index跳转到我们定义的Controller中，一般情况下index路由是可以忽略不写的，此时自动定位到index中： 戳这里下载Asp.net Core开发实战.pdf","categories":[{"name":"C#","slug":"C","permalink":"http://blog.guitoubing.top/categories/C/"}],"tags":[{"name":".Net","slug":"Net","permalink":"http://blog.guitoubing.top/tags/Net/"},{"name":"MySQL","slug":"MySQL","permalink":"http://blog.guitoubing.top/tags/MySQL/"},{"name":"EFCore","slug":"EFCore","permalink":"http://blog.guitoubing.top/tags/EFCore/"}]},{"title":"我们梦中见","slug":"我们梦中见","date":"2018-06-26T04:29:17.000Z","updated":"2021-05-16T11:03:25.055Z","comments":true,"path":"2018/06/26/我们梦中见/","link":"","permalink":"http://blog.guitoubing.top/2018/06/26/我们梦中见/","excerpt":"“Yeah It’s on. ” 26/06/2018当兵回来用的虚拟主机建的博客，hexo建在本地。前段时间电脑重装了，以前的博客就落满了灰，现迁移过来，换个心情。 14/10/2017（是不是个技博自己心里没点B数？","text":"“Yeah It’s on. ” 26/06/2018当兵回来用的虚拟主机建的博客，hexo建在本地。前段时间电脑重装了，以前的博客就落满了灰，现迁移过来，换个心情。 14/10/2017（是不是个技博自己心里没点B数？ 应该说天明学长在技术方面给予了很大的支持，为她打call！ 说要建站已经是三年前了，那时候在某课网上闲逛看到了关于Linux搭建服务器的视频，学了点，发现，what are you fk saying？后来自己买了本书，噢~ 更™不懂了。然后，就去了号子（？？）。转眼两年过去了，是该重新做人了，该搬的砖还得搬，搬不完还想吃饭？ 买的第一个虚拟主机是景安一台国内主机，这个时候还是不知道国内主机和海外主机有啥区别，只知道国内主机便宜，不，新用户免费。于是买了个试了下。配套的买了个top域名¥15.00/月，是贼贵了。绑定域名时发现需要备案，备案就备案吧，流程走下去。一大堆东西拍了照填了表提交上去了想的差不多了吧。结果跟我说非上海本地户口要™居住证或者临时居住证，我哪里去办，户口都没迁过来，想想要不找个备案不怎么严的省份备案下，看了下河南（？？？）以及其他，要么是要本地手机号要么就是居住证，算了，贵国厉害，我买海外。于是买了个HK主机，¥199/年（后来看到阿里云服务器学生价¥10/月+com域名就扇了自己一巴掌，你有钱行了吧）。 接下来是干货了（扯淡然后就是绑定域名了没啥说的。 对于一个毫无前端经验的人来说，有了这些又有啥用，别人进你网站就为了看你在云里面存了多少种子？ 这里又要提到天明学长了，在她网站中得知有了个Hexo的框架，仿佛看到了未来。至于Hexo怎么用，官方文档里面都很详尽了，这里讲几点用的时候踩过的坑，以备。 _config.YML配置，比较重要的几个地方路径URLurl: http://guitoubing.top/root: / url和root一定要注意，最后面的“/”千万不要忘了，不然在hexo generate的时候肯定会报错 在generate后要注意public文件夹的位置，public文件夹一般自动创建在当前目录下，我在server后，本地服务器浏览是没有问题的，但是点开public文件夹里面的index就会连不上css，当然上传到服务器之后肯定也是连不上的了，因为root: /这行代码认为你当前工作目录是在根目录下（硬盘根目录或者服务器根目录），有的同学会想那我把root改成我当前位置不就好了，我也试过，此时public里面的index可以正常浏览，但是传到deploy到服务器上就又连不上了，因为服务器里面没有你当前这样的路径呀。这里我用的笨办法，把创建好的public文件夹复制到硬盘根目录下，然后发现本地服务器上index是可以正常显示了，传到服务器上之后也是可以的。 Disqus插件disqus_username: guitoubing 因为多说已经关闭服务了，只能用Disqus，而Disqus又是需要科学上网才能加载的，所以也没办法了。如果你能科学上网，那只要把这里的disqus_username改成自己注册的账号即可，我用的主题hexo-theme-huxblog已经集成了Disqus的js代码，所以不需要其他设置，如果用的其他主题/themes/layout里面的ejs文件中添加js代码即可。 Analytics# Analytics settings# Baidu Analyticsba_track_id: bcfce8e737b***********04c164dc96# Google Analyticsga_track_id: &apos;UA-10*******-1&apos; # Format: UA-xxxxxx-xxga_domain: guitoubing.top deploydeploy: type: ftpsync host: guitoubing.top user: webmaster@HK****** pass: tanrui106 remote: /WEB/ port: 21 deploy就是部署到服务器上咯，因为我用的是HK虚拟主机，所以配置如上，这里的各个信息都是你所部署的服务器信息没什么好说的。 _config.YML配置完成了就可以开始创作咯hexo new &quot;blog&quot; hexo ghexo s Hexo官方文档都有详细使用方法，不赘述。 有几句MMP当讲古有一商人，于川中收购一批苎麻、小麦、桔子、兽皮，从水路出川。船至半途，水急桨朽，桨折断而顺水去，船夫甚急，问商人： 无桨不得行船，你所携货物中可有长直之物当桨？ 商人安慰他： 莫急，我有桔麻麦皮不知当桨不当桨？ 从开始接触hexo到成功deploy到服务器上，算下来该有一下午加一晚上了。应该说两年没接触编程了，那句“程序员写了一段让自己不用再写代码的代码”已经不是笑话，也许是两年之前也啥屁不懂，现在越来越觉得放眼看世界是多重要。当我还熬夜敲着基础代码时，互联网上已经有了其他解决方案，倒不是说基础代码不重要，而是已经有人用基础代码敲出了不用再敲基础代码的代码，那么，吃肉，还是喝汤，看自己选择了。（我选择狗带）","categories":[{"name":"心记","slug":"心记","permalink":"http://blog.guitoubing.top/categories/心记/"}],"tags":[]}]}